Aleksei Bochkovskii&AmaÃ«l Delaunoy&Hugo Germain&Marcel Santos\\ANDYichao Zhou&Stephan R. Richter  
Apple&Vladlen Koltun

###### Abstract

We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code & weights at [https://github.com/apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)

## 1 Introduction

Figure 1: Results on images from the AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)) (1st & 3rd column) and DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)) (2nd column) datasets. Input image on top, estimated depth maps from Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)), and Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)) below. Depth Pro produces zero-shot metric depth maps with absolute scale at 2.25-megapixel native resolution in 0.3 seconds on a V100 GPU.

Zero-shot monocular depth estimation underpins a growing variety of applications, such as advanced image editing, view synthesis, and conditional image generation. Inspired by MiDaSÂ (Ranftl etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib83)) and many follow-up worksÂ (Ranftl etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib82); Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112); Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76); Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), applications increasingly leverage the ability to derive a dense pixelwise depth map for any image.

Our work is motivated in particular by novel view synthesis from a single image, an exciting application that has been transformed by advances in monocular depth estimationÂ (Hedman etÂ al., [2017](https://arxiv.org/html/2410.02073v1#bib.bib37); Shih etÂ al., [2020](https://arxiv.org/html/2410.02073v1#bib.bib90); Jampani etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib43); Khan etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib46)). Applications such as view synthesis imply a number of desiderata for monocular depth estimation. First, the depth estimator should work zero-shot on any image, not restricted to a specific domainÂ (Ranftl etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib83); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112)). Furthermore, the method should ideally produce _metric_ depth maps in this zero-shot regime, to accurately reproduce object shapes, scene layouts, and absolute scalesÂ (Guizilini etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib35); Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)). For the broadest applicability â€˜in the wildâ€™, the method should produce metric depth maps with absolute scale even if no camera intrinsics (such as focal length) are provided with the imageÂ (Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)). This enables view synthesis scenarios such as â€œSynthesize a view of this scene from 63 mm awayâ€ for essentially arbitrary single imagesÂ (Dodgson, [2004](https://arxiv.org/html/2410.02073v1#bib.bib21)).

Second, for the most compelling results, the monocular depth estimator should operate at high resolution and produce fine-grained depth maps that closely adhere to image details such as hair, fur, and other fine structuresÂ (Miangoleh etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib70); Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45); Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)). One benefit of producing sharp depth maps that accurately trace intricate details is the elimination of â€œflying pixelsâ€, which can degrade image quality in applications such as view synthesisÂ (Jampani etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib43)).

Third, for many interactive application scenarios, the depth estimator should operate at low latency, processing a high-resolution image in less than a second to support interactive view synthesis â€œqueriesâ€ on demand. Low latency is a common characteristic of methods that reduce zero-shot monocular depth estimation to a single forward pass through a neural network (Ranftl etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib82); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112); Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)), â€‰ but it is not always shared by methods that employ more computationally demanding machinery at test timeÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45); Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)).

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x1.png)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x2.png)

Figure 2: Boundary recall versus runtime. Depth Pro outperforms prior work by a multiplicative factor in boundary accuracy while being orders of magnitude faster than works focusing on fine-grained predictions (e.g.,Â Marigold, PatchFusion).

In this work, we present a foundation model for zero-shot metric monocular depth estimation that meets all of these desiderata. Our model, Depth Pro, produces metric depth maps with absolute scale on arbitrary images â€˜in the wildâ€™ without requiring metadata such as camera intrinsics. It operates at high resolution, producing 2.25-megapixel depth maps (with a native output resolution of 1536Ã—1536153615361536\\times 15361536 Ã— 1536 before optional further upsampling) in 0.3 seconds on a V100 GPU. Fig.Â [1](https://arxiv.org/html/2410.02073v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") shows some representative results. Depth Pro dramatically outperforms all prior work in sharp delineation of object boundaries, including fine structures such as hair, fur, and vegetation. As shown in Fig.Â [2](https://arxiv.org/html/2410.02073v1#S1.F2 "Figure 2 â€£ 1 Introduction â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), Depth Pro offers unparalleled boundary tracing, outperforming all prior work by a multiplicative factor in boundary recall. (See Sec.Â [4](https://arxiv.org/html/2410.02073v1#S4 "4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") for additional detail.) Compared to the prior state of the art in boundary accuracy (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45); Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)), Depth Pro is one to two orders of magnitude faster, yields much more accurate boundaries, and provides metric depth maps with absolute scale.

Depth Pro is enabled by a number of technical contributions. First, we design an efficient multi-scale ViT-based architecture for capturing the global image context while also adhering to fine structures at high resolution. Second, we derive a new set of metrics that enable leveraging highly accurate matting datasets for quantifying the accuracy of boundary tracing in evaluating monocular depth maps. Third, we devise a set of loss functions and a training curriculum that promote sharp depth estimates while training on real-world datasets that provide coarse and inaccurate supervision around boundaries, along with synthetic datasets that offer accurate pixelwise ground truth but limited realism. Fourth, we contribute zero-shot focal length estimation from a single image that dramatically outperforms the prior state of the art.

## 2 Related work

Early work on monocular depth estimation focused on training on individual datasets recorded with a single cameraÂ (Saxena etÂ al., [2009](https://arxiv.org/html/2410.02073v1#bib.bib85); Eigen etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib24); Eigen & Fergus, [2015](https://arxiv.org/html/2410.02073v1#bib.bib23)). Although this setup directly enabled metric depth predictions, it was limited to single datasets and narrow domains.

Zero-shot depth estimation. MegaDepthÂ (Li & Snavely, [2018](https://arxiv.org/html/2410.02073v1#bib.bib60)) demonstrated that training on a diverse dataset allows generalizing monocular depth prediction beyond a specific domain. MiDaSÂ (Ranftl etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib83)) advanced this idea by training on a large mix of diverse datasets with a scale-and-shift-invariant loss. Follow-up works applied this recipe to transformer-based architecturesÂ (Ranftl etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib82); Birkl etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib7)) and further expanded the set of feasible datasets through self-supervisionÂ (Spencer etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib94); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112)). A line of work uses self-supervision to learn from unlabeled image and video dataÂ (Petrovai & Nedevschi, [2022](https://arxiv.org/html/2410.02073v1#bib.bib75); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112)). A number of recent approachesÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45); Gui etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib33)) harness diffusion models to synthesize relative depth maps. Although some of these methods demonstrated excellent generalization, their predictions are ambiguous in scale and shift, which precludes downstream applications that require accurate shapes, sizes, or distances.

Zero-shot metric depth. A line of work sought to improve metric depth prediction through a global distribution of depth valuesÂ (Fu etÂ al., [2018](https://arxiv.org/html/2410.02073v1#bib.bib29); Bhat etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib4); [2022](https://arxiv.org/html/2410.02073v1#bib.bib5); Li etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib62)) and further conditioning on scene typeÂ (Bhat etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib6)). A different approach directly takes into account camera intrinsics. Cam-ConvsÂ (FÃ¡cil etÂ al., [2019](https://arxiv.org/html/2410.02073v1#bib.bib26)) conditioned convolutions on the camera intrinsics. LeReSÂ (Yin etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib115)) trains a separate network for undistorting point clouds to recover scale and shift, Metric3DÂ (Yin etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib116)) scales images or depth maps to a canonical space and remaps estimated depth given the focal length, and ZeroDepthÂ (Guizilini etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib35)) learns camera-specific embedddings in a variational framework. DMDÂ (Saxena etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib86)) conditions a diffusion model on the field of view. Metric3DÂ v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)) leverages surface normals as an auxilliary output to improve metric depth. All of these methods require the camera intrinsics to be known and accurate. More recent works attempt to reason about unknown camera intrinsics either through a separate networkÂ (Spencer etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib95)) or by predicting a camera embedding for conditioning its depth predictions in a spherical spaceÂ (Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)). Akin to these recent approaches, our method does not require the focal length to be provided as input. We propose to directly estimate the field of view from intermediate features of the depth prediction network, and show that this substantially outperforms the prior state of the art in the task of cross-domain focal length estimation.

Sharp occluding contours. SharpNetÂ (Ramamonjisoa & Lepetit, [2019](https://arxiv.org/html/2410.02073v1#bib.bib79)) incorporates normals and occluding contour constraints, but requires additional contour and normal supervision during training. BoostingDepthÂ (Miangoleh etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib70)) obtains detailed predictions from a low-resolution network by applying it independently to image patches. Since the patches lack global context, BoostingDepth fuses them through a sophisticated multi-step pipeline. PatchFusionÂ (Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)) refines this concept through image-adaptive patch sampling and tailored modules that enable end-to-end training. A recent line of work leverages diffusion priors to enhance the sharpness of occlusion boundariesÂ (Gui etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib33); Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)). These approaches predominantly focus on predicting relative (rather than metric) depth. We propose a simpler architecture without task-specific modules or diffusion priors and demonstrate that even sharper and more accurate results can be obtained while producing metric depth maps and reducing runtime by more than two orders of magnitude.

Guided depth super-resolution uses the input image to upsample low-resolution depth predictionsÂ (Metzger etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib69); Zhong etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib123)). SMDNetÂ (Tosi etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib100)) predicts bimodal mixture densities to sharpen occluding contours. And RamamonjisoaÂ etÂ al.Â (Ramamonjisoa etÂ al., [2020](https://arxiv.org/html/2410.02073v1#bib.bib80)) introduce a module for learning to sharpen depth boundaries of a pretrained network. These works are orthogonal to ours and could be applied to further upsample our high-resolution predictions.

To evaluate boundary tracing in predicted depth maps, Koch etÂ al. ([2018](https://arxiv.org/html/2410.02073v1#bib.bib50)) introduce the iBims dataset with manual annotations of occluding contours and corresponding metrics. The need for manual annotation and highly accurate depth ground truth constrain the benchmark to a small set of indoor scenes. We contribute metrics based on segmentation and matting datasets that provide a complementary view by enabling evaluation on complex, dynamic environments or scenes with exceedingly fine detail for which ground-truth depth is impossible to obtain.

Multi-scale vision transformers. Vision transformers (ViTs) have emerged as the dominant general-purpose architecture for perception tasks, but operate at low resolutionÂ (Dosovitskiy etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib22)). NaÃ¯vely scaling the architecture to higher resolutions is prohibitive due to the computational complexity. Several works identified the attention layers as the main obstacle to scaling up ViT and have proposed alternativesÂ (Zhu etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib124); Liu etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib65); Li etÂ al., [2022c](https://arxiv.org/html/2410.02073v1#bib.bib59); Chu etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib16); Liu etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib66); [2023](https://arxiv.org/html/2410.02073v1#bib.bib64); Cai etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib12); Jaegle etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib42)).

Another line of work modified the ViT architecture to produce a hierarchy of featuresÂ (Fan etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib27); Xie etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib111); Yuan etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib118); Ranftl etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib82); Chen etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib13); Lee etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib54)).

Rather than modifying the ViT architecture, which requires computationally expensive retraining, we propose a network architecture that applies a plain ViT backbone at multiple scales and fuses predictions into a single high-resolution output. This architecture benefits from ongoing improvements in ViT pretraining, as new variants can be easily swapped inÂ (Oquab etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib72); Peng etÂ al., [2022b](https://arxiv.org/html/2410.02073v1#bib.bib74); Sun etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib97)).

Pretrained vision transformers have been adapted for semantic segmenation and object detection. ViT-AdapterÂ (Chen etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib15)) and ViT-CoMerÂ (Xia etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib108)) supplement a pretrained ViT with a convolutional network for dense prediction, whereas ViT-DetÂ (Li etÂ al., [2022b](https://arxiv.org/html/2410.02073v1#bib.bib58)) builds a feature pyramid on top of a pretrained ViT. Distinct from these, we fuse features from the ViT applied at multiple scales to learn global context together with local detail.

## 3 Method

### 3.1 Network

The key idea of our architecture is to apply plain ViT encodersÂ (Dosovitskiy etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib22)) on patches extracted at multiple scales and fuse the patch predictions into a single high-resolution dense prediction in an end-to-end trainable model. Fig.Â [3](https://arxiv.org/html/2410.02073v1#S3.F3 "Figure 3 â€£ 3.1 Network â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") illustrates the architecture. For predicting depth, we employ two ViT encoders, a patch encoder and an image encoder. The patch encoder is applied on patches extracted at multiple scales. Intuitively, this may allow learning representations that are scale-invariant as weights are shared across scales. The image encoder anchors the patch predictions in a global context. It is applied to the whole input image, downsampled to the base input resolution of the chosen encoder backbone (in our case 384Ã—\\timesÃ—384).

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x3.jpeg)

Figure 3: Overview of the network architecture. An image is downsampled at several scales. At each scale, it is split into patches, which are processed by a ViT-based patch encoder, with weights shared across scales. Patches are merged into feature maps, upsampled, and fused via a DPT decoder. Predictions are anchored by a separate image encoder that provides global context.

The whole network operates at a fixed resolution of 1536Ã—\\timesÃ—1536, which was chosen as a multiple of the ViTâ€™s 384Ã—\\timesÃ—384\. This guarantees a sufficiently large receptive field and constant runtimes for any image while preventing out-of-memory errors (which we repeatedly observed for variable-resolution approaches on large images). Confirming this design choice, the results we report in Sec.Â [4](https://arxiv.org/html/2410.02073v1#S4 "4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") and Tab.Â [5](https://arxiv.org/html/2410.02073v1#A1.T5 "Table 5 â€£ A.3 Runtime â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") demonstrate that Depth Pro is consistently orders of magnitude faster than variable-resolution approaches while being more accurate and producing sharper boundaries. A key benefit of assembling our architecture from plain ViT encoders over custom encoders is the abundance of pretrained ViT-based backbones that can be harnessedÂ (Oquab etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib72); Peng etÂ al., [2022b](https://arxiv.org/html/2410.02073v1#bib.bib74); Sun etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib97)). We evaluate several pretrained backbones and compare our architecture to other high-resolution architectures in the appendices (Tab.Â [8](https://arxiv.org/html/2410.02073v1#A2.T8 "Table 8 â€£ B.2 High-resolution alternatives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") and Sec.Â [B.2](https://arxiv.org/html/2410.02073v1#A2.SS2 "B.2 High-resolution alternatives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")).

After downsampling to 1536Ã—\\timesÃ—1536, the input image is split into patches of 384Ã—384384384384\\times 384384 Ã— 384. For the two finest scales, we let patches overlap to avoid seams. At each scale, the patches are fed into the patch encoder, which produces a feature tensor at resolution 24Ã—24242424\\times 2424 Ã— 24 per input patch (features 3 â€“ 6 in Fig.Â [3](https://arxiv.org/html/2410.02073v1#S3.F3 "Figure 3 â€£ 3.1 Network â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")). At the finest scale we further extract intermediate features (features 1 & 2 in Fig.Â [3](https://arxiv.org/html/2410.02073v1#S3.F3 "Figure 3 â€£ 3.1 Network â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")) to capture finer-grained details. We merge the feature patches into maps, which are fed into the decoder module, which resembles the DPT decoderÂ (Ranftl etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib82)).

In addition to sharing representations across scales, the patch-based application of the encoder network allows trivial parallelization as patches can be processed independently. Another source of computational efficiency comes from the lower computational complexity of patch-based processing in comparison to scaling up the ViT to higher resolutions. The reason is multi-head self-attentionÂ (Vaswani etÂ al., [2017](https://arxiv.org/html/2410.02073v1#bib.bib102)), whose computational complexity scales quadratically with the number of input pixels, and thus quartically in image dimension.

### 3.2 Sharp monocular depth estimation

#### Training objectives.

For each input image Iğ¼Iitalic\_I, our network fğ‘“fitalic\_f predicts a canonical inverse depth image C\=fâ¢(I)ğ¶ğ‘“ğ¼C=f(I)italic\_C = italic\_f ( italic\_I ). To obtain a dense metric depth map Dmsubscriptğ·ğ‘šD\_{m}italic\_D start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT, we scale by the horizontal field of view, represented by the focal length fğ‘ğ‘¥subscriptğ‘“ğ‘ğ‘¥f\_{\\mathit{px}}italic\_f start\_POSTSUBSCRIPT italic\_px end\_POSTSUBSCRIPT and the width wğ‘¤witalic\_wÂ (Yin etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib116)): Dm\=fğ‘ğ‘¥wâ¢C.subscriptğ·ğ‘šsubscriptğ‘“ğ‘ğ‘¥ğ‘¤ğ¶D\_{m}=\\tfrac{f\_{\\mathit{px}}}{wC}.italic\_D start\_POSTSUBSCRIPT italic\_m end\_POSTSUBSCRIPT = divide start\_ARG italic\_f start\_POSTSUBSCRIPT italic\_px end\_POSTSUBSCRIPT end\_ARG start\_ARG italic\_w italic\_C end\_ARG .

We train with several objectives, all based on canonical inverse depth, because this prioritizes areas close to the camera over farther areas or the whole scene, and thus supports visual quality in applications such as novel view synthesis. Let C^^ğ¶\\hat{C}over^ start\_ARG italic\_C end\_ARG be the ground-truth canonical inverse depth. For all metric datasets we compute the mean absolute errorÂ (â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT, Eq.Â [1](https://arxiv.org/html/2410.02073v1#S3.E1 "In Training objectives. â€£ 3.2 Sharp monocular depth estimation â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")) per pixel iğ‘–iitalic\_i, and discard pixels with an error in the top 20%percent2020\\%20 % per image for real-world (as opposed to synthetic) datasets:

<table id="A4.EGx1"><tbody id="S3.E1"><tr><td></td><td><math alttext="\displaystyle\mathcal{L}_{\mathit{MAE}}(\hat{C},C)=\frac{1}{N}\sum^{N}_{i}|%
\hat{C}_{i}-C_{i}|." display="inline" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml"><mrow id="S3.E1.m1.3.3.1.1.3" xref="S3.E1.m1.3.3.1.1.3.cmml"><msub id="S3.E1.m1.3.3.1.1.3.2" xref="S3.E1.m1.3.3.1.1.3.2.cmml"><mi id="S3.E1.m1.3.3.1.1.3.2.2" xref="S3.E1.m1.3.3.1.1.3.2.2.cmml">â„’</mi><mi id="S3.E1.m1.3.3.1.1.3.2.3" xref="S3.E1.m1.3.3.1.1.3.2.3.cmml">ğ‘€ğ´ğ¸</mi></msub><mo id="S3.E1.m1.3.3.1.1.3.1" xref="S3.E1.m1.3.3.1.1.3.1.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.1.1.3.3.2" xref="S3.E1.m1.3.3.1.1.3.3.1.cmml"><mo id="S3.E1.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S3.E1.m1.3.3.1.1.3.3.1.cmml">(</mo><mover accent="true" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mi id="S3.E1.m1.1.1.2" xref="S3.E1.m1.1.1.2.cmml">C</mi><mo id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.cmml">^</mo></mover><mo id="S3.E1.m1.3.3.1.1.3.3.2.2" xref="S3.E1.m1.3.3.1.1.3.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">C</mi><mo id="S3.E1.m1.3.3.1.1.3.3.2.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.1.1.2" xref="S3.E1.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E1.m1.3.3.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.3.cmml"><mfrac id="S3.E1.m1.3.3.1.1.1.3a" xref="S3.E1.m1.3.3.1.1.1.3.cmml"><mn id="S3.E1.m1.3.3.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.3.2.cmml">1</mn><mi id="S3.E1.m1.3.3.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.3.3.cmml">N</mi></mfrac></mstyle><mo id="S3.E1.m1.3.3.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><munderover id="S3.E1.m1.3.3.1.1.1.1.2a" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E1.m1.3.3.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.3.cmml">i</mi><mi id="S3.E1.m1.3.3.1.1.1.1.2.2.3" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml">N</mi></munderover></mstyle><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E1.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml"><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mover accent="true" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml">C</mi><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.1.cmml">^</mo></mover><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">C</mi><mi id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.3.3.1.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.1.2" lspace="0em" xref="S3.E1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1"><eq id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.2"></eq><apply id="S3.E1.m1.3.3.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.3"><times id="S3.E1.m1.3.3.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.1"></times><apply id="S3.E1.m1.3.3.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.3.2.1.cmml" xref="S3.E1.m1.3.3.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.3.2.2.cmml" xref="S3.E1.m1.3.3.1.1.3.2.2">â„’</ci><ci id="S3.E1.m1.3.3.1.1.3.2.3.cmml" xref="S3.E1.m1.3.3.1.1.3.2.3">ğ‘€ğ´ğ¸</ci></apply><interval closure="open" id="S3.E1.m1.3.3.1.1.3.3.1.cmml" xref="S3.E1.m1.3.3.1.1.3.3.2"><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><ci id="S3.E1.m1.1.1.1.cmml" xref="S3.E1.m1.1.1.1">^</ci><ci id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1.2">ğ¶</ci></apply><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ¶</ci></interval></apply><apply id="S3.E1.m1.3.3.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1"><times id="S3.E1.m1.3.3.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.2"></times><apply id="S3.E1.m1.3.3.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3"><divide id="S3.E1.m1.3.3.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.3"></divide><cn id="S3.E1.m1.3.3.1.1.1.3.2.cmml" type="integer" xref="S3.E1.m1.3.3.1.1.1.3.2">1</cn><ci id="S3.E1.m1.3.3.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.3.3">ğ‘</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">superscript</csymbol><sum id="S3.E1.m1.3.3.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.2"></sum><ci id="S3.E1.m1.3.3.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.2.3">ğ‘</ci></apply><ci id="S3.E1.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1"><abs id="S3.E1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.2"></abs><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1"><minus id="S3.E1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.1"></minus><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2"><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.1">^</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.2.2">ğ¶</ci></apply><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.2">ğ¶</ci><ci id="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\displaystyle\mathcal{L}_{\mathit{MAE}}(\hat{C},C)=\frac{1}{N}\sum^{N}_{i}|% \hat{C}_{i}-C_{i}|.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">caligraphic_L start_POSTSUBSCRIPT italic_MAE end_POSTSUBSCRIPT ( over^ start_ARG italic_C end_ARG , italic_C ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘ start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | over^ start_ARG italic_C end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_C start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | .</annotation></semantics></math></td><td></td><td rowspan="1"><span>(1)</span></td></tr></tbody></table>

For all non-metric datasets (i.e.,Â those without reliable camera intrinsics or inconsistent scale), we normalize predictions and ground truth via the mean absolute deviation from the medianÂ (Ranftl etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib83)) before applying a loss. We further compute errors on the first and second derivatives of (canoncial) inverse depth maps at multiple scales. Let âˆ‡âˆ—subscriptâˆ‡\\nabla\_{\*}âˆ‡ start\_POSTSUBSCRIPT âˆ— end\_POSTSUBSCRIPT indicate a spatial derivative operatorÂ âˆ—\*âˆ—, such as Scharr (S)Â (Scharr etÂ al., [1997](https://arxiv.org/html/2410.02073v1#bib.bib87)) or Laplace (L), and pğ‘pitalic\_p the error norm. We define the multi-scale derivative loss over Mğ‘€Mitalic\_M scales as

<table id="A4.EGx2"><tbody id="S3.E2"><tr><td></td><td><math alttext="\displaystyle\mathcal{L}_{*,p,M}(C,\hat{C})=\frac{1}{M}\sum^{M}_{j}\frac{1}{N_%
{j}}\sum_{i}^{N_{j}}|\nabla_{*}C^{j}_{i}-\nabla_{*}\hat{C}^{j}_{i}|^{p}," display="inline" id="S3.E2.m1.6"><semantics id="S3.E2.m1.6a"><mrow id="S3.E2.m1.6.6.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1" xref="S3.E2.m1.6.6.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.3" xref="S3.E2.m1.6.6.1.1.3.cmml"><msub id="S3.E2.m1.6.6.1.1.3.2" xref="S3.E2.m1.6.6.1.1.3.2.cmml"><mi id="S3.E2.m1.6.6.1.1.3.2.2" xref="S3.E2.m1.6.6.1.1.3.2.2.cmml">â„’</mi><mrow id="S3.E2.m1.3.3.3.5" xref="S3.E2.m1.3.3.3.4.cmml"><mo id="S3.E2.m1.1.1.1.1" rspace="0em" xref="S3.E2.m1.1.1.1.1.cmml">âˆ—</mo><mo id="S3.E2.m1.3.3.3.5.1" xref="S3.E2.m1.3.3.3.4.cmml">,</mo><mi id="S3.E2.m1.2.2.2.2" xref="S3.E2.m1.2.2.2.2.cmml">p</mi><mo id="S3.E2.m1.3.3.3.5.2" xref="S3.E2.m1.3.3.3.4.cmml">,</mo><mi id="S3.E2.m1.3.3.3.3" xref="S3.E2.m1.3.3.3.3.cmml">M</mi></mrow></msub><mo id="S3.E2.m1.6.6.1.1.3.1" xref="S3.E2.m1.6.6.1.1.3.1.cmml">â¢</mo><mrow id="S3.E2.m1.6.6.1.1.3.3.2" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml"><mo id="S3.E2.m1.6.6.1.1.3.3.2.1" stretchy="false" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">(</mo><mi id="S3.E2.m1.4.4" xref="S3.E2.m1.4.4.cmml">C</mi><mo id="S3.E2.m1.6.6.1.1.3.3.2.2" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">,</mo><mover accent="true" id="S3.E2.m1.5.5" xref="S3.E2.m1.5.5.cmml"><mi id="S3.E2.m1.5.5.2" xref="S3.E2.m1.5.5.2.cmml">C</mi><mo id="S3.E2.m1.5.5.1" xref="S3.E2.m1.5.5.1.cmml">^</mo></mover><mo id="S3.E2.m1.6.6.1.1.3.3.2.3" stretchy="false" xref="S3.E2.m1.6.6.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.2" xref="S3.E2.m1.6.6.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.6.6.1.1.1" xref="S3.E2.m1.6.6.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.6.6.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.3.cmml"><mfrac id="S3.E2.m1.6.6.1.1.1.3a" xref="S3.E2.m1.6.6.1.1.1.3.cmml"><mn id="S3.E2.m1.6.6.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.3.2.cmml">1</mn><mi id="S3.E2.m1.6.6.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.3.3.cmml">M</mi></mfrac></mstyle><mo id="S3.E2.m1.6.6.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.6.6.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.6.6.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml"><munderover id="S3.E2.m1.6.6.1.1.1.1.2a" xref="S3.E2.m1.6.6.1.1.1.1.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.6.6.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.6.6.1.1.1.1.2.3" xref="S3.E2.m1.6.6.1.1.1.1.2.3.cmml">j</mi><mi id="S3.E2.m1.6.6.1.1.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.2.2.3.cmml">M</mi></munderover></mstyle><mrow id="S3.E2.m1.6.6.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.6.6.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.3.cmml"><mfrac id="S3.E2.m1.6.6.1.1.1.1.1.3a" xref="S3.E2.m1.6.6.1.1.1.1.1.3.cmml"><mn id="S3.E2.m1.6.6.1.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.3.2.cmml">1</mn><msub id="S3.E2.m1.6.6.1.1.1.1.1.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.3.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3.2.cmml">N</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.3.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3.3.cmml">j</mi></msub></mfrac></mstyle><mo id="S3.E2.m1.6.6.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.6.6.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml"><munderover id="S3.E2.m1.6.6.1.1.1.1.1.1.2a" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2.cmml">âˆ‘</mo><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3.cmml">i</mi><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.2.cmml">N</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.3.cmml">j</mi></msub></munderover></mstyle><msup id="S3.E2.m1.6.6.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1.cmml">|</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.2" rspace="0.167em" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.2.cmml">âˆ‡</mo><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.3.cmml">âˆ—</mo></msub><msubsup id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">C</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">j</mi></msubsup></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml"><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.2" rspace="0.167em" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml">âˆ‡</mo><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml">âˆ—</mo></msub><msubsup id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mover accent="true" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml"><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2.cmml">C</mi><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.1" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.3.cmml">j</mi></msubsup></mrow></mrow><mo id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1.cmml">|</mo></mrow><mi id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml">p</mi></msup></mrow></mrow></mrow></mrow></mrow><mo id="S3.E2.m1.6.6.1.2" xref="S3.E2.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.6b"><apply id="S3.E2.m1.6.6.1.1.cmml" xref="S3.E2.m1.6.6.1"><eq id="S3.E2.m1.6.6.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.2"></eq><apply id="S3.E2.m1.6.6.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.3"><times id="S3.E2.m1.6.6.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3.1"></times><apply id="S3.E2.m1.6.6.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.3.2.2">â„’</ci><list id="S3.E2.m1.3.3.3.4.cmml" xref="S3.E2.m1.3.3.3.5"><times id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1"></times><ci id="S3.E2.m1.2.2.2.2.cmml" xref="S3.E2.m1.2.2.2.2">ğ‘</ci><ci id="S3.E2.m1.3.3.3.3.cmml" xref="S3.E2.m1.3.3.3.3">ğ‘€</ci></list></apply><interval closure="open" id="S3.E2.m1.6.6.1.1.3.3.1.cmml" xref="S3.E2.m1.6.6.1.1.3.3.2"><ci id="S3.E2.m1.4.4.cmml" xref="S3.E2.m1.4.4">ğ¶</ci><apply id="S3.E2.m1.5.5.cmml" xref="S3.E2.m1.5.5"><ci id="S3.E2.m1.5.5.1.cmml" xref="S3.E2.m1.5.5.1">^</ci><ci id="S3.E2.m1.5.5.2.cmml" xref="S3.E2.m1.5.5.2">ğ¶</ci></apply></interval></apply><apply id="S3.E2.m1.6.6.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.2"></times><apply id="S3.E2.m1.6.6.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3"><divide id="S3.E2.m1.6.6.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.3"></divide><cn id="S3.E2.m1.6.6.1.1.1.3.2.cmml" type="integer" xref="S3.E2.m1.6.6.1.1.1.3.2">1</cn><ci id="S3.E2.m1.6.6.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.3.3">ğ‘€</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1"><apply id="S3.E2.m1.6.6.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2">subscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2">superscript</csymbol><sum id="S3.E2.m1.6.6.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2.2.2"></sum><ci id="S3.E2.m1.6.6.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2.2.3">ğ‘€</ci></apply><ci id="S3.E2.m1.6.6.1.1.1.1.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.2.3">ğ‘—</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1"><times id="S3.E2.m1.6.6.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.2"></times><apply id="S3.E2.m1.6.6.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3"><divide id="S3.E2.m1.6.6.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3"></divide><cn id="S3.E2.m1.6.6.1.1.1.1.1.3.2.cmml" type="integer" xref="S3.E2.m1.6.6.1.1.1.1.1.3.2">1</cn><apply id="S3.E2.m1.6.6.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.3.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.3.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3.2">ğ‘</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.3.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.3.3.3">ğ‘—</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1"><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.2"></sum><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.2">ğ‘</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.2.3.3">ğ‘—</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1"><abs id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.2"></abs><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2"><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.2">âˆ‡</ci><times id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.1.3"></times></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2">subscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.2">ğ¶</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.2.3">ğ‘—</ci></apply><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3"><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1">subscript</csymbol><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.2">âˆ‡</ci><times id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.1.3"></times></apply><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2">superscript</csymbol><apply id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2"><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.1.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.1">^</ci><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.2.2">ğ¶</ci></apply><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.2.3">ğ‘—</ci></apply><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.1.1.1.3.2.3">ğ‘–</ci></apply></apply></apply></apply><ci id="S3.E2.m1.6.6.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.6.6.1.1.1.1.1.1.1.3">ğ‘</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.6c">\displaystyle\mathcal{L}_{*,p,M}(C,\hat{C})=\frac{1}{M}\sum^{M}_{j}\frac{1}{N_% {j}}\sum_{i}^{N_{j}}|\nabla_{*}C^{j}_{i}-\nabla_{*}\hat{C}^{j}_{i}|^{p},</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.6d">caligraphic_L start_POSTSUBSCRIPT âˆ— , italic_p , italic_M end_POSTSUBSCRIPT ( italic_C , over^ start_ARG italic_C end_ARG ) = divide start_ARG 1 end_ARG start_ARG italic_M end_ARG âˆ‘ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT | âˆ‡ start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT italic_C start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - âˆ‡ start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT over^ start_ARG italic_C end_ARG start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ,</annotation></semantics></math></td><td></td><td rowspan="1"><span>(2)</span></td></tr></tbody></table>

where the scales jğ‘—jitalic\_j are computed by blurring and downsampling the inverse depth maps by a factor of 2 per scale. As shorthands we define the Mean Absolute Gradient Error â„’ğ‘€ğ´ğºğ¸\=â„’S,1,6subscriptâ„’ğ‘€ğ´ğºğ¸subscriptâ„’ğ‘†16\\mathcal{L}\_{\\mathit{MAGE}}=\\mathcal{L}\_{S,1,6}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAGE end\_POSTSUBSCRIPT = caligraphic\_L start\_POSTSUBSCRIPT italic\_S , 1 , 6 end\_POSTSUBSCRIPT, the Mean Absolute Laplace Error â„’ğ‘€ğ´ğ¿ğ¸\=â„’L,1,6subscriptâ„’ğ‘€ğ´ğ¿ğ¸subscriptâ„’ğ¿16\\mathcal{L}\_{\\mathit{MALE}}=\\mathcal{L}\_{L,1,6}caligraphic\_L start\_POSTSUBSCRIPT italic\_MALE end\_POSTSUBSCRIPT = caligraphic\_L start\_POSTSUBSCRIPT italic\_L , 1 , 6 end\_POSTSUBSCRIPT, and the Mean Squared Gradient Error â„’ğ‘€ğ‘†ğºğ¸\=â„’S,2,6subscriptâ„’ğ‘€ğ‘†ğºğ¸subscriptâ„’ğ‘†26\\mathcal{L}\_{\\mathit{MSGE}}=\\mathcal{L}\_{S,2,6}caligraphic\_L start\_POSTSUBSCRIPT italic\_MSGE end\_POSTSUBSCRIPT = caligraphic\_L start\_POSTSUBSCRIPT italic\_S , 2 , 6 end\_POSTSUBSCRIPT.

#### Training curriculum.

We propose a training curriculum motivated by the following observations. First, training on a large mix of real-world and synthetic datasets improves generalization as measured by zero-shot accuracyÂ (Ranftl etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib83); [2021](https://arxiv.org/html/2410.02073v1#bib.bib82); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112); Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)). Second, synthetic datasets provide pixel-accurate ground truth, whereas real-world datasets often contain missing areas, mismatched depth, or false measurements on object boundaries. Third, predictions get sharper over the course of training.

Based on these observations, we design a two-stage training curriculum. In the first stage, we aim to learn robust features that allow the network to generalize across domains. To that end, we train on a mix of all labeled training sets. Specifically, we minimize â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT on metric datasets and its normalized version on non-metric datasets. â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT is chosen for its robustness in handling potentially corrupted real-world ground truth. To steer the network towards sharp boundaries, we aim to also supervise on gradients of the predictions. Done naÃ¯vely, however, this can hinder optimization and slow down convergence. We found that a scale-and-shift-invariant loss on gradients, applied only to synthetic datasets, worked best. Controlled experiments are reported in the appendices.

The second stage of training is designed to sharpen boundaries and reveal fine details in the predicted depth maps. To minimize the effect of inaccurate ground truth, at this stage we only train on synthetic datasets that provide high-quality pixel-accurate ground truth. (Note that this inverts the common practice of first training on synthetic data and then fine-tuning on real dataÂ (Gaidon etÂ al., [2016](https://arxiv.org/html/2410.02073v1#bib.bib30); GÃ³mez etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib32); Sun etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib96)).) Specifically, we again minimize the â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT and supplement it with a selection of losses on the first- and second-order derivatives: â„’ğ‘€ğ´ğºğ¸subscriptâ„’ğ‘€ğ´ğºğ¸\\mathcal{L}\_{\\mathit{MAGE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAGE end\_POSTSUBSCRIPT, â„’ğ‘€ğ´ğ¿ğ¸subscriptâ„’ğ‘€ğ´ğ¿ğ¸\\mathcal{L}\_{\\mathit{MALE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MALE end\_POSTSUBSCRIPT, and â„’ğ‘€ğ‘†ğºğ¸subscriptâ„’ğ‘€ğ‘†ğºğ¸\\mathcal{L}\_{\\mathit{MSGE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MSGE end\_POSTSUBSCRIPT. We provide a detailed specification of the loss functions that are applied at each stage in the appendices.

#### Evaluation metrics for sharp boundaries.

Applications such as novel view synthesis require depth maps to adhere to object boundaries. This is particularly challenging for thin structures. Misaligned or blurry boundaries can make objects appear distorted or split into parts. Common benchmarks for monocular depth prediction rarely take boundary sharpness into account. This may be attributed in part to the lack of diverse and realistic datasets with precise pixel-accurate ground-truth depth. To address this shortcoming, we propose a new set of metrics specifically for the evaluation of depth boundaries. Our key observation is that we can leverage existing high-quality annotations for matting, saliency, or segmentation as ground truth for depth boundaries. We treat annotations for these tasks as binary maps, which define a foreground/background relationship between an object and its environment. (This relationship may not hold in every case, especially for segmentation masks. However, we can easily discard such problematic cases through manual inspection. It is much easier to filter out a segmentation mask than to annotate it.) To ensure that the relationship holds, we only consider pixels around edges in the binary map.

We first define the metrics for depth maps and later derive the formulation for binary segmentation masks. Motivated by the ranking lossÂ (Chen etÂ al., [2016](https://arxiv.org/html/2410.02073v1#bib.bib14)), we use the pairwise depth ratio of neighboring pixels to define a foreground/background relationship. Let i,jğ‘–ğ‘—i,jitalic\_i , italic\_j be the locations of two neighboring pixels. We then define an occluding contour cdsubscriptğ‘ğ‘‘c\_{d}italic\_c start\_POSTSUBSCRIPT italic\_d end\_POSTSUBSCRIPT derived from a depth map dğ‘‘ditalic\_d as cdâ¢(i,j)\=\[dâ¢(j)dâ¢(i)\>(1+t100)\]subscriptğ‘ğ‘‘ğ‘–ğ‘—delimited-\[\]ğ‘‘ğ‘—ğ‘‘ğ‘–1ğ‘¡100c\_{d}(i,j)=\\left\[\\tfrac{d(j)}{d(i)}>(1+\\tfrac{t}{100})\\right\]italic\_c start\_POSTSUBSCRIPT italic\_d end\_POSTSUBSCRIPT ( italic\_i , italic\_j ) = \[ divide start\_ARG italic\_d ( italic\_j ) end\_ARG start\_ARG italic\_d ( italic\_i ) end\_ARG > ( 1 + divide start\_ARG italic\_t end\_ARG start\_ARG 100 end\_ARG ) \], where \[â‹…\]delimited-\[\]â‹…\[\\cdot\]\[ â‹… \] is the Iverson bracket. Intuitively, this indicates the presence of an occluding contour between pixels iğ‘–iitalic\_i and jğ‘—jitalic\_j if their corresponding depth differs by more than t%percentğ‘¡t\\%italic\_t %. For all pairs of neighboring pixels, we can then compute the precision (Pğ‘ƒPitalic\_P) and recall (Rğ‘…Ritalic\_R) as

<table id="S3.E3"><tbody><tr><td></td><td><math alttext="\text{P}(t)=\frac{\sum_{i,j\in N(i)}c_{d}(i,j)\wedge c_{\hat{d}}(i,j)}{\sum_{i%
,j\in N(i)}c_{d}(i,j)}\text{ and }\text{R}(t)=\frac{\sum_{i,j\in N(i)}c_{d}(i,%
j)\wedge c_{\hat{d}}(i,j)}{\sum_{i,j\in N(i)}c_{\hat{d}}(i,j)}." display="block" id="S3.E3.m1.27"><semantics id="S3.E3.m1.27a"><mrow id="S3.E3.m1.27.27.1" xref="S3.E3.m1.27.27.1.1.cmml"><mrow id="S3.E3.m1.27.27.1.1" xref="S3.E3.m1.27.27.1.1.cmml"><mrow id="S3.E3.m1.27.27.1.1.2" xref="S3.E3.m1.27.27.1.1.2.cmml"><mtext id="S3.E3.m1.27.27.1.1.2.2" xref="S3.E3.m1.27.27.1.1.2.2a.cmml">P</mtext><mo id="S3.E3.m1.27.27.1.1.2.1" xref="S3.E3.m1.27.27.1.1.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.27.27.1.1.2.3.2" xref="S3.E3.m1.27.27.1.1.2.cmml"><mo id="S3.E3.m1.27.27.1.1.2.3.2.1" stretchy="false" xref="S3.E3.m1.27.27.1.1.2.cmml">(</mo><mi id="S3.E3.m1.25.25" xref="S3.E3.m1.25.25.cmml">t</mi><mo id="S3.E3.m1.27.27.1.1.2.3.2.2" stretchy="false" xref="S3.E3.m1.27.27.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.27.27.1.1.3" xref="S3.E3.m1.27.27.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.27.27.1.1.4" xref="S3.E3.m1.27.27.1.1.4.cmml"><mfrac id="S3.E3.m1.12.12" xref="S3.E3.m1.12.12.cmml"><mrow id="S3.E3.m1.7.7.7" xref="S3.E3.m1.7.7.7.cmml"><mrow id="S3.E3.m1.7.7.7.9" xref="S3.E3.m1.7.7.7.9.cmml"><msub id="S3.E3.m1.7.7.7.9.1" xref="S3.E3.m1.7.7.7.9.1.cmml"><mo id="S3.E3.m1.7.7.7.9.1.2" xref="S3.E3.m1.7.7.7.9.1.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.cmml"><mrow id="S3.E3.m1.3.3.3.3.3.5.2" xref="S3.E3.m1.3.3.3.3.3.5.1.cmml"><mi id="S3.E3.m1.2.2.2.2.2.2" xref="S3.E3.m1.2.2.2.2.2.2.cmml">i</mi><mo id="S3.E3.m1.3.3.3.3.3.5.2.1" xref="S3.E3.m1.3.3.3.3.3.5.1.cmml">,</mo><mi id="S3.E3.m1.3.3.3.3.3.3" xref="S3.E3.m1.3.3.3.3.3.3.cmml">j</mi></mrow><mo id="S3.E3.m1.3.3.3.3.3.4" xref="S3.E3.m1.3.3.3.3.3.4.cmml">âˆˆ</mo><mrow id="S3.E3.m1.3.3.3.3.3.6" xref="S3.E3.m1.3.3.3.3.3.6.cmml"><mi id="S3.E3.m1.3.3.3.3.3.6.2" xref="S3.E3.m1.3.3.3.3.3.6.2.cmml">N</mi><mo id="S3.E3.m1.3.3.3.3.3.6.1" xref="S3.E3.m1.3.3.3.3.3.6.1.cmml">â¢</mo><mrow id="S3.E3.m1.3.3.3.3.3.6.3.2" xref="S3.E3.m1.3.3.3.3.3.6.cmml"><mo id="S3.E3.m1.3.3.3.3.3.6.3.2.1" stretchy="false" xref="S3.E3.m1.3.3.3.3.3.6.cmml">(</mo><mi id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.cmml">i</mi><mo id="S3.E3.m1.3.3.3.3.3.6.3.2.2" stretchy="false" xref="S3.E3.m1.3.3.3.3.3.6.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S3.E3.m1.7.7.7.9.2" xref="S3.E3.m1.7.7.7.9.2.cmml"><msub id="S3.E3.m1.7.7.7.9.2.2" xref="S3.E3.m1.7.7.7.9.2.2.cmml"><mi id="S3.E3.m1.7.7.7.9.2.2.2" xref="S3.E3.m1.7.7.7.9.2.2.2.cmml">c</mi><mi id="S3.E3.m1.7.7.7.9.2.2.3" xref="S3.E3.m1.7.7.7.9.2.2.3.cmml">d</mi></msub><mo id="S3.E3.m1.7.7.7.9.2.1" xref="S3.E3.m1.7.7.7.9.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.7.7.7.9.2.3.2" xref="S3.E3.m1.7.7.7.9.2.3.1.cmml"><mo id="S3.E3.m1.7.7.7.9.2.3.2.1" stretchy="false" xref="S3.E3.m1.7.7.7.9.2.3.1.cmml">(</mo><mi id="S3.E3.m1.4.4.4.4" xref="S3.E3.m1.4.4.4.4.cmml">i</mi><mo id="S3.E3.m1.7.7.7.9.2.3.2.2" xref="S3.E3.m1.7.7.7.9.2.3.1.cmml">,</mo><mi id="S3.E3.m1.5.5.5.5" xref="S3.E3.m1.5.5.5.5.cmml">j</mi><mo id="S3.E3.m1.7.7.7.9.2.3.2.3" stretchy="false" xref="S3.E3.m1.7.7.7.9.2.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.7.7.7.8" xref="S3.E3.m1.7.7.7.8.cmml">âˆ§</mo><mrow id="S3.E3.m1.7.7.7.10" xref="S3.E3.m1.7.7.7.10.cmml"><msub id="S3.E3.m1.7.7.7.10.2" xref="S3.E3.m1.7.7.7.10.2.cmml"><mi id="S3.E3.m1.7.7.7.10.2.2" xref="S3.E3.m1.7.7.7.10.2.2.cmml">c</mi><mover accent="true" id="S3.E3.m1.7.7.7.10.2.3" xref="S3.E3.m1.7.7.7.10.2.3.cmml"><mi id="S3.E3.m1.7.7.7.10.2.3.2" xref="S3.E3.m1.7.7.7.10.2.3.2.cmml">d</mi><mo id="S3.E3.m1.7.7.7.10.2.3.1" xref="S3.E3.m1.7.7.7.10.2.3.1.cmml">^</mo></mover></msub><mo id="S3.E3.m1.7.7.7.10.1" xref="S3.E3.m1.7.7.7.10.1.cmml">â¢</mo><mrow id="S3.E3.m1.7.7.7.10.3.2" xref="S3.E3.m1.7.7.7.10.3.1.cmml"><mo id="S3.E3.m1.7.7.7.10.3.2.1" stretchy="false" xref="S3.E3.m1.7.7.7.10.3.1.cmml">(</mo><mi id="S3.E3.m1.6.6.6.6" xref="S3.E3.m1.6.6.6.6.cmml">i</mi><mo id="S3.E3.m1.7.7.7.10.3.2.2" xref="S3.E3.m1.7.7.7.10.3.1.cmml">,</mo><mi id="S3.E3.m1.7.7.7.7" xref="S3.E3.m1.7.7.7.7.cmml">j</mi><mo id="S3.E3.m1.7.7.7.10.3.2.3" stretchy="false" xref="S3.E3.m1.7.7.7.10.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E3.m1.12.12.12" xref="S3.E3.m1.12.12.12.cmml"><msub id="S3.E3.m1.12.12.12.6" xref="S3.E3.m1.12.12.12.6.cmml"><mo id="S3.E3.m1.12.12.12.6.2" xref="S3.E3.m1.12.12.12.6.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.10.10.10.3.3" xref="S3.E3.m1.10.10.10.3.3.cmml"><mrow id="S3.E3.m1.10.10.10.3.3.5.2" xref="S3.E3.m1.10.10.10.3.3.5.1.cmml"><mi id="S3.E3.m1.9.9.9.2.2.2" xref="S3.E3.m1.9.9.9.2.2.2.cmml">i</mi><mo id="S3.E3.m1.10.10.10.3.3.5.2.1" xref="S3.E3.m1.10.10.10.3.3.5.1.cmml">,</mo><mi id="S3.E3.m1.10.10.10.3.3.3" xref="S3.E3.m1.10.10.10.3.3.3.cmml">j</mi></mrow><mo id="S3.E3.m1.10.10.10.3.3.4" xref="S3.E3.m1.10.10.10.3.3.4.cmml">âˆˆ</mo><mrow id="S3.E3.m1.10.10.10.3.3.6" xref="S3.E3.m1.10.10.10.3.3.6.cmml"><mi id="S3.E3.m1.10.10.10.3.3.6.2" xref="S3.E3.m1.10.10.10.3.3.6.2.cmml">N</mi><mo id="S3.E3.m1.10.10.10.3.3.6.1" xref="S3.E3.m1.10.10.10.3.3.6.1.cmml">â¢</mo><mrow id="S3.E3.m1.10.10.10.3.3.6.3.2" xref="S3.E3.m1.10.10.10.3.3.6.cmml"><mo id="S3.E3.m1.10.10.10.3.3.6.3.2.1" stretchy="false" xref="S3.E3.m1.10.10.10.3.3.6.cmml">(</mo><mi id="S3.E3.m1.8.8.8.1.1.1" xref="S3.E3.m1.8.8.8.1.1.1.cmml">i</mi><mo id="S3.E3.m1.10.10.10.3.3.6.3.2.2" stretchy="false" xref="S3.E3.m1.10.10.10.3.3.6.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S3.E3.m1.12.12.12.7" xref="S3.E3.m1.12.12.12.7.cmml"><msub id="S3.E3.m1.12.12.12.7.2" xref="S3.E3.m1.12.12.12.7.2.cmml"><mi id="S3.E3.m1.12.12.12.7.2.2" xref="S3.E3.m1.12.12.12.7.2.2.cmml">c</mi><mi id="S3.E3.m1.12.12.12.7.2.3" xref="S3.E3.m1.12.12.12.7.2.3.cmml">d</mi></msub><mo id="S3.E3.m1.12.12.12.7.1" xref="S3.E3.m1.12.12.12.7.1.cmml">â¢</mo><mrow id="S3.E3.m1.12.12.12.7.3.2" xref="S3.E3.m1.12.12.12.7.3.1.cmml"><mo id="S3.E3.m1.12.12.12.7.3.2.1" stretchy="false" xref="S3.E3.m1.12.12.12.7.3.1.cmml">(</mo><mi id="S3.E3.m1.11.11.11.4" xref="S3.E3.m1.11.11.11.4.cmml">i</mi><mo id="S3.E3.m1.12.12.12.7.3.2.2" xref="S3.E3.m1.12.12.12.7.3.1.cmml">,</mo><mi id="S3.E3.m1.12.12.12.5" xref="S3.E3.m1.12.12.12.5.cmml">j</mi><mo id="S3.E3.m1.12.12.12.7.3.2.3" stretchy="false" xref="S3.E3.m1.12.12.12.7.3.1.cmml">)</mo></mrow></mrow></mrow></mfrac><mo id="S3.E3.m1.27.27.1.1.4.1" xref="S3.E3.m1.27.27.1.1.4.1.cmml">â¢</mo><mrow id="S3.E3.m1.27.27.1.1.4.2" xref="S3.E3.m1.27.27.1.1.4.2c.cmml"><mtext id="S3.E3.m1.27.27.1.1.4.2a" xref="S3.E3.m1.27.27.1.1.4.2c.cmml">&nbsp;and&nbsp;</mtext><mtext id="S3.E3.m1.27.27.1.1.4.2b" xref="S3.E3.m1.27.27.1.1.4.2c.cmml">R</mtext></mrow><mo id="S3.E3.m1.27.27.1.1.4.1a" xref="S3.E3.m1.27.27.1.1.4.1.cmml">â¢</mo><mrow id="S3.E3.m1.27.27.1.1.4.3.2" xref="S3.E3.m1.27.27.1.1.4.cmml"><mo id="S3.E3.m1.27.27.1.1.4.3.2.1" stretchy="false" xref="S3.E3.m1.27.27.1.1.4.cmml">(</mo><mi id="S3.E3.m1.26.26" xref="S3.E3.m1.26.26.cmml">t</mi><mo id="S3.E3.m1.27.27.1.1.4.3.2.2" stretchy="false" xref="S3.E3.m1.27.27.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.27.27.1.1.5" xref="S3.E3.m1.27.27.1.1.5.cmml">=</mo><mfrac id="S3.E3.m1.24.24" xref="S3.E3.m1.24.24.cmml"><mrow id="S3.E3.m1.19.19.7" xref="S3.E3.m1.19.19.7.cmml"><mrow id="S3.E3.m1.19.19.7.9" xref="S3.E3.m1.19.19.7.9.cmml"><msub id="S3.E3.m1.19.19.7.9.1" xref="S3.E3.m1.19.19.7.9.1.cmml"><mo id="S3.E3.m1.19.19.7.9.1.2" xref="S3.E3.m1.19.19.7.9.1.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.15.15.3.3.3" xref="S3.E3.m1.15.15.3.3.3.cmml"><mrow id="S3.E3.m1.15.15.3.3.3.5.2" xref="S3.E3.m1.15.15.3.3.3.5.1.cmml"><mi id="S3.E3.m1.14.14.2.2.2.2" xref="S3.E3.m1.14.14.2.2.2.2.cmml">i</mi><mo id="S3.E3.m1.15.15.3.3.3.5.2.1" xref="S3.E3.m1.15.15.3.3.3.5.1.cmml">,</mo><mi id="S3.E3.m1.15.15.3.3.3.3" xref="S3.E3.m1.15.15.3.3.3.3.cmml">j</mi></mrow><mo id="S3.E3.m1.15.15.3.3.3.4" xref="S3.E3.m1.15.15.3.3.3.4.cmml">âˆˆ</mo><mrow id="S3.E3.m1.15.15.3.3.3.6" xref="S3.E3.m1.15.15.3.3.3.6.cmml"><mi id="S3.E3.m1.15.15.3.3.3.6.2" xref="S3.E3.m1.15.15.3.3.3.6.2.cmml">N</mi><mo id="S3.E3.m1.15.15.3.3.3.6.1" xref="S3.E3.m1.15.15.3.3.3.6.1.cmml">â¢</mo><mrow id="S3.E3.m1.15.15.3.3.3.6.3.2" xref="S3.E3.m1.15.15.3.3.3.6.cmml"><mo id="S3.E3.m1.15.15.3.3.3.6.3.2.1" stretchy="false" xref="S3.E3.m1.15.15.3.3.3.6.cmml">(</mo><mi id="S3.E3.m1.13.13.1.1.1.1" xref="S3.E3.m1.13.13.1.1.1.1.cmml">i</mi><mo id="S3.E3.m1.15.15.3.3.3.6.3.2.2" stretchy="false" xref="S3.E3.m1.15.15.3.3.3.6.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S3.E3.m1.19.19.7.9.2" xref="S3.E3.m1.19.19.7.9.2.cmml"><msub id="S3.E3.m1.19.19.7.9.2.2" xref="S3.E3.m1.19.19.7.9.2.2.cmml"><mi id="S3.E3.m1.19.19.7.9.2.2.2" xref="S3.E3.m1.19.19.7.9.2.2.2.cmml">c</mi><mi id="S3.E3.m1.19.19.7.9.2.2.3" xref="S3.E3.m1.19.19.7.9.2.2.3.cmml">d</mi></msub><mo id="S3.E3.m1.19.19.7.9.2.1" xref="S3.E3.m1.19.19.7.9.2.1.cmml">â¢</mo><mrow id="S3.E3.m1.19.19.7.9.2.3.2" xref="S3.E3.m1.19.19.7.9.2.3.1.cmml"><mo id="S3.E3.m1.19.19.7.9.2.3.2.1" stretchy="false" xref="S3.E3.m1.19.19.7.9.2.3.1.cmml">(</mo><mi id="S3.E3.m1.16.16.4.4" xref="S3.E3.m1.16.16.4.4.cmml">i</mi><mo id="S3.E3.m1.19.19.7.9.2.3.2.2" xref="S3.E3.m1.19.19.7.9.2.3.1.cmml">,</mo><mi id="S3.E3.m1.17.17.5.5" xref="S3.E3.m1.17.17.5.5.cmml">j</mi><mo id="S3.E3.m1.19.19.7.9.2.3.2.3" stretchy="false" xref="S3.E3.m1.19.19.7.9.2.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.19.19.7.8" xref="S3.E3.m1.19.19.7.8.cmml">âˆ§</mo><mrow id="S3.E3.m1.19.19.7.10" xref="S3.E3.m1.19.19.7.10.cmml"><msub id="S3.E3.m1.19.19.7.10.2" xref="S3.E3.m1.19.19.7.10.2.cmml"><mi id="S3.E3.m1.19.19.7.10.2.2" xref="S3.E3.m1.19.19.7.10.2.2.cmml">c</mi><mover accent="true" id="S3.E3.m1.19.19.7.10.2.3" xref="S3.E3.m1.19.19.7.10.2.3.cmml"><mi id="S3.E3.m1.19.19.7.10.2.3.2" xref="S3.E3.m1.19.19.7.10.2.3.2.cmml">d</mi><mo id="S3.E3.m1.19.19.7.10.2.3.1" xref="S3.E3.m1.19.19.7.10.2.3.1.cmml">^</mo></mover></msub><mo id="S3.E3.m1.19.19.7.10.1" xref="S3.E3.m1.19.19.7.10.1.cmml">â¢</mo><mrow id="S3.E3.m1.19.19.7.10.3.2" xref="S3.E3.m1.19.19.7.10.3.1.cmml"><mo id="S3.E3.m1.19.19.7.10.3.2.1" stretchy="false" xref="S3.E3.m1.19.19.7.10.3.1.cmml">(</mo><mi id="S3.E3.m1.18.18.6.6" xref="S3.E3.m1.18.18.6.6.cmml">i</mi><mo id="S3.E3.m1.19.19.7.10.3.2.2" xref="S3.E3.m1.19.19.7.10.3.1.cmml">,</mo><mi id="S3.E3.m1.19.19.7.7" xref="S3.E3.m1.19.19.7.7.cmml">j</mi><mo id="S3.E3.m1.19.19.7.10.3.2.3" stretchy="false" xref="S3.E3.m1.19.19.7.10.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.E3.m1.24.24.12" xref="S3.E3.m1.24.24.12.cmml"><msub id="S3.E3.m1.24.24.12.6" xref="S3.E3.m1.24.24.12.6.cmml"><mo id="S3.E3.m1.24.24.12.6.2" xref="S3.E3.m1.24.24.12.6.2.cmml">âˆ‘</mo><mrow id="S3.E3.m1.22.22.10.3.3" xref="S3.E3.m1.22.22.10.3.3.cmml"><mrow id="S3.E3.m1.22.22.10.3.3.5.2" xref="S3.E3.m1.22.22.10.3.3.5.1.cmml"><mi id="S3.E3.m1.21.21.9.2.2.2" xref="S3.E3.m1.21.21.9.2.2.2.cmml">i</mi><mo id="S3.E3.m1.22.22.10.3.3.5.2.1" xref="S3.E3.m1.22.22.10.3.3.5.1.cmml">,</mo><mi id="S3.E3.m1.22.22.10.3.3.3" xref="S3.E3.m1.22.22.10.3.3.3.cmml">j</mi></mrow><mo id="S3.E3.m1.22.22.10.3.3.4" xref="S3.E3.m1.22.22.10.3.3.4.cmml">âˆˆ</mo><mrow id="S3.E3.m1.22.22.10.3.3.6" xref="S3.E3.m1.22.22.10.3.3.6.cmml"><mi id="S3.E3.m1.22.22.10.3.3.6.2" xref="S3.E3.m1.22.22.10.3.3.6.2.cmml">N</mi><mo id="S3.E3.m1.22.22.10.3.3.6.1" xref="S3.E3.m1.22.22.10.3.3.6.1.cmml">â¢</mo><mrow id="S3.E3.m1.22.22.10.3.3.6.3.2" xref="S3.E3.m1.22.22.10.3.3.6.cmml"><mo id="S3.E3.m1.22.22.10.3.3.6.3.2.1" stretchy="false" xref="S3.E3.m1.22.22.10.3.3.6.cmml">(</mo><mi id="S3.E3.m1.20.20.8.1.1.1" xref="S3.E3.m1.20.20.8.1.1.1.cmml">i</mi><mo id="S3.E3.m1.22.22.10.3.3.6.3.2.2" stretchy="false" xref="S3.E3.m1.22.22.10.3.3.6.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S3.E3.m1.24.24.12.7" xref="S3.E3.m1.24.24.12.7.cmml"><msub id="S3.E3.m1.24.24.12.7.2" xref="S3.E3.m1.24.24.12.7.2.cmml"><mi id="S3.E3.m1.24.24.12.7.2.2" xref="S3.E3.m1.24.24.12.7.2.2.cmml">c</mi><mover accent="true" id="S3.E3.m1.24.24.12.7.2.3" xref="S3.E3.m1.24.24.12.7.2.3.cmml"><mi id="S3.E3.m1.24.24.12.7.2.3.2" xref="S3.E3.m1.24.24.12.7.2.3.2.cmml">d</mi><mo id="S3.E3.m1.24.24.12.7.2.3.1" xref="S3.E3.m1.24.24.12.7.2.3.1.cmml">^</mo></mover></msub><mo id="S3.E3.m1.24.24.12.7.1" xref="S3.E3.m1.24.24.12.7.1.cmml">â¢</mo><mrow id="S3.E3.m1.24.24.12.7.3.2" xref="S3.E3.m1.24.24.12.7.3.1.cmml"><mo id="S3.E3.m1.24.24.12.7.3.2.1" stretchy="false" xref="S3.E3.m1.24.24.12.7.3.1.cmml">(</mo><mi id="S3.E3.m1.23.23.11.4" xref="S3.E3.m1.23.23.11.4.cmml">i</mi><mo id="S3.E3.m1.24.24.12.7.3.2.2" xref="S3.E3.m1.24.24.12.7.3.1.cmml">,</mo><mi id="S3.E3.m1.24.24.12.5" xref="S3.E3.m1.24.24.12.5.cmml">j</mi><mo id="S3.E3.m1.24.24.12.7.3.2.3" stretchy="false" xref="S3.E3.m1.24.24.12.7.3.1.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow><mo id="S3.E3.m1.27.27.1.2" lspace="0em" xref="S3.E3.m1.27.27.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.27b"><apply id="S3.E3.m1.27.27.1.1.cmml" xref="S3.E3.m1.27.27.1"><and id="S3.E3.m1.27.27.1.1a.cmml" xref="S3.E3.m1.27.27.1"></and><apply id="S3.E3.m1.27.27.1.1b.cmml" xref="S3.E3.m1.27.27.1"><eq id="S3.E3.m1.27.27.1.1.3.cmml" xref="S3.E3.m1.27.27.1.1.3"></eq><apply id="S3.E3.m1.27.27.1.1.2.cmml" xref="S3.E3.m1.27.27.1.1.2"><times id="S3.E3.m1.27.27.1.1.2.1.cmml" xref="S3.E3.m1.27.27.1.1.2.1"></times><ci id="S3.E3.m1.27.27.1.1.2.2a.cmml" xref="S3.E3.m1.27.27.1.1.2.2"><mtext id="S3.E3.m1.27.27.1.1.2.2.cmml" xref="S3.E3.m1.27.27.1.1.2.2">P</mtext></ci><ci id="S3.E3.m1.25.25.cmml" xref="S3.E3.m1.25.25">ğ‘¡</ci></apply><apply id="S3.E3.m1.27.27.1.1.4.cmml" xref="S3.E3.m1.27.27.1.1.4"><times id="S3.E3.m1.27.27.1.1.4.1.cmml" xref="S3.E3.m1.27.27.1.1.4.1"></times><apply id="S3.E3.m1.12.12.cmml" xref="S3.E3.m1.12.12"><divide id="S3.E3.m1.12.12.13.cmml" xref="S3.E3.m1.12.12"></divide><apply id="S3.E3.m1.7.7.7.cmml" xref="S3.E3.m1.7.7.7"><and id="S3.E3.m1.7.7.7.8.cmml" xref="S3.E3.m1.7.7.7.8"></and><apply id="S3.E3.m1.7.7.7.9.cmml" xref="S3.E3.m1.7.7.7.9"><apply id="S3.E3.m1.7.7.7.9.1.cmml" xref="S3.E3.m1.7.7.7.9.1"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.7.9.1.1.cmml" xref="S3.E3.m1.7.7.7.9.1">subscript</csymbol><sum id="S3.E3.m1.7.7.7.9.1.2.cmml" xref="S3.E3.m1.7.7.7.9.1.2"></sum><apply id="S3.E3.m1.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3"><in id="S3.E3.m1.3.3.3.3.3.4.cmml" xref="S3.E3.m1.3.3.3.3.3.4"></in><list id="S3.E3.m1.3.3.3.3.3.5.1.cmml" xref="S3.E3.m1.3.3.3.3.3.5.2"><ci id="S3.E3.m1.2.2.2.2.2.2.cmml" xref="S3.E3.m1.2.2.2.2.2.2">ğ‘–</ci><ci id="S3.E3.m1.3.3.3.3.3.3.cmml" xref="S3.E3.m1.3.3.3.3.3.3">ğ‘—</ci></list><apply id="S3.E3.m1.3.3.3.3.3.6.cmml" xref="S3.E3.m1.3.3.3.3.3.6"><times id="S3.E3.m1.3.3.3.3.3.6.1.cmml" xref="S3.E3.m1.3.3.3.3.3.6.1"></times><ci id="S3.E3.m1.3.3.3.3.3.6.2.cmml" xref="S3.E3.m1.3.3.3.3.3.6.2">ğ‘</ci><ci id="S3.E3.m1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1">ğ‘–</ci></apply></apply></apply><apply id="S3.E3.m1.7.7.7.9.2.cmml" xref="S3.E3.m1.7.7.7.9.2"><times id="S3.E3.m1.7.7.7.9.2.1.cmml" xref="S3.E3.m1.7.7.7.9.2.1"></times><apply id="S3.E3.m1.7.7.7.9.2.2.cmml" xref="S3.E3.m1.7.7.7.9.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.7.9.2.2.1.cmml" xref="S3.E3.m1.7.7.7.9.2.2">subscript</csymbol><ci id="S3.E3.m1.7.7.7.9.2.2.2.cmml" xref="S3.E3.m1.7.7.7.9.2.2.2">ğ‘</ci><ci id="S3.E3.m1.7.7.7.9.2.2.3.cmml" xref="S3.E3.m1.7.7.7.9.2.2.3">ğ‘‘</ci></apply><interval closure="open" id="S3.E3.m1.7.7.7.9.2.3.1.cmml" xref="S3.E3.m1.7.7.7.9.2.3.2"><ci id="S3.E3.m1.4.4.4.4.cmml" xref="S3.E3.m1.4.4.4.4">ğ‘–</ci><ci id="S3.E3.m1.5.5.5.5.cmml" xref="S3.E3.m1.5.5.5.5">ğ‘—</ci></interval></apply></apply><apply id="S3.E3.m1.7.7.7.10.cmml" xref="S3.E3.m1.7.7.7.10"><times id="S3.E3.m1.7.7.7.10.1.cmml" xref="S3.E3.m1.7.7.7.10.1"></times><apply id="S3.E3.m1.7.7.7.10.2.cmml" xref="S3.E3.m1.7.7.7.10.2"><csymbol cd="ambiguous" id="S3.E3.m1.7.7.7.10.2.1.cmml" xref="S3.E3.m1.7.7.7.10.2">subscript</csymbol><ci id="S3.E3.m1.7.7.7.10.2.2.cmml" xref="S3.E3.m1.7.7.7.10.2.2">ğ‘</ci><apply id="S3.E3.m1.7.7.7.10.2.3.cmml" xref="S3.E3.m1.7.7.7.10.2.3"><ci id="S3.E3.m1.7.7.7.10.2.3.1.cmml" xref="S3.E3.m1.7.7.7.10.2.3.1">^</ci><ci id="S3.E3.m1.7.7.7.10.2.3.2.cmml" xref="S3.E3.m1.7.7.7.10.2.3.2">ğ‘‘</ci></apply></apply><interval closure="open" id="S3.E3.m1.7.7.7.10.3.1.cmml" xref="S3.E3.m1.7.7.7.10.3.2"><ci id="S3.E3.m1.6.6.6.6.cmml" xref="S3.E3.m1.6.6.6.6">ğ‘–</ci><ci id="S3.E3.m1.7.7.7.7.cmml" xref="S3.E3.m1.7.7.7.7">ğ‘—</ci></interval></apply></apply><apply id="S3.E3.m1.12.12.12.cmml" xref="S3.E3.m1.12.12.12"><apply id="S3.E3.m1.12.12.12.6.cmml" xref="S3.E3.m1.12.12.12.6"><csymbol cd="ambiguous" id="S3.E3.m1.12.12.12.6.1.cmml" xref="S3.E3.m1.12.12.12.6">subscript</csymbol><sum id="S3.E3.m1.12.12.12.6.2.cmml" xref="S3.E3.m1.12.12.12.6.2"></sum><apply id="S3.E3.m1.10.10.10.3.3.cmml" xref="S3.E3.m1.10.10.10.3.3"><in id="S3.E3.m1.10.10.10.3.3.4.cmml" xref="S3.E3.m1.10.10.10.3.3.4"></in><list id="S3.E3.m1.10.10.10.3.3.5.1.cmml" xref="S3.E3.m1.10.10.10.3.3.5.2"><ci id="S3.E3.m1.9.9.9.2.2.2.cmml" xref="S3.E3.m1.9.9.9.2.2.2">ğ‘–</ci><ci id="S3.E3.m1.10.10.10.3.3.3.cmml" xref="S3.E3.m1.10.10.10.3.3.3">ğ‘—</ci></list><apply id="S3.E3.m1.10.10.10.3.3.6.cmml" xref="S3.E3.m1.10.10.10.3.3.6"><times id="S3.E3.m1.10.10.10.3.3.6.1.cmml" xref="S3.E3.m1.10.10.10.3.3.6.1"></times><ci id="S3.E3.m1.10.10.10.3.3.6.2.cmml" xref="S3.E3.m1.10.10.10.3.3.6.2">ğ‘</ci><ci id="S3.E3.m1.8.8.8.1.1.1.cmml" xref="S3.E3.m1.8.8.8.1.1.1">ğ‘–</ci></apply></apply></apply><apply id="S3.E3.m1.12.12.12.7.cmml" xref="S3.E3.m1.12.12.12.7"><times id="S3.E3.m1.12.12.12.7.1.cmml" xref="S3.E3.m1.12.12.12.7.1"></times><apply id="S3.E3.m1.12.12.12.7.2.cmml" xref="S3.E3.m1.12.12.12.7.2"><csymbol cd="ambiguous" id="S3.E3.m1.12.12.12.7.2.1.cmml" xref="S3.E3.m1.12.12.12.7.2">subscript</csymbol><ci id="S3.E3.m1.12.12.12.7.2.2.cmml" xref="S3.E3.m1.12.12.12.7.2.2">ğ‘</ci><ci id="S3.E3.m1.12.12.12.7.2.3.cmml" xref="S3.E3.m1.12.12.12.7.2.3">ğ‘‘</ci></apply><interval closure="open" id="S3.E3.m1.12.12.12.7.3.1.cmml" xref="S3.E3.m1.12.12.12.7.3.2"><ci id="S3.E3.m1.11.11.11.4.cmml" xref="S3.E3.m1.11.11.11.4">ğ‘–</ci><ci id="S3.E3.m1.12.12.12.5.cmml" xref="S3.E3.m1.12.12.12.5">ğ‘—</ci></interval></apply></apply></apply><ci id="S3.E3.m1.27.27.1.1.4.2c.cmml" xref="S3.E3.m1.27.27.1.1.4.2"><mrow id="S3.E3.m1.27.27.1.1.4.2.cmml" xref="S3.E3.m1.27.27.1.1.4.2"><mtext id="S3.E3.m1.27.27.1.1.4.2a.cmml" xref="S3.E3.m1.27.27.1.1.4.2">&nbsp;and&nbsp;</mtext><mtext id="S3.E3.m1.27.27.1.1.4.2b.cmml" xref="S3.E3.m1.27.27.1.1.4.2">R</mtext></mrow></ci><ci id="S3.E3.m1.26.26.cmml" xref="S3.E3.m1.26.26">ğ‘¡</ci></apply></apply><apply id="S3.E3.m1.27.27.1.1c.cmml" xref="S3.E3.m1.27.27.1"><eq id="S3.E3.m1.27.27.1.1.5.cmml" xref="S3.E3.m1.27.27.1.1.5"></eq><share href="https://arxiv.org/html/2410.02073v1#S3.E3.m1.27.27.1.1.4.cmml" id="S3.E3.m1.27.27.1.1d.cmml" xref="S3.E3.m1.27.27.1"></share><apply id="S3.E3.m1.24.24.cmml" xref="S3.E3.m1.24.24"><divide id="S3.E3.m1.24.24.13.cmml" xref="S3.E3.m1.24.24"></divide><apply id="S3.E3.m1.19.19.7.cmml" xref="S3.E3.m1.19.19.7"><and id="S3.E3.m1.19.19.7.8.cmml" xref="S3.E3.m1.19.19.7.8"></and><apply id="S3.E3.m1.19.19.7.9.cmml" xref="S3.E3.m1.19.19.7.9"><apply id="S3.E3.m1.19.19.7.9.1.cmml" xref="S3.E3.m1.19.19.7.9.1"><csymbol cd="ambiguous" id="S3.E3.m1.19.19.7.9.1.1.cmml" xref="S3.E3.m1.19.19.7.9.1">subscript</csymbol><sum id="S3.E3.m1.19.19.7.9.1.2.cmml" xref="S3.E3.m1.19.19.7.9.1.2"></sum><apply id="S3.E3.m1.15.15.3.3.3.cmml" xref="S3.E3.m1.15.15.3.3.3"><in id="S3.E3.m1.15.15.3.3.3.4.cmml" xref="S3.E3.m1.15.15.3.3.3.4"></in><list id="S3.E3.m1.15.15.3.3.3.5.1.cmml" xref="S3.E3.m1.15.15.3.3.3.5.2"><ci id="S3.E3.m1.14.14.2.2.2.2.cmml" xref="S3.E3.m1.14.14.2.2.2.2">ğ‘–</ci><ci id="S3.E3.m1.15.15.3.3.3.3.cmml" xref="S3.E3.m1.15.15.3.3.3.3">ğ‘—</ci></list><apply id="S3.E3.m1.15.15.3.3.3.6.cmml" xref="S3.E3.m1.15.15.3.3.3.6"><times id="S3.E3.m1.15.15.3.3.3.6.1.cmml" xref="S3.E3.m1.15.15.3.3.3.6.1"></times><ci id="S3.E3.m1.15.15.3.3.3.6.2.cmml" xref="S3.E3.m1.15.15.3.3.3.6.2">ğ‘</ci><ci id="S3.E3.m1.13.13.1.1.1.1.cmml" xref="S3.E3.m1.13.13.1.1.1.1">ğ‘–</ci></apply></apply></apply><apply id="S3.E3.m1.19.19.7.9.2.cmml" xref="S3.E3.m1.19.19.7.9.2"><times id="S3.E3.m1.19.19.7.9.2.1.cmml" xref="S3.E3.m1.19.19.7.9.2.1"></times><apply id="S3.E3.m1.19.19.7.9.2.2.cmml" xref="S3.E3.m1.19.19.7.9.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.19.19.7.9.2.2.1.cmml" xref="S3.E3.m1.19.19.7.9.2.2">subscript</csymbol><ci id="S3.E3.m1.19.19.7.9.2.2.2.cmml" xref="S3.E3.m1.19.19.7.9.2.2.2">ğ‘</ci><ci id="S3.E3.m1.19.19.7.9.2.2.3.cmml" xref="S3.E3.m1.19.19.7.9.2.2.3">ğ‘‘</ci></apply><interval closure="open" id="S3.E3.m1.19.19.7.9.2.3.1.cmml" xref="S3.E3.m1.19.19.7.9.2.3.2"><ci id="S3.E3.m1.16.16.4.4.cmml" xref="S3.E3.m1.16.16.4.4">ğ‘–</ci><ci id="S3.E3.m1.17.17.5.5.cmml" xref="S3.E3.m1.17.17.5.5">ğ‘—</ci></interval></apply></apply><apply id="S3.E3.m1.19.19.7.10.cmml" xref="S3.E3.m1.19.19.7.10"><times id="S3.E3.m1.19.19.7.10.1.cmml" xref="S3.E3.m1.19.19.7.10.1"></times><apply id="S3.E3.m1.19.19.7.10.2.cmml" xref="S3.E3.m1.19.19.7.10.2"><csymbol cd="ambiguous" id="S3.E3.m1.19.19.7.10.2.1.cmml" xref="S3.E3.m1.19.19.7.10.2">subscript</csymbol><ci id="S3.E3.m1.19.19.7.10.2.2.cmml" xref="S3.E3.m1.19.19.7.10.2.2">ğ‘</ci><apply id="S3.E3.m1.19.19.7.10.2.3.cmml" xref="S3.E3.m1.19.19.7.10.2.3"><ci id="S3.E3.m1.19.19.7.10.2.3.1.cmml" xref="S3.E3.m1.19.19.7.10.2.3.1">^</ci><ci id="S3.E3.m1.19.19.7.10.2.3.2.cmml" xref="S3.E3.m1.19.19.7.10.2.3.2">ğ‘‘</ci></apply></apply><interval closure="open" id="S3.E3.m1.19.19.7.10.3.1.cmml" xref="S3.E3.m1.19.19.7.10.3.2"><ci id="S3.E3.m1.18.18.6.6.cmml" xref="S3.E3.m1.18.18.6.6">ğ‘–</ci><ci id="S3.E3.m1.19.19.7.7.cmml" xref="S3.E3.m1.19.19.7.7">ğ‘—</ci></interval></apply></apply><apply id="S3.E3.m1.24.24.12.cmml" xref="S3.E3.m1.24.24.12"><apply id="S3.E3.m1.24.24.12.6.cmml" xref="S3.E3.m1.24.24.12.6"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.12.6.1.cmml" xref="S3.E3.m1.24.24.12.6">subscript</csymbol><sum id="S3.E3.m1.24.24.12.6.2.cmml" xref="S3.E3.m1.24.24.12.6.2"></sum><apply id="S3.E3.m1.22.22.10.3.3.cmml" xref="S3.E3.m1.22.22.10.3.3"><in id="S3.E3.m1.22.22.10.3.3.4.cmml" xref="S3.E3.m1.22.22.10.3.3.4"></in><list id="S3.E3.m1.22.22.10.3.3.5.1.cmml" xref="S3.E3.m1.22.22.10.3.3.5.2"><ci id="S3.E3.m1.21.21.9.2.2.2.cmml" xref="S3.E3.m1.21.21.9.2.2.2">ğ‘–</ci><ci id="S3.E3.m1.22.22.10.3.3.3.cmml" xref="S3.E3.m1.22.22.10.3.3.3">ğ‘—</ci></list><apply id="S3.E3.m1.22.22.10.3.3.6.cmml" xref="S3.E3.m1.22.22.10.3.3.6"><times id="S3.E3.m1.22.22.10.3.3.6.1.cmml" xref="S3.E3.m1.22.22.10.3.3.6.1"></times><ci id="S3.E3.m1.22.22.10.3.3.6.2.cmml" xref="S3.E3.m1.22.22.10.3.3.6.2">ğ‘</ci><ci id="S3.E3.m1.20.20.8.1.1.1.cmml" xref="S3.E3.m1.20.20.8.1.1.1">ğ‘–</ci></apply></apply></apply><apply id="S3.E3.m1.24.24.12.7.cmml" xref="S3.E3.m1.24.24.12.7"><times id="S3.E3.m1.24.24.12.7.1.cmml" xref="S3.E3.m1.24.24.12.7.1"></times><apply id="S3.E3.m1.24.24.12.7.2.cmml" xref="S3.E3.m1.24.24.12.7.2"><csymbol cd="ambiguous" id="S3.E3.m1.24.24.12.7.2.1.cmml" xref="S3.E3.m1.24.24.12.7.2">subscript</csymbol><ci id="S3.E3.m1.24.24.12.7.2.2.cmml" xref="S3.E3.m1.24.24.12.7.2.2">ğ‘</ci><apply id="S3.E3.m1.24.24.12.7.2.3.cmml" xref="S3.E3.m1.24.24.12.7.2.3"><ci id="S3.E3.m1.24.24.12.7.2.3.1.cmml" xref="S3.E3.m1.24.24.12.7.2.3.1">^</ci><ci id="S3.E3.m1.24.24.12.7.2.3.2.cmml" xref="S3.E3.m1.24.24.12.7.2.3.2">ğ‘‘</ci></apply></apply><interval closure="open" id="S3.E3.m1.24.24.12.7.3.1.cmml" xref="S3.E3.m1.24.24.12.7.3.2"><ci id="S3.E3.m1.23.23.11.4.cmml" xref="S3.E3.m1.23.23.11.4">ğ‘–</ci><ci id="S3.E3.m1.24.24.12.5.cmml" xref="S3.E3.m1.24.24.12.5">ğ‘—</ci></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.27c">\text{P}(t)=\frac{\sum_{i,j\in N(i)}c_{d}(i,j)\wedge c_{\hat{d}}(i,j)}{\sum_{i% ,j\in N(i)}c_{d}(i,j)}\text{ and }\text{R}(t)=\frac{\sum_{i,j\in N(i)}c_{d}(i,% j)\wedge c_{\hat{d}}(i,j)}{\sum_{i,j\in N(i)}c_{\hat{d}}(i,j)}.</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.27d">P ( italic_t ) = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i , italic_j âˆˆ italic_N ( italic_i ) end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_i , italic_j ) âˆ§ italic_c start_POSTSUBSCRIPT over^ start_ARG italic_d end_ARG end_POSTSUBSCRIPT ( italic_i , italic_j ) end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i , italic_j âˆˆ italic_N ( italic_i ) end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_i , italic_j ) end_ARG and roman_R ( italic_t ) = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i , italic_j âˆˆ italic_N ( italic_i ) end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_i , italic_j ) âˆ§ italic_c start_POSTSUBSCRIPT over^ start_ARG italic_d end_ARG end_POSTSUBSCRIPT ( italic_i , italic_j ) end_ARG start_ARG âˆ‘ start_POSTSUBSCRIPT italic_i , italic_j âˆˆ italic_N ( italic_i ) end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT over^ start_ARG italic_d end_ARG end_POSTSUBSCRIPT ( italic_i , italic_j ) end_ARG .</annotation></semantics></math></td><td></td><td rowspan="1"><span>(3)</span></td></tr></tbody></table>

Note that both Pğ‘ƒPitalic\_P and Rğ‘…Ritalic\_R are scale-invariant. In our experiments, we report the F1 score. To account for multiple relative depth ratios, we further perform a weighted averaging of the F1 values with thresholds that range linearly from tmâ¢iâ¢n\=5subscriptğ‘¡ğ‘šğ‘–ğ‘›5t\_{min}=5italic\_t start\_POSTSUBSCRIPT italic\_m italic\_i italic\_n end\_POSTSUBSCRIPT = 5 to tmâ¢aâ¢x\=25subscriptğ‘¡ğ‘šğ‘ğ‘¥25t\_{max}=25italic\_t start\_POSTSUBSCRIPT italic\_m italic\_a italic\_x end\_POSTSUBSCRIPT = 25, with stronger weights towards high threshold values. Compared to other edge-based metrics (such as the edge accuracy and completion from iBimsÂ (Koch etÂ al., [2018](https://arxiv.org/html/2410.02073v1#bib.bib50))), our metric does not require any manual edge annotation, but simply pixelwise ground truth, which is easily obtained for synthetic datasets.

Similarly, we can also identify occluding contours from binary label maps that can be derived from real-world segmentation, saliency, and matting datasets. Given a binary mask bğ‘bitalic\_b over the image, we define the presence of an occluding contour cbsubscriptğ‘ğ‘c\_{b}italic\_c start\_POSTSUBSCRIPT italic\_b end\_POSTSUBSCRIPT between pixels i,jğ‘–ğ‘—i,jitalic\_i , italic\_j as cbâ¢(i,j)\=bâ¢(i)âˆ§Â¬bâ¢(j)subscriptğ‘ğ‘ğ‘–ğ‘—ğ‘ğ‘–ğ‘ğ‘—c\_{b}(i,j)=b(i)\\land\\lnot b(j)italic\_c start\_POSTSUBSCRIPT italic\_b end\_POSTSUBSCRIPT ( italic\_i , italic\_j ) = italic\_b ( italic\_i ) âˆ§ Â¬ italic\_b ( italic\_j ). With this definition at hand, we compute the recall Râ¢(t)Rğ‘¡\\text{R}(t)R ( italic\_t ) by replacing the occluding contours from depth maps in Eq.Â [3](https://arxiv.org/html/2410.02073v1#S3.E3 "In Evaluation metrics for sharp boundaries. â€£ 3.2 Sharp monocular depth estimation â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") with those from binary maps. Since the binary maps commonly label whole objects, we cannot obtain ground-truth occluding contours that do not align with object silhouettes. Thus the boundary annotation is incompleteÂ â€“ some but not all occluding contours are identified by this procedure. Therefore we can only compute the recall but not the precision for binary maps.

To penalize blurry edges, we suppress non-maximum values of cd^subscriptğ‘^ğ‘‘c\_{\\hat{d}}italic\_c start\_POSTSUBSCRIPT over^ start\_ARG italic\_d end\_ARG end\_POSTSUBSCRIPT within the valid bounds of cd^â¢(i,j)subscriptğ‘^ğ‘‘ğ‘–ğ‘—c\_{\\hat{d}}(i,j)italic\_c start\_POSTSUBSCRIPT over^ start\_ARG italic\_d end\_ARG end\_POSTSUBSCRIPT ( italic\_i , italic\_j ) connected components. For additional experiments and qualitative results we refer to the appendices.

### 3.3 Focal length estimation

To handle images that may have inaccurate or missing EXIF metadata, we supplement our network with a focal length estimation head. A small convolutional head ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field-of-view. We use â„’2subscriptâ„’2\\mathcal{L}\_{2}caligraphic\_L start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT as the training loss. We train the focal length head and the ViT encoder after the depth estimation training. Separating the focal length training has several benefits over joint training with the depth network. It avoids the necessity of balancing the depth and focal length training objectives. It also allows training the focal length head on a different set of datasets, excluding some narrow-domain single-camera datasets that are used in training the depth estimation network, and adding large-scale image datasets that provide focal length supervision but no depth supervision. Further details are provided in the appendices.

## 4 Experiments

This section summarizes the key results. Additional details and experiments are reported in the appendices, including details on datasets, hyperparameters, experimental protocols, and the comparison of runtimes, which is summarized in Fig.Â [2](https://arxiv.org/html/2410.02073v1#S1.F2 "Figure 2 â€£ 1 Introduction â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). The appendices also report controlled experiments, including controlled studies on network architectures, training objectives, and training curricula.

Here we summarize a number of key comparisons of Depth Pro to state-of-the-art metric monocular depth estimation systems. One challenge in conducting such a comparison is that many leading recent systems are trained on bespoke combinations of datasets. Some systems use proprietary datasets that are not publicly available, and some use datasets that are only available under restrictive licenses. Some recent systems also train on unlabeled datasets or incorporate pretrained models (e.g., diffusion models) that were trained on additional massive datasets. This rules out the possibility of a comparison that controls for training data (e.g., only comparing to systems that use the same datasets we do). At this stage of this research area, the only feasible comparison to other leading cross-domain monocular depth estimation models is on a full system-to-system basis. Fully trained models (each trained on a large, partially overlapping and partially distinct collection of datasets) are compared to each other zero-shot on datasets that none of the compared systems trained on.

Zero-shot metric depth. We evaluate our methodâ€™s ability to predict zero-shot _metric_ depth and compare against the state of the art in Tab.Â [1](https://arxiv.org/html/2410.02073v1#S4.T1 "Table 1 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). Our baselines include Depth AnythingÂ (Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112)), Metric3DÂ (Yin etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib116)), PatchFusionÂ (Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)), UniDepthÂ (Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)), ZeroDepthÂ (Guizilini etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib35)) and ZoeDepthÂ (Bhat etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib6)). We also report results for the very recent Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)) and Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)).

As an overall summary measure of metric depth accuracy, Tab.Â [1](https://arxiv.org/html/2410.02073v1#S4.T1 "Table 1 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") uses the Î´1subscriptğ›¿1\\delta\_{1}italic\_Î´ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT metricÂ (Ladicky etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib52)), which is commonly used for this purposeÂ (Yin etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib116); Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112); Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)). It is defined as the percentage of inlier pixels, for which the predicted and ground-truth depths are within 25% of each other. We picked this metric for its robustness, with the strictest threshold found in the literature (25%percent2525\\%25 %).

Corresponding tables for additional metrics can be found in Sec.Â [A.2](https://arxiv.org/html/2410.02073v1#A1.SS2 "A.2 Zero-shot metric depth â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") of the appendices, including ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™\\mathit{AbsRel}italic\_AbsRelÂ (Ladicky etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib52)), ğ¿ğ‘œğ‘”10subscriptğ¿ğ‘œğ‘”10\\mathit{Log}\_{10}italic\_Log start\_POSTSUBSCRIPT 10 end\_POSTSUBSCRIPT, Î´2subscriptğ›¿2\\delta\_{2}italic\_Î´ start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT and Î´3subscriptğ›¿3\\delta\_{3}italic\_Î´ start\_POSTSUBSCRIPT 3 end\_POSTSUBSCRIPT scores, as well as point-cloud metricsÂ (Spencer etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib93)). Tab.Â [1](https://arxiv.org/html/2410.02073v1#S4.T1 "Table 1 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") also reports the average rank of each method across datasets, a common way to summarize cross-dataset performanceÂ (Ranftl etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib83)).

We report results on BoosterÂ (Ramirez etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib81)), MiddleburyÂ (Scharstein etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib88)), Sun-RGBDÂ (Song etÂ al., [2015](https://arxiv.org/html/2410.02073v1#bib.bib92)), ETH3DÂ (SchÃ¶ps etÂ al., [2017](https://arxiv.org/html/2410.02073v1#bib.bib89)), nuScenesÂ (Caesar etÂ al., [2020](https://arxiv.org/html/2410.02073v1#bib.bib11)), and SintelÂ (Butler etÂ al., [2012](https://arxiv.org/html/2410.02073v1#bib.bib9)), because, to our knowledge, they were never used in training any of the evaluated systems. Despite our best efforts, we were not able to run ZeroDepth on Booster, Middlebury, or Sun-RGBD as it consistently ran out of memory due to the high image resolutions. More details on our evaluation setup can be found in Sec.Â [C](https://arxiv.org/html/2410.02073v1#A3 "Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") of the appendix.

The results in Tab.Â [1](https://arxiv.org/html/2410.02073v1#S4.T1 "Table 1 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") confirm the findings of Piccinelli etÂ al. ([2024](https://arxiv.org/html/2410.02073v1#bib.bib76)), who observed considerable domain bias in some of the leading metric depth estimation models. Notably, Depth Anything v1Â &Â v2 focus on _relative_ depth estimation; for metric depth, they provide different models for different domains, fine-tuned either for indoor or for outdoor scenes. Metric3D v1Â &Â v2 provide domain-invariant models, but their performance depends strongly on careful selection of the crop size at test time, which is performed _per domain_ in their experiments and thus violates the zero-shot premise. We tried setting the crop size automatically based on the aspect ratio of the image, but this substantially degraded the performance of Metric3D; for this reason, we use the recommended non-zero-shot protocol, with the recommended per-domain crop sizes. Since domain-specific models and crop sizes violate the strict zero-shot premise we (and other baselines) operate under, we mark the Depth Anything and Metric3D results in gray in Tab.Â [1](https://arxiv.org/html/2410.02073v1#S4.T1 "Table 1 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second").

We find that Depth Pro demonstrates the strongest generalization by consistently scoring among the top approaches per dataset and obtaining the best average rank across all datasets.

Table 1: Zero-shot metric depth accuracy. We report the Î´1subscriptğ›¿1\\delta\_{1}italic\_Î´ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT score per dataset (higher is better) and aggregate performance across datasets via the average rank (lower is better). Methods in gray are not strictly zero-shot. Results on additional metrics and datasets are presented in the appendices.

Table 2: Zero-shot boundary accuracy. We report the F1 score for dataset with ground-truth depth, and boundary recall (Rğ‘…Ritalic\_R) for matting and segmentation datasets. Qualitative results are shown on a sample from the AM-2k datasetÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)). Higher is better for all metrics.

<table id="S4.T2.14"><tbody><tr id="S4.T2.14.6"><td id="S4.T2.9.1.1"><img alt="[Uncaptioned image]" height="120" id="S4.T2.9.1.1.g1" src="chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x4.png" width="120"></td><td id="S4.T2.10.2.2"><img alt="[Uncaptioned image]" height="120" id="S4.T2.10.2.2.g1" src="chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x5.png" width="120"></td><td id="S4.T2.11.3.3"><img alt="[Uncaptioned image]" height="120" id="S4.T2.11.3.3.g1" src="chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x6.png" width="120"></td><td id="S4.T2.12.4.4"><img alt="[Uncaptioned image]" height="120" id="S4.T2.12.4.4.g1" src="chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x7.png" width="120"></td><td id="S4.T2.13.5.5"><img alt="[Uncaptioned image]" height="120" id="S4.T2.13.5.5.g1" src="chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x8.png" width="120"></td><td id="S4.T2.14.6.6"><img alt="[Uncaptioned image]" height="120" id="S4.T2.14.6.6.g1" src="chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x9.png" width="120"></td></tr><tr id="S4.T2.14.7.1"><td id="S4.T2.14.7.1.1">Image</td><td id="S4.T2.14.7.1.2">Alpha Matte</td><td id="S4.T2.14.7.1.3">Depth Pro (Ours)</td><td id="S4.T2.14.7.1.4">DepthAnything v2</td><td id="S4.T2.14.7.1.5">PatchFusion</td><td id="S4.T2.14.7.1.6">Marigold</td></tr></tbody></table>

Zero-shot boundaries. Tab.Â [2](https://arxiv.org/html/2410.02073v1#S4.T2 "Table 2 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") summarizes the evaluation of boundary accuracy for Depth Pro and a number of baselines. This evaluation is conducted in a zero-shot setting: models are only evaluated on datasets that were not seen during training. Since our boundary metrics are scale-invariant, our baselines here also include methods that only predict relative (rather than absolute metric) depth. Our absolute baselines include Metric3DÂ (Yin etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib116)), Metric3D v2 (â€˜giantâ€™ model)Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), PatchFusionÂ (Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)), UniDepthÂ (Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)), and ZoeDepthÂ (Bhat etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib6)). We also report results for the relative variants of Depth Anything v1Â &Â v2Â (Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112); [b](https://arxiv.org/html/2410.02073v1#bib.bib113)) because they yield sharper boundaries than their metric counterparts. Lastly, we include MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), a recent diffusion-based relative depth model that became popular due to its high-fidelity predictions. We use the boundary metrics introduced in Sec.Â [3.2](https://arxiv.org/html/2410.02073v1#S3.SS2.SSS0.Px3 "Evaluation metrics for sharp boundaries. â€£ 3.2 Sharp monocular depth estimation â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), and report the average boundary F1 score for datasets with ground-truth depth, and boundary recall (Rğ‘…Ritalic\_R) for datasets with matting or segmentation annotations. For image matting datasets, a pixel is marked as occluding when the value of the alpha matte is above 0.10.10.10.1.

The datasets include SintelÂ (Butler etÂ al., [2012](https://arxiv.org/html/2410.02073v1#bib.bib9)) and SpringÂ (Mehl etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib68)), which are synthetic. We also include the iBims datasetÂ (Koch etÂ al., [2018](https://arxiv.org/html/2410.02073v1#bib.bib50)) which is often used specifically to evaluate depth boundaries, despite having low resolution. We refer to the appendices for a full slate of iBims-specific metrics. To evaluate high-frequency structures encountered in natural images (such as hair or fur), we use AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)) and P3M-10kÂ (Li etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib55)), which are high-resolution image matting datasets that were used to evaluate image matting modelsÂ (Li etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib57)). Additionally, we further report results on the DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)) image segmentation dataset. This is an object segmentation dataset that provides highly accurate binary masks across diverse images. We manually remove samples in which the segmented object is occluded by foreground objects. Fig.Â [2](https://arxiv.org/html/2410.02073v1#S1.F2 "Figure 2 â€£ 1 Introduction â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") visually summarizes the boundary recall metric on the AM-2k dataset, as a function of runtime.

We find that Depth Pro produces more accurate boundaries than all baselines on all datasets, by a significant margin. As can be observed in Fig.Â [1](https://arxiv.org/html/2410.02073v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), in the images in Tab.Â [2](https://arxiv.org/html/2410.02073v1#S4.T2 "Table 2 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), and the additional results in Sec.Â [A](https://arxiv.org/html/2410.02073v1#A1 "Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), the competitive metric accuracy of the recent Metric3D v2 and Depth Anything v2 models does not imply sharp boundaries. Depth Pro has a consistently higher recall for thin structures like hair and fur and yields sharper boundaries. This is also true in comparison to the diffusion-based Marigold, which leverages a prior trained on billions of real-word images, as well as PatchFusion, which operates at variable resolution. Note that the runtime of Depth Pro is orders of magnitude faster than Marigold and PatchFusion (see Fig.Â [2](https://arxiv.org/html/2410.02073v1#S1.F2 "Figure 2 â€£ 1 Introduction â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") & Tab.Â [5](https://arxiv.org/html/2410.02073v1#A1.T5 "Table 5 â€£ A.3 Runtime â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")). Fig.Â [4](https://arxiv.org/html/2410.02073v1#S4.F4 "Figure 4 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") demonstrates the benefits of sharp boundary prediction for novel view synthesis from a single image.

Figure 4: Impact on novel view synthesis. We plug depth maps produced by Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)), and Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)) into a recent publicly available novel view synthesis systemÂ (Khan etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib46)). We demonstrate results on images from AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)) (1st & 3rd column) and DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)) (2nd column). Depth Pro produces sharper and more accurate depth maps, yielding cleaner synthesized views. Depth Anything v2 and Metric3D v2 suffer from misalignment between the input images and estimated depth maps, resulting in foreground pixels bleeding into the background. Marigold is considerably slower than Depth Pro and produces less accurate boundaries, yielding artifacts in synthesized images. Zoom in for detail.

Focal length estimation. Previous workÂ (Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76); Kocabas etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib49); Baradad & Torralba, [2020](https://arxiv.org/html/2410.02073v1#bib.bib2)) does not provide comprehensive systematic evaluations of focal length estimators on in-the-wild images. To address this, we curated a _zero-shot_ test dataset. To this end, we selected diverse datasets with intact EXIF data, enabling reliable assessment of focal length estimation accuracy. FiveKÂ (Bychkovsky etÂ al., [2011](https://arxiv.org/html/2410.02073v1#bib.bib10)), DDDPÂ (Abuolaim & Brown, [2020](https://arxiv.org/html/2410.02073v1#bib.bib1)), and RAISEÂ (Dang-Nguyen etÂ al., [2015](https://arxiv.org/html/2410.02073v1#bib.bib18)) contribute professional-grade photographs taken with SLR cameras. SPAQÂ (Fang etÂ al., [2020](https://arxiv.org/html/2410.02073v1#bib.bib28)) provides casual photographs from mobile phones. PPR10KÂ (Liang etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib63)) provides high-quality portrait images. Finally, ZOOMÂ (Zhang etÂ al., [2019](https://arxiv.org/html/2410.02073v1#bib.bib122)) includes sets of scenes captured at various optical zoom levels.

Table 3: Comparison on focal length estimation. We report Î´25%subscriptğ›¿percent25\\delta\_{25\\%}italic\_Î´ start\_POSTSUBSCRIPT 25 % end\_POSTSUBSCRIPT and Î´50%subscriptğ›¿percent50\\delta\_{50\\%}italic\_Î´ start\_POSTSUBSCRIPT 50 % end\_POSTSUBSCRIPT for each dataset, i.e., the percentage of images with relative error (focal length in mm) less than 25% and 50%, respectively.

Tab.Â [3](https://arxiv.org/html/2410.02073v1#S4.T3 "Table 3 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") compares Depth Pro against state-of-the-art focal length estimators and shows the percentage of images with relative estimation error under 25% and 50%, respectively. Depth Pro is the most accurate across all datasets. For example, on PPR10K, a dataset of human portraits, our method leads with 64.6% of the images having a focal length error below 25%, while the second-best method, SPEC, only achieves 34.6% on this metric. We attribute this superior performance to our network design and training protocol, which decouple training of the focal length estimator from the depth network, enabling us to use different training sets for these two tasks. Further controlled experiments are reported in the appendices.

## 5 Conclusion & limitations

Depth Pro produces high-resolution metric depth maps with high-frequency detail at sub-second runtimes. Our model achieves state-of-the-art zero-shot metric depth estimation accuracy without requiring metadata such as camera intrinsics, and traces out occlusion boundaries in unprecedented detail, facilitating applications such as novel view synthesis from single images â€˜in the wildâ€™. While Depth Pro outperforms prior work along multiple dimensions, it is not without limitations. For example, the model is limited in dealing with translucent surfaces and volumetric scattering, where the definition of single pixel depth is ill-posed and ambiguous.

## Supplemental material

In SectionÂ [A](https://arxiv.org/html/2410.02073v1#A1 "Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), we provide additional results and experiments. Sec.Â [A.1](https://arxiv.org/html/2410.02073v1#A1.SS1 "A.1 Qualitative results â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") presents further qualitative comparisons to baselines, Sec.[A.2](https://arxiv.org/html/2410.02073v1#A1.SS2 "A.2 Zero-shot metric depth â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") presents a more detailed zero-shot evaluation, Sec.[A.3](https://arxiv.org/html/2410.02073v1#A1.SS3 "A.3 Runtime â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") lists runtimes for all evaluated methods, and Sec.[A.4](https://arxiv.org/html/2410.02073v1#A1.SS4 "A.4 Boundary experiments â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") presents additional experiments on boundary accuracy. SectionÂ [B](https://arxiv.org/html/2410.02073v1#A2 "Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") showcases a selection of controlled experiments on Depth Pro that helped guide architectural choices (Sec.[B.1](https://arxiv.org/html/2410.02073v1#A2.SS1 "B.1 Network backbone â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), Sec.[B.2](https://arxiv.org/html/2410.02073v1#A2.SS2 "B.2 High-resolution alternatives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), and Sec.[B.5](https://arxiv.org/html/2410.02073v1#A2.SS5 "B.5 Focal length estimation â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")), training objective design (Sec.[B.3](https://arxiv.org/html/2410.02073v1#A2.SS3 "B.3 Training objectives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")), and curriculum training (Sec.[B.4](https://arxiv.org/html/2410.02073v1#A2.SS4 "B.4 Full curricula â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")). In SectionÂ [C](https://arxiv.org/html/2410.02073v1#A3 "Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), we provide additional implementation, training and evaluation details, including a complete summary of the datasets that were involved in this paper. Finally, SectionÂ [D](https://arxiv.org/html/2410.02073v1#A4 "Appendix D Applications â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") provides additional material on downstream applications.

## Appendix A Additional Results

### A.1 Qualitative results

<table id="A1.F5.20"><tbody><tr id="A1.F5.20.21.1"><td id="A1.F5.20.21.1.1">Input Image</td><td id="A1.F5.20.21.1.2">Depth Pro (Ours)</td><td id="A1.F5.20.21.1.3">Depth Anything v2</td><td id="A1.F5.20.21.1.4">Marigold</td><td id="A1.F5.20.21.1.5">Metric3D v2</td><td id="A1.F5.20.21.1.6"></td></tr><tr id="A1.F5.5.5"><td id="A1.F5.1.1.1"></td><td id="A1.F5.2.2.2"></td><td id="A1.F5.3.3.3"></td><td id="A1.F5.4.4.4"></td><td id="A1.F5.5.5.5"></td><td id="A1.F5.5.5.6"></td></tr><tr id="A1.F5.20.22.2"><td id="A1.F5.20.22.2.1"></td><td id="A1.F5.20.22.2.2"></td><td id="A1.F5.20.22.2.3"></td><td id="A1.F5.20.22.2.4"></td><td id="A1.F5.20.22.2.5"></td><td id="A1.F5.20.22.2.6"></td></tr><tr id="A1.F5.10.10"><td id="A1.F5.6.6.1"></td><td id="A1.F5.7.7.2"></td><td id="A1.F5.8.8.3"></td><td id="A1.F5.9.9.4"></td><td id="A1.F5.10.10.5"></td><td id="A1.F5.10.10.6"></td></tr><tr id="A1.F5.20.23.3"><td id="A1.F5.20.23.3.1"></td><td id="A1.F5.20.23.3.2"></td><td id="A1.F5.20.23.3.3"></td><td id="A1.F5.20.23.3.4"></td><td id="A1.F5.20.23.3.5"></td><td id="A1.F5.20.23.3.6"></td></tr><tr id="A1.F5.15.15"><td id="A1.F5.11.11.1"></td><td id="A1.F5.12.12.2"></td><td id="A1.F5.13.13.3"></td><td id="A1.F5.14.14.4"></td><td id="A1.F5.15.15.5"></td><td id="A1.F5.15.15.6"></td></tr><tr id="A1.F5.20.24.4"><td id="A1.F5.20.24.4.1"></td><td id="A1.F5.20.24.4.2"></td><td id="A1.F5.20.24.4.3"></td><td id="A1.F5.20.24.4.4"></td><td id="A1.F5.20.24.4.5"></td><td id="A1.F5.20.24.4.6"></td></tr><tr id="A1.F5.20.20"><td id="A1.F5.16.16.1"></td><td id="A1.F5.17.17.2"></td><td id="A1.F5.18.18.3"></td><td id="A1.F5.19.19.4"></td><td id="A1.F5.20.20.5"></td><td id="A1.F5.20.20.6"></td></tr><tr id="A1.F5.20.25.5"><td id="A1.F5.20.25.5.1"></td><td id="A1.F5.20.25.5.2"></td><td id="A1.F5.20.25.5.3"></td><td id="A1.F5.20.25.5.4"></td><td id="A1.F5.20.25.5.5"></td><td id="A1.F5.20.25.5.6"></td></tr></tbody></table>

Figure 5: Zero-shot results of Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), and Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)) on images from UnsplashÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), and DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)).

<table id="A1.F6.20"><tbody><tr id="A1.F6.20.21.1"><td id="A1.F6.20.21.1.1">Input Image</td><td id="A1.F6.20.21.1.2">Depth Pro (Ours)</td><td id="A1.F6.20.21.1.3">Depth Anything v2</td><td id="A1.F6.20.21.1.4">Marigold</td><td id="A1.F6.20.21.1.5">Metric3D v2</td><td id="A1.F6.20.21.1.6"></td></tr><tr id="A1.F6.5.5"><td id="A1.F6.1.1.1"></td><td id="A1.F6.2.2.2"></td><td id="A1.F6.3.3.3"></td><td id="A1.F6.4.4.4"></td><td id="A1.F6.5.5.5"></td><td id="A1.F6.5.5.6"></td></tr><tr id="A1.F6.20.22.2"><td id="A1.F6.20.22.2.1"></td><td id="A1.F6.20.22.2.2"></td><td id="A1.F6.20.22.2.3"></td><td id="A1.F6.20.22.2.4"></td><td id="A1.F6.20.22.2.5"></td><td id="A1.F6.20.22.2.6"></td></tr><tr id="A1.F6.10.10"><td id="A1.F6.6.6.1"></td><td id="A1.F6.7.7.2"></td><td id="A1.F6.8.8.3"></td><td id="A1.F6.9.9.4"></td><td id="A1.F6.10.10.5"></td><td id="A1.F6.10.10.6"></td></tr><tr id="A1.F6.20.23.3"><td id="A1.F6.20.23.3.1"></td><td id="A1.F6.20.23.3.2"></td><td id="A1.F6.20.23.3.3"></td><td id="A1.F6.20.23.3.4"></td><td id="A1.F6.20.23.3.5"></td><td id="A1.F6.20.23.3.6"></td></tr><tr id="A1.F6.15.15"><td id="A1.F6.11.11.1"></td><td id="A1.F6.12.12.2"></td><td id="A1.F6.13.13.3"></td><td id="A1.F6.14.14.4"></td><td id="A1.F6.15.15.5"></td><td id="A1.F6.15.15.6"></td></tr><tr id="A1.F6.20.24.4"><td id="A1.F6.20.24.4.1"></td><td id="A1.F6.20.24.4.2"></td><td id="A1.F6.20.24.4.3"></td><td id="A1.F6.20.24.4.4"></td><td id="A1.F6.20.24.4.5"></td><td id="A1.F6.20.24.4.6"></td></tr><tr id="A1.F6.20.20"><td id="A1.F6.16.16.1"></td><td id="A1.F6.17.17.2"></td><td id="A1.F6.18.18.3"></td><td id="A1.F6.19.19.4"></td><td id="A1.F6.20.20.5"></td><td id="A1.F6.20.20.6"></td></tr><tr id="A1.F6.20.25.5"><td id="A1.F6.20.25.5.1"></td><td id="A1.F6.20.25.5.2"></td><td id="A1.F6.20.25.5.3"></td><td id="A1.F6.20.25.5.4"></td><td id="A1.F6.20.25.5.5"></td><td id="A1.F6.20.25.5.6"></td></tr></tbody></table>

Figure 6: Zero-shot results of Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), and Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)) on images from UnsplashÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), and DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)).

<table id="A1.F7.20"><tbody><tr id="A1.F7.20.21.1"><td id="A1.F7.20.21.1.1">Input Image</td><td id="A1.F7.20.21.1.2">Depth Pro (Ours)</td><td id="A1.F7.20.21.1.3">Depth Anything v2</td><td id="A1.F7.20.21.1.4">Marigold</td><td id="A1.F7.20.21.1.5">Metric3D v2</td><td id="A1.F7.20.21.1.6"></td></tr><tr id="A1.F7.5.5"><td id="A1.F7.1.1.1"></td><td id="A1.F7.2.2.2"></td><td id="A1.F7.3.3.3"></td><td id="A1.F7.4.4.4"></td><td id="A1.F7.5.5.5"></td><td id="A1.F7.5.5.6"></td></tr><tr id="A1.F7.20.22.2"><td id="A1.F7.20.22.2.1"></td><td id="A1.F7.20.22.2.2"></td><td id="A1.F7.20.22.2.3"></td><td id="A1.F7.20.22.2.4"></td><td id="A1.F7.20.22.2.5"></td><td id="A1.F7.20.22.2.6"></td></tr><tr id="A1.F7.10.10"><td id="A1.F7.6.6.1"></td><td id="A1.F7.7.7.2"></td><td id="A1.F7.8.8.3"></td><td id="A1.F7.9.9.4"></td><td id="A1.F7.10.10.5"></td><td id="A1.F7.10.10.6"></td></tr><tr id="A1.F7.20.23.3"><td id="A1.F7.20.23.3.1"></td><td id="A1.F7.20.23.3.2"></td><td id="A1.F7.20.23.3.3"></td><td id="A1.F7.20.23.3.4"></td><td id="A1.F7.20.23.3.5"></td><td id="A1.F7.20.23.3.6"></td></tr><tr id="A1.F7.15.15"><td id="A1.F7.11.11.1"></td><td id="A1.F7.12.12.2"></td><td id="A1.F7.13.13.3"></td><td id="A1.F7.14.14.4"></td><td id="A1.F7.15.15.5"></td><td id="A1.F7.15.15.6"></td></tr><tr id="A1.F7.20.24.4"><td id="A1.F7.20.24.4.1"></td><td id="A1.F7.20.24.4.2"></td><td id="A1.F7.20.24.4.3"></td><td id="A1.F7.20.24.4.4"></td><td id="A1.F7.20.24.4.5"></td><td id="A1.F7.20.24.4.6"></td></tr><tr id="A1.F7.20.20"><td id="A1.F7.16.16.1"></td><td id="A1.F7.17.17.2"></td><td id="A1.F7.18.18.3"></td><td id="A1.F7.19.19.4"></td><td id="A1.F7.20.20.5"></td><td id="A1.F7.20.20.6"></td></tr><tr id="A1.F7.20.25.5"><td id="A1.F7.20.25.5.1"></td><td id="A1.F7.20.25.5.2"></td><td id="A1.F7.20.25.5.3"></td><td id="A1.F7.20.25.5.4"></td><td id="A1.F7.20.25.5.5"></td><td id="A1.F7.20.25.5.6"></td></tr></tbody></table>

Figure 7: Zero-shot results of Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), and Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)) on images from UnsplashÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), and DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)).

We provide additional qualitative results of Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), and Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)) on in-the-wild images from AM-2kÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)), DIS-5kÂ (Qin etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib77)), and Unsplash<sup>1</sup> in Fig.Â [5](https://arxiv.org/html/2410.02073v1#A1.F5 "Figure 5 â€£ A.1 Qualitative results â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), Fig.Â [6](https://arxiv.org/html/2410.02073v1#A1.F6 "Figure 6 â€£ A.1 Qualitative results â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), and Fig.Â [7](https://arxiv.org/html/2410.02073v1#A1.F7 "Figure 7 â€£ A.1 Qualitative results â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). Fine details are repeatedly missed by Metric3DÂ v2 and Depth Anything v2. Marigold reproduces finer details than Metric3DÂ v2 and Depth Anything v2, but commonly yields noisy predictions.

### A.2 Zero-shot metric depth

Expanding on the summary in Tab.[1](https://arxiv.org/html/2410.02073v1#S4.T1 "Table 1 â€£ 4 Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), we provide additional results for zero-shot metric depth estimation in Tab.Â [4](https://arxiv.org/html/2410.02073v1#A1.T4 "Table 4 â€£ A.2 Zero-shot metric depth â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). We report results on BoosterÂ (Ramirez etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib81)), MiddleburyÂ (Scharstein etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib88)), Sun-RGBDÂ (Song etÂ al., [2015](https://arxiv.org/html/2410.02073v1#bib.bib92)), ETH3DÂ (SchÃ¶ps etÂ al., [2017](https://arxiv.org/html/2410.02073v1#bib.bib89)), nuScenesÂ (Caesar etÂ al., [2020](https://arxiv.org/html/2410.02073v1#bib.bib11)), and SintelÂ (Butler etÂ al., [2012](https://arxiv.org/html/2410.02073v1#bib.bib9)). Our baselines include Depth AnythingÂ (Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112)) and Depth Anything v2Â (Yang etÂ al., [2024b](https://arxiv.org/html/2410.02073v1#bib.bib113)), Metric3DÂ (Yin etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib116)) and Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)), PatchFusionÂ (Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)), UniDepthÂ (Piccinelli etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib76)), ZeroDepthÂ (Guizilini etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib35)), and ZoeDepthÂ (Bhat etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib6)). To preserve the zero-shot setting, we do not report results for models that were trained on the same dataset as the evaluation dataset. We report commonly used metrics in the depth estimation literature, namely ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™\\mathit{AbsRel}italic\_AbsRel, ğ¿ğ‘œğ‘”10subscriptğ¿ğ‘œğ‘”10\\mathit{Log}\_{10}italic\_Log start\_POSTSUBSCRIPT 10 end\_POSTSUBSCRIPTÂ (Saxena etÂ al., [2009](https://arxiv.org/html/2410.02073v1#bib.bib85)), Î´1subscriptğ›¿1\\delta\_{1}italic\_Î´ start\_POSTSUBSCRIPT 1 end\_POSTSUBSCRIPT, Î´2subscriptğ›¿2\\delta\_{2}italic\_Î´ start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT and Î´3subscriptğ›¿3\\delta\_{3}italic\_Î´ start\_POSTSUBSCRIPT 3 end\_POSTSUBSCRIPT scoresÂ (Ladicky etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib52)), as well as point-cloud metricsÂ (Spencer etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib93)). Due to the high resolution of Booster images, we were not able to obtain point-cloud metrics in reasonable time.

Table 4: Additional zero-shot metric depth evaluation. We report additional metrics used in the depth estimation literature, namely ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™\\mathit{AbsRel}italic\_AbsRelÂ (Ladicky etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib52)), ğ¿ğ‘œğ‘”10subscriptğ¿ğ‘œğ‘”10\\mathit{Log}\_{10}italic\_Log start\_POSTSUBSCRIPT 10 end\_POSTSUBSCRIPT, Î´2subscriptğ›¿2\\delta\_{2}italic\_Î´ start\_POSTSUBSCRIPT 2 end\_POSTSUBSCRIPT and Î´3subscriptğ›¿3\\delta\_{3}italic\_Î´ start\_POSTSUBSCRIPT 3 end\_POSTSUBSCRIPT scores, as well as point-cloud metricsÂ (Spencer etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib93)) on BoosterÂ (Ramirez etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib81)), MiddleburyÂ (Scharstein etÂ al., [2014](https://arxiv.org/html/2410.02073v1#bib.bib88)), Sun-RGBDÂ (Song etÂ al., [2015](https://arxiv.org/html/2410.02073v1#bib.bib92)), ETH3DÂ (SchÃ¶ps etÂ al., [2017](https://arxiv.org/html/2410.02073v1#bib.bib89)), nuScenesÂ (Caesar etÂ al., [2020](https://arxiv.org/html/2410.02073v1#bib.bib11)), and SintelÂ (Butler etÂ al., [2012](https://arxiv.org/html/2410.02073v1#bib.bib9)). For fair comparison, all reported results were reproduced in our environment.

### A.3 Runtime

To assess the latency of our approach in comparison to baselines, we test all approaches on images of varying sizes and report results in Tab.Â [5](https://arxiv.org/html/2410.02073v1#A1.T5 "Table 5 â€£ A.3 Runtime â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). We pick common image resolutions (VGA: 640Ã—\\timesÃ—480, HD: 1920Ã—\\timesÃ—1080, 4K: 4032Ã—\\timesÃ—3024) and measure each methodâ€™s average runtime for processing an image of the given size. All reported runtimes are reproduced in our environment and include preprocessing, eventual resizing (for methods operating at a fixed internal resolution), and inference of each model. We further report the parameter counts and flops (at HD resolution) for each method as measured with the fvcore package.

Among all approaches with a fixed output resolution, Depth Pro has the highest native output resolution, processing more than 3 times as many pixels as the next highest, Metric3D v2Â (Hu etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib38)). Yet Depth Pro has less than half the parameter count and requires only a third of the runtime compared to Metric3D v2.

The variable-resolution approaches (PatchFusionÂ (Li etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib61)) and ZeroDepthÂ (Guizilini etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib35))) have considerably larger runtime, with the faster model, ZeroDepth, taking almost 4 times as long as Depth Pro, even for small VGA images.

Table 5: Model performance, measured on a V100-32G GPU. We report runtimes in milliseconds (ms) on images of multiple sizes, as well as model parameter counts and flops. For fairness, the reported runtimes are reproduced in our environment. Entries are sorted by the native output resolution.

<table id="A1.T5.17"><tbody><tr id="A1.T5.9.9"><td id="A1.T5.9.9.10">Method</td><td id="A1.T5.9.9.11"><span id="A1.T5.9.9.11.1"><span id="A1.T5.9.9.11.1.1"><span id="A1.T5.9.9.11.1.1.1">Parameter</span> <span id="A1.T5.9.9.11.1.1.2">count</span></span></span></td><td id="A1.T5.2.2.2">Flops<sub id="A1.T5.2.2.2.1"><span id="A1.T5.2.2.2.1.1">HD</span></sub><math alttext="\downarrow" display="inline" id="A1.T5.2.2.2.m2.1"><semantics id="A1.T5.2.2.2.m2.1a"><mo id="A1.T5.2.2.2.m2.1.1" stretchy="false" xref="A1.T5.2.2.2.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.2.m2.1b"><ci id="A1.T5.2.2.2.m2.1.1.cmml" xref="A1.T5.2.2.2.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.2.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.2.2.2.m2.1d">â†“</annotation></semantics></math></td><td colspan="2" id="A1.T5.3.3.3"><span id="A1.T5.3.3.3.1"><span id="A1.T5.3.3.3.1.1"><span id="A1.T5.3.3.3.1.1.1">Native output</span> <span id="A1.T5.3.3.3.1.1.2">resolution</span> </span></span><math alttext="\!\uparrow" display="inline" id="A1.T5.3.3.3.m1.1"><semantics id="A1.T5.3.3.3.m1.1a"><mo id="A1.T5.3.3.3.m1.1.1" stretchy="false" xref="A1.T5.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.3.m1.1b"><ci id="A1.T5.3.3.3.m1.1.1.cmml" xref="A1.T5.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.3.m1.1c">\!\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.3.3.3.m1.1d">â†‘</annotation></semantics></math></td><td id="A1.T5.5.5.5"><math alttext="t_{\mathit{VGA}}" display="inline" id="A1.T5.4.4.4.m1.1"><semantics id="A1.T5.4.4.4.m1.1a"><msub id="A1.T5.4.4.4.m1.1.1" xref="A1.T5.4.4.4.m1.1.1.cmml"><mi id="A1.T5.4.4.4.m1.1.1.2" xref="A1.T5.4.4.4.m1.1.1.2.cmml">t</mi><mi id="A1.T5.4.4.4.m1.1.1.3" xref="A1.T5.4.4.4.m1.1.1.3.cmml">ğ‘‰ğºğ´</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.4.m1.1b"><apply id="A1.T5.4.4.4.m1.1.1.cmml" xref="A1.T5.4.4.4.m1.1.1"><csymbol cd="ambiguous" id="A1.T5.4.4.4.m1.1.1.1.cmml" xref="A1.T5.4.4.4.m1.1.1">subscript</csymbol><ci id="A1.T5.4.4.4.m1.1.1.2.cmml" xref="A1.T5.4.4.4.m1.1.1.2">ğ‘¡</ci><ci id="A1.T5.4.4.4.m1.1.1.3.cmml" xref="A1.T5.4.4.4.m1.1.1.3">ğ‘‰ğºğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.4.m1.1c">t_{\mathit{VGA}}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.4.4.4.m1.1d">italic_t start_POSTSUBSCRIPT italic_VGA end_POSTSUBSCRIPT</annotation></semantics></math> (ms) <math alttext="\downarrow" display="inline" id="A1.T5.5.5.5.m2.1"><semantics id="A1.T5.5.5.5.m2.1a"><mo id="A1.T5.5.5.5.m2.1.1" stretchy="false" xref="A1.T5.5.5.5.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A1.T5.5.5.5.m2.1b"><ci id="A1.T5.5.5.5.m2.1.1.cmml" xref="A1.T5.5.5.5.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.5.5.5.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.5.5.5.m2.1d">â†“</annotation></semantics></math></td><td id="A1.T5.7.7.7"><math alttext="t_{\mathit{HD}}" display="inline" id="A1.T5.6.6.6.m1.1"><semantics id="A1.T5.6.6.6.m1.1a"><msub id="A1.T5.6.6.6.m1.1.1" xref="A1.T5.6.6.6.m1.1.1.cmml"><mi id="A1.T5.6.6.6.m1.1.1.2" xref="A1.T5.6.6.6.m1.1.1.2.cmml">t</mi><mi id="A1.T5.6.6.6.m1.1.1.3" xref="A1.T5.6.6.6.m1.1.1.3.cmml">ğ»ğ·</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T5.6.6.6.m1.1b"><apply id="A1.T5.6.6.6.m1.1.1.cmml" xref="A1.T5.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="A1.T5.6.6.6.m1.1.1.1.cmml" xref="A1.T5.6.6.6.m1.1.1">subscript</csymbol><ci id="A1.T5.6.6.6.m1.1.1.2.cmml" xref="A1.T5.6.6.6.m1.1.1.2">ğ‘¡</ci><ci id="A1.T5.6.6.6.m1.1.1.3.cmml" xref="A1.T5.6.6.6.m1.1.1.3">ğ»ğ·</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.6.6.6.m1.1c">t_{\mathit{HD}}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.6.6.6.m1.1d">italic_t start_POSTSUBSCRIPT italic_HD end_POSTSUBSCRIPT</annotation></semantics></math> (ms) <math alttext="\downarrow" display="inline" id="A1.T5.7.7.7.m2.1"><semantics id="A1.T5.7.7.7.m2.1a"><mo id="A1.T5.7.7.7.m2.1.1" stretchy="false" xref="A1.T5.7.7.7.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A1.T5.7.7.7.m2.1b"><ci id="A1.T5.7.7.7.m2.1.1.cmml" xref="A1.T5.7.7.7.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.7.7.7.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.7.7.7.m2.1d">â†“</annotation></semantics></math></td><td id="A1.T5.9.9.9"><math alttext="t_{\mathit{4K}}" display="inline" id="A1.T5.8.8.8.m1.1"><semantics id="A1.T5.8.8.8.m1.1a"><msub id="A1.T5.8.8.8.m1.1.1" xref="A1.T5.8.8.8.m1.1.1.cmml"><mi id="A1.T5.8.8.8.m1.1.1.2" xref="A1.T5.8.8.8.m1.1.1.2.cmml">t</mi><mrow id="A1.T5.8.8.8.m1.1.1.3" xref="A1.T5.8.8.8.m1.1.1.3.cmml"><mn id="A1.T5.8.8.8.m1.1.1.3.2" mathvariant="italic" xref="A1.T5.8.8.8.m1.1.1.3.2.cmml">4</mn><mo id="A1.T5.8.8.8.m1.1.1.3.1" xref="A1.T5.8.8.8.m1.1.1.3.1.cmml">â¢</mo><mi id="A1.T5.8.8.8.m1.1.1.3.3" xref="A1.T5.8.8.8.m1.1.1.3.3.cmml">K</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A1.T5.8.8.8.m1.1b"><apply id="A1.T5.8.8.8.m1.1.1.cmml" xref="A1.T5.8.8.8.m1.1.1"><csymbol cd="ambiguous" id="A1.T5.8.8.8.m1.1.1.1.cmml" xref="A1.T5.8.8.8.m1.1.1">subscript</csymbol><ci id="A1.T5.8.8.8.m1.1.1.2.cmml" xref="A1.T5.8.8.8.m1.1.1.2">ğ‘¡</ci><apply id="A1.T5.8.8.8.m1.1.1.3.cmml" xref="A1.T5.8.8.8.m1.1.1.3"><times id="A1.T5.8.8.8.m1.1.1.3.1.cmml" xref="A1.T5.8.8.8.m1.1.1.3.1"></times><cn id="A1.T5.8.8.8.m1.1.1.3.2.cmml" type="integer" xref="A1.T5.8.8.8.m1.1.1.3.2">4</cn><ci id="A1.T5.8.8.8.m1.1.1.3.3.cmml" xref="A1.T5.8.8.8.m1.1.1.3.3">ğ¾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.8.8.8.m1.1c">t_{\mathit{4K}}</annotation><annotation encoding="application/x-llamapun" id="A1.T5.8.8.8.m1.1d">italic_t start_POSTSUBSCRIPT italic_4 italic_K end_POSTSUBSCRIPT</annotation></semantics></math> (ms) <math alttext="\downarrow" display="inline" id="A1.T5.9.9.9.m2.1"><semantics id="A1.T5.9.9.9.m2.1a"><mo id="A1.T5.9.9.9.m2.1.1" stretchy="false" xref="A1.T5.9.9.9.m2.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A1.T5.9.9.9.m2.1b"><ci id="A1.T5.9.9.9.m2.1.1.cmml" xref="A1.T5.9.9.9.m2.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.9.9.9.m2.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T5.9.9.9.m2.1d">â†“</annotation></semantics></math></td></tr><tr id="A1.T5.10.10"><td id="A1.T5.10.10.2">DPT</td><td id="A1.T5.10.10.3">123M</td><td id="A1.T5.10.10.4">-</td><td id="A1.T5.10.10.1">384 <math alttext="\times" display="inline" id="A1.T5.10.10.1.m1.1"><semantics id="A1.T5.10.10.1.m1.1a"><mo id="A1.T5.10.10.1.m1.1.1" xref="A1.T5.10.10.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.10.10.1.m1.1b"><times id="A1.T5.10.10.1.m1.1.1.cmml" xref="A1.T5.10.10.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.10.10.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.10.10.1.m1.1d">Ã—</annotation></semantics></math> 384</td><td id="A1.T5.10.10.5">= 0.15 MP</td><td id="A1.T5.10.10.6">33.2</td><td id="A1.T5.10.10.7">30.6</td><td id="A1.T5.10.10.8">27.8</td></tr><tr id="A1.T5.11.11"><td id="A1.T5.11.11.2">ZoeDepth</td><td id="A1.T5.11.11.3">340M</td><td id="A1.T5.11.11.4">-</td><td id="A1.T5.11.11.1">384 <math alttext="\times" display="inline" id="A1.T5.11.11.1.m1.1"><semantics id="A1.T5.11.11.1.m1.1a"><mo id="A1.T5.11.11.1.m1.1.1" xref="A1.T5.11.11.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.11.11.1.m1.1b"><times id="A1.T5.11.11.1.m1.1.1.cmml" xref="A1.T5.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.11.11.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.11.11.1.m1.1d">Ã—</annotation></semantics></math> 512</td><td id="A1.T5.11.11.5">= 0.20 MP</td><td id="A1.T5.11.11.6">235.7</td><td id="A1.T5.11.11.7">235.1</td><td id="A1.T5.11.11.8">235.4</td></tr><tr id="A1.T5.12.12"><td id="A1.T5.12.12.2">DepthAnything v2</td><td id="A1.T5.12.12.3">335M</td><td id="A1.T5.12.12.4">1827G</td><td id="A1.T5.12.12.1">518 <math alttext="\times" display="inline" id="A1.T5.12.12.1.m1.1"><semantics id="A1.T5.12.12.1.m1.1a"><mo id="A1.T5.12.12.1.m1.1.1" xref="A1.T5.12.12.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.12.12.1.m1.1b"><times id="A1.T5.12.12.1.m1.1.1.cmml" xref="A1.T5.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.12.12.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.12.12.1.m1.1d">Ã—</annotation></semantics></math> 518</td><td id="A1.T5.12.12.5">= 0.27 MP</td><td id="A1.T5.12.12.6">90.9</td><td id="A1.T5.12.12.7">91.1</td><td id="A1.T5.12.12.8">91.2</td></tr><tr id="A1.T5.13.13"><td id="A1.T5.13.13.2">UniDepth</td><td id="A1.T5.13.13.3">347M</td><td id="A1.T5.13.13.4">630G</td><td id="A1.T5.13.13.1">462 <math alttext="\times" display="inline" id="A1.T5.13.13.1.m1.1"><semantics id="A1.T5.13.13.1.m1.1a"><mo id="A1.T5.13.13.1.m1.1.1" xref="A1.T5.13.13.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.13.13.1.m1.1b"><times id="A1.T5.13.13.1.m1.1.1.cmml" xref="A1.T5.13.13.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.13.13.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.13.13.1.m1.1d">Ã—</annotation></semantics></math> 616</td><td id="A1.T5.13.13.5">= 0.28 MP</td><td id="A1.T5.13.13.6">178.5</td><td id="A1.T5.13.13.7">183.0</td><td id="A1.T5.13.13.8">198.1</td></tr><tr id="A1.T5.14.14"><td id="A1.T5.14.14.2">Metric3D</td><td id="A1.T5.14.14.3">203M</td><td id="A1.T5.14.14.4">477G</td><td id="A1.T5.14.14.1">480 <math alttext="\times" display="inline" id="A1.T5.14.14.1.m1.1"><semantics id="A1.T5.14.14.1.m1.1a"><mo id="A1.T5.14.14.1.m1.1.1" xref="A1.T5.14.14.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.14.14.1.m1.1b"><times id="A1.T5.14.14.1.m1.1.1.cmml" xref="A1.T5.14.14.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.14.14.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.14.14.1.m1.1d">Ã—</annotation></semantics></math> 1216</td><td id="A1.T5.14.14.5">= 0.58 MP</td><td id="A1.T5.14.14.6">217.9</td><td id="A1.T5.14.14.7">263.8</td><td id="A1.T5.14.14.8">398.1</td></tr><tr id="A1.T5.15.15"><td id="A1.T5.15.15.2">Marigold</td><td id="A1.T5.15.15.3">949M</td><td id="A1.T5.15.15.4">-</td><td id="A1.T5.15.15.1">768 <math alttext="\times" display="inline" id="A1.T5.15.15.1.m1.1"><semantics id="A1.T5.15.15.1.m1.1a"><mo id="A1.T5.15.15.1.m1.1.1" xref="A1.T5.15.15.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.15.15.1.m1.1b"><times id="A1.T5.15.15.1.m1.1.1.cmml" xref="A1.T5.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.15.15.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.15.15.1.m1.1d">Ã—</annotation></semantics></math> 768</td><td id="A1.T5.15.15.5">= 0.59 MP</td><td id="A1.T5.15.15.6">5174.3</td><td id="A1.T5.15.15.7">4433.6</td><td id="A1.T5.15.15.8">4977.6</td></tr><tr id="A1.T5.16.16"><td id="A1.T5.16.16.2">Metric3D v2</td><td id="A1.T5.16.16.3">1.378G</td><td id="A1.T5.16.16.4">6830G</td><td id="A1.T5.16.16.1">616 <math alttext="\times" display="inline" id="A1.T5.16.16.1.m1.1"><semantics id="A1.T5.16.16.1.m1.1a"><mo id="A1.T5.16.16.1.m1.1.1" xref="A1.T5.16.16.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.16.16.1.m1.1b"><times id="A1.T5.16.16.1.m1.1.1.cmml" xref="A1.T5.16.16.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.16.16.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.16.16.1.m1.1d">Ã—</annotation></semantics></math> 1064</td><td id="A1.T5.16.16.5">= 0.66 MP</td><td id="A1.T5.16.16.6">1299.6</td><td id="A1.T5.16.16.7">1299.7</td><td id="A1.T5.16.16.8">1390.2</td></tr><tr id="A1.T5.17.18.1"><td id="A1.T5.17.18.1.1">PatchFusion</td><td id="A1.T5.17.18.1.2">203M</td><td id="A1.T5.17.18.1.3">-</td><td colspan="2" id="A1.T5.17.18.1.4">Original (tile-based)</td><td id="A1.T5.17.18.1.5">84012.0</td><td id="A1.T5.17.18.1.6">84029.9</td><td id="A1.T5.17.18.1.7">84453.9</td></tr><tr id="A1.T5.17.19.2"><td id="A1.T5.17.19.2.1">ZeroDepth</td><td id="A1.T5.17.19.2.2">233M</td><td id="A1.T5.17.19.2.3">10862G</td><td colspan="2" id="A1.T5.17.19.2.4">Original</td><td id="A1.T5.17.19.2.5">1344.3</td><td id="A1.T5.17.19.2.6">8795.7</td><td id="A1.T5.17.19.2.7">34992.2</td></tr><tr id="A1.T5.17.17"><td id="A1.T5.17.17.2">Depth Pro</td><td id="A1.T5.17.17.3">504M</td><td id="A1.T5.17.17.4">4370G</td><td id="A1.T5.17.17.1">1536 <math alttext="\times" display="inline" id="A1.T5.17.17.1.m1.1"><semantics id="A1.T5.17.17.1.m1.1a"><mo id="A1.T5.17.17.1.m1.1.1" xref="A1.T5.17.17.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="A1.T5.17.17.1.m1.1b"><times id="A1.T5.17.17.1.m1.1.1.cmml" xref="A1.T5.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.17.17.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="A1.T5.17.17.1.m1.1d">Ã—</annotation></semantics></math> 1536</td><td id="A1.T5.17.17.5">= 2.36 MP</td><td id="A1.T5.17.17.6">341.3</td><td id="A1.T5.17.17.7">341.3</td><td id="A1.T5.17.17.8">341.3</td></tr></tbody></table>

### A.4 Boundary experiments

Boundary metrics empirical study. To illustrate how our boundary metrics work, we report additional qualitative edge metric results in Fig.Â [8](https://arxiv.org/html/2410.02073v1#A1.F8 "Figure 8 â€£ A.4 Boundary experiments â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). In particular, we show the occluding contours derived from the ground-truth and predicted depth, which illustrate how incorrect depth boundary predictions can impact the metric. Furthermore, to illustrate the behavior of the boundary precision and recall measurements under various image perturbations we also provide an empirical study in Fig.Â [9](https://arxiv.org/html/2410.02073v1#A1.F9 "Figure 9 â€£ A.4 Boundary experiments â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). We report both quantitative and qualitative results on samples from the UnrealStereo4K datasetÂ (Tosi etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib100)). Our results empirically demonstrate the correlation between erroneous depth edge predictions and low precision and recall values.

Figure 8: Evaluation metrics for sharp boundaries. We propose novel metrics to evaluate the sharpness of occlusion boundaries. The metrics can be computed on ground-truth depth maps (first two rows), and binary maps that can be derived from matting or segmentation datasets (subsequent rows). Each row shows a sample image, the ground truth for deriving occlusion boundaries, our prediction, ground-truth occluding contours, and occluding contours from the prediction. For these visualizations we set t\=15ğ‘¡15t=15italic\_t = 15.

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x35.png)

Figure 9: Boundary evaluation metrics empirical study. We demonstrate how various types of image perturbations impact our proposed edge metrics. We report quantitative and qualitative results for multiple ground-truth perturbations, such as simple image shifts, downsampling followed by upsamplings, and Gaussian blurring. We report both ground-truth and perturbed occluding contours, used to derive our Fğ¹Fitalic\_F1 scores. Our results empirically demonstrate the correlation between erroneous depth edge predictions and low precision and recall values.

Results on the iBims datasetÂ (Koch etÂ al., [2018](https://arxiv.org/html/2410.02073v1#bib.bib50)). We supplement our boundary evaluation by results on the iBims dataset, which is commonly used for evaluating depth boundaries. iBims consists of images of indoor scenes that have been laser-scanned. The images are at 640Ã—480640480640\\times 480640 Ã— 480 resolution and have been supplemented with manually annotated occlusion boundary maps to facilitate evaluation. The iBims benchmark uses _Depth Directed Errors_ (DDE), which evaluate overall metric depth accuracy, _Depth Boundary Errors_ (DBE), which are similar in spirit to our proposed boundary metric but require manual annotation, and _Planar Errors_, which evaluate the accuracy of planes derived from the depth maps.

We find that Depth Pro is on par with the state of the art according to the DDE and PE metrics, and significantly outperforms all prior work according to the boundary metrics.

Table 6: Zero-shot metric depth evalution on the iBims datasetÂ (Koch etÂ al., [2018](https://arxiv.org/html/2410.02073v1#bib.bib50)). We report the iBims-specific Depth Directed Errors (DDE), Depth Boundary Errors (DBE) and Planar Errors (PE). For fairness, all reported results were reproduced in our environment. Please see Sec.Â [A.4](https://arxiv.org/html/2410.02073v1#A1.SS4 "A.4 Boundary experiments â€£ Appendix A Additional Results â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")

## Appendix B Controlled Experiments

We conduct several controlled experiments to investigate the impact of various components and design decisions in Depth Pro. Specifically, we aim to assess the contribution of key components in the network architecture (Sec.Â [B.1](https://arxiv.org/html/2410.02073v1#A2.SS1 "B.1 Network backbone â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")), training objective (Sec.Â [B.3](https://arxiv.org/html/2410.02073v1#A2.SS3 "B.3 Training objectives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")), training curriculum (Sec.Â [B.4](https://arxiv.org/html/2410.02073v1#A2.SS4 "B.4 Full curricula â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")), and the focal length estimation head (Sec.Â [B.5](https://arxiv.org/html/2410.02073v1#A2.SS5 "B.5 Focal length estimation â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")).

### B.1 Network backbone

We begin by evaluating various candidate image encoder backbones within our network architecture. To assess their performance, we conduct a comparative analysis utilizing off-the-shelf models available from the TIMM library (Wightman, [2019](https://arxiv.org/html/2410.02073v1#bib.bib105)). Using the pretrained weights, we train each backbone at 384 Ã—\\timesÃ— 384 resolution across five RGB-D datasets (Keystone, HRWSI, RedWeb, TartanAir, and Hypersim) and evaluate their performance in terms of metric depth accuracy across multiple datasets, including Booster, Hypersim, Middlebury, and NYUv2, utilizing metrics such as ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™ğ´ğ‘ğ‘ ğ‘…ğ‘’ğ‘™\\mathit{AbsRel}italic\_AbsRel for affine-invariant depth and ğ¿ğ‘œğ‘”10subscriptğ¿ğ‘œğ‘”10\\mathit{Log}\_{10}italic\_Log start\_POSTSUBSCRIPT 10 end\_POSTSUBSCRIPT for metric depth in Tab.Â [7](https://arxiv.org/html/2410.02073v1#A2.T7 "Table 7 â€£ B.2 High-resolution alternatives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). We find that ViT-L DINOv2 Â (Oquab etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib72)) outperforms all other backbones by a significant margin and conclude that the combination of backbone and pretraining strategy considerably affects downstream performance. Following this analysis, we pick ViT-L DINOv2 for both our encoder backbones.

### B.2 High-resolution alternatives

We further evaluate alternative high-resolution 1536Ã—\\timesÃ—1536 network structures and different pre-trained weights (Tab.Â [8](https://arxiv.org/html/2410.02073v1#A2.T8 "Table 8 â€£ B.2 High-resolution alternatives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")). To do this, we test generalization accuracy by training on a train split of some datasets and testing on a val or test split of other datasets, following the Stage 1 protocol for all models in accordance with Tab.Â [14](https://arxiv.org/html/2410.02073v1#A3.T14 "Table 14 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") and Tab.Â [15](https://arxiv.org/html/2410.02073v1#A3.T15 "Table 15 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). All ViT models use a patch size of 16Ã—\\timesÃ—16\. For weights pretrained with a patch size of 14Ã—\\timesÃ—14 we apply bicubic interpolation to the weights of the convolutional patch embedding layer and scale these weights inversely to the number of pixels (i.e., the weights are reduced by a factor of 1.3). All ViT models use resolution 1536Ã—\\timesÃ—1536, for this we apply bicubic interpolation to positional embeddings prior to training. The Depth Pro approach in all cases uses ViT with resolution 384Ã—\\timesÃ—384 and patch size 16Ã—\\timesÃ—16 for both the patch encoder and the image encoder. SWINv2 and convolutional models are pretrained on ImageNetÂ (Deng etÂ al., [2009](https://arxiv.org/html/2410.02073v1#bib.bib20)). Other models use different pretraining approaches described in their papers: CLIPÂ (Radford etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib78)), MAEÂ (He etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib36)), BeiTv2Â (Peng etÂ al., [2022b](https://arxiv.org/html/2410.02073v1#bib.bib74)), and DINOv2Â (Oquab etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib72)). For the Segment Anything model we use publicly available pretrained weights, which were initialized using MAE pretrainingÂ (He etÂ al., [2022](https://arxiv.org/html/2410.02073v1#bib.bib36)) and subsequently trained for segmentation as described in their paperÂ (Kirillov etÂ al., [2023](https://arxiv.org/html/2410.02073v1#bib.bib48)).

We find that the presented Depth Pro approach is faster and more accurate for object boundaries than the plain ViT, with comparable metric depth accuracy. In comparison to other transformer-based and convolutional models, Depth Pro has comparable latency, several times lower metric depth error, and several times higher recall accuracy for object boundaries.

Table 7: Comparison of image encoder backbones candidates. We train each backbone at 384Ã—\\timesÃ—384 resolution across five RGB-D datasets: Keystone, HRWSI, RedWeb, TartanAir, and Hypersim. To ensure fair comparison, we select backbone candidates with comparable computational complexity, measured in Flops using the fvcore libraryÂ (Facebook Research, [2022](https://arxiv.org/html/2410.02073v1#bib.bib25)), and an equivalent number of parameters. We identify ViT-L DINOv2Â (Oquab etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib72)) as the optimal choice for our image encoder backbone, given its superior depth accuracy performance.

Table 8: High-resolution alternatives. Generalization accuracy of alternative high-resolution 1536Ã—\\timesÃ—1536 models and different pretrained weights. All models are trained identically using Stage 1 in accordance with Tab.Â [14](https://arxiv.org/html/2410.02073v1#A3.T14 "Table 14 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") and Tab.Â [15](https://arxiv.org/html/2410.02073v1#A3.T15 "Table 15 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). Latency measured on a single GPU V100 with FP16 precision using batch=1. All ViT models use a patch size of 16Ã—\\timesÃ—16\. Depth Pro employs a ViT-L DINOv2Â (Oquab etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib72)) for the image and patch encoders.

<table id="A2.T8.9"><tbody><tr id="A2.T8.9.6.1"><td id="A2.T8.9.6.1.1"></td><td id="A2.T8.9.6.1.2"></td><td id="A2.T8.9.6.1.3"></td><td colspan="2" id="A2.T8.9.6.1.4">Metric depth accuracy</td><td colspan="2" id="A2.T8.9.6.1.5">Boundary accuracy</td></tr><tr id="A2.T8.9.5"><td id="A2.T8.9.5.6"></td><td id="A2.T8.9.5.7">Method</td><td id="A2.T8.5.1.1">Latency, ms <math alttext="\downarrow" display="inline" id="A2.T8.5.1.1.m1.1"><semantics id="A2.T8.5.1.1.m1.1a"><mo id="A2.T8.5.1.1.m1.1.1" stretchy="false" xref="A2.T8.5.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T8.5.1.1.m1.1b"><ci id="A2.T8.5.1.1.m1.1.1.cmml" xref="A2.T8.5.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.5.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T8.5.1.1.m1.1d">â†“</annotation></semantics></math></td><td id="A2.T8.6.2.2">NYUv2 <math alttext="\delta_{1}\!\uparrow" display="inline" id="A2.T8.6.2.2.m1.1"><semantics id="A2.T8.6.2.2.m1.1a"><mrow id="A2.T8.6.2.2.m1.1.1" xref="A2.T8.6.2.2.m1.1.1.cmml"><msub id="A2.T8.6.2.2.m1.1.1.2" xref="A2.T8.6.2.2.m1.1.1.2.cmml"><mi id="A2.T8.6.2.2.m1.1.1.2.2" xref="A2.T8.6.2.2.m1.1.1.2.2.cmml">Î´</mi><mn id="A2.T8.6.2.2.m1.1.1.2.3" xref="A2.T8.6.2.2.m1.1.1.2.3.cmml">1</mn></msub><mo id="A2.T8.6.2.2.m1.1.1.1" lspace="0.108em" stretchy="false" xref="A2.T8.6.2.2.m1.1.1.1.cmml">â†‘</mo><mi id="A2.T8.6.2.2.m1.1.1.3" xref="A2.T8.6.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T8.6.2.2.m1.1b"><apply id="A2.T8.6.2.2.m1.1.1.cmml" xref="A2.T8.6.2.2.m1.1.1"><ci id="A2.T8.6.2.2.m1.1.1.1.cmml" xref="A2.T8.6.2.2.m1.1.1.1">â†‘</ci><apply id="A2.T8.6.2.2.m1.1.1.2.cmml" xref="A2.T8.6.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T8.6.2.2.m1.1.1.2.1.cmml" xref="A2.T8.6.2.2.m1.1.1.2">subscript</csymbol><ci id="A2.T8.6.2.2.m1.1.1.2.2.cmml" xref="A2.T8.6.2.2.m1.1.1.2.2">ğ›¿</ci><cn id="A2.T8.6.2.2.m1.1.1.2.3.cmml" type="integer" xref="A2.T8.6.2.2.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="A2.T8.6.2.2.m1.1.1.3.cmml" xref="A2.T8.6.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.6.2.2.m1.1c">\delta_{1}\!\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T8.6.2.2.m1.1d">italic_Î´ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â†‘</annotation></semantics></math></td><td id="A2.T8.7.3.3">iBims <math alttext="\delta_{1}\!\uparrow" display="inline" id="A2.T8.7.3.3.m1.1"><semantics id="A2.T8.7.3.3.m1.1a"><mrow id="A2.T8.7.3.3.m1.1.1" xref="A2.T8.7.3.3.m1.1.1.cmml"><msub id="A2.T8.7.3.3.m1.1.1.2" xref="A2.T8.7.3.3.m1.1.1.2.cmml"><mi id="A2.T8.7.3.3.m1.1.1.2.2" xref="A2.T8.7.3.3.m1.1.1.2.2.cmml">Î´</mi><mn id="A2.T8.7.3.3.m1.1.1.2.3" xref="A2.T8.7.3.3.m1.1.1.2.3.cmml">1</mn></msub><mo id="A2.T8.7.3.3.m1.1.1.1" lspace="0.108em" stretchy="false" xref="A2.T8.7.3.3.m1.1.1.1.cmml">â†‘</mo><mi id="A2.T8.7.3.3.m1.1.1.3" xref="A2.T8.7.3.3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T8.7.3.3.m1.1b"><apply id="A2.T8.7.3.3.m1.1.1.cmml" xref="A2.T8.7.3.3.m1.1.1"><ci id="A2.T8.7.3.3.m1.1.1.1.cmml" xref="A2.T8.7.3.3.m1.1.1.1">â†‘</ci><apply id="A2.T8.7.3.3.m1.1.1.2.cmml" xref="A2.T8.7.3.3.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T8.7.3.3.m1.1.1.2.1.cmml" xref="A2.T8.7.3.3.m1.1.1.2">subscript</csymbol><ci id="A2.T8.7.3.3.m1.1.1.2.2.cmml" xref="A2.T8.7.3.3.m1.1.1.2.2">ğ›¿</ci><cn id="A2.T8.7.3.3.m1.1.1.2.3.cmml" type="integer" xref="A2.T8.7.3.3.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="A2.T8.7.3.3.m1.1.1.3.cmml" xref="A2.T8.7.3.3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.7.3.3.m1.1c">\delta_{1}\!\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T8.7.3.3.m1.1d">italic_Î´ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â†‘</annotation></semantics></math></td><td id="A2.T8.8.4.4">iBims F1<math alttext="\uparrow" display="inline" id="A2.T8.8.4.4.m1.1"><semantics id="A2.T8.8.4.4.m1.1a"><mo id="A2.T8.8.4.4.m1.1.1" stretchy="false" xref="A2.T8.8.4.4.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A2.T8.8.4.4.m1.1b"><ci id="A2.T8.8.4.4.m1.1.1.cmml" xref="A2.T8.8.4.4.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.8.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T8.8.4.4.m1.1d">â†‘</annotation></semantics></math></td><td id="A2.T8.9.5.5">DIS R<math alttext="\uparrow" display="inline" id="A2.T8.9.5.5.m1.1"><semantics id="A2.T8.9.5.5.m1.1a"><mo id="A2.T8.9.5.5.m1.1.1" stretchy="false" xref="A2.T8.9.5.5.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="A2.T8.9.5.5.m1.1b"><ci id="A2.T8.9.5.5.m1.1.1.cmml" xref="A2.T8.9.5.5.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.9.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T8.9.5.5.m1.1d">â†‘</annotation></semantics></math></td></tr><tr id="A2.T8.9.7.2"><td id="A2.T8.9.7.2.1" rowspan="2"><span id="A2.T8.9.7.2.1.1"><span id="A2.T8.9.7.2.1.1.1"><span><span id="A2.T8.9.7.2.1.1.1.1">Conv.</span></span></span></span></td><td id="A2.T8.9.7.2.2">EfficientNetV2-XL&nbsp;<cite>(Tan &amp; Le, <a href="https://arxiv.org/html/2410.02073v1#bib.bib98" title="">2021</a>)</cite></td><td id="A2.T8.9.7.2.3">118</td><td id="A2.T8.9.7.2.4">4.4</td><td id="A2.T8.9.7.2.5">7.0</td><td id="A2.T8.9.7.2.6">0.005</td><td id="A2.T8.9.7.2.7">0.000</td></tr><tr id="A2.T8.9.8.3"><td id="A2.T8.9.8.3.1">ConvNext-XXL&nbsp;<cite>(Liu et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib67" title="">2022b</a>)</cite></td><td id="A2.T8.9.8.3.2">304</td><td id="A2.T8.9.8.3.3">68.0</td><td id="A2.T8.9.8.3.4">38.3</td><td id="A2.T8.9.8.3.5">0.134</td><td id="A2.T8.9.8.3.6">0.031</td></tr><tr id="A2.T8.9.9.4"><td id="A2.T8.9.9.4.1"></td><td id="A2.T8.9.9.4.2">ConvNextv2-H&nbsp;<cite>(Woo et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib106" title="">2023</a>)</cite></td><td id="A2.T8.9.9.4.3">287</td><td id="A2.T8.9.9.4.4">70.0</td><td id="A2.T8.9.9.4.5">56.6</td><td id="A2.T8.9.9.4.6">0.131</td><td id="A2.T8.9.9.4.7">0.044</td></tr><tr id="A2.T8.9.10.5"><td id="A2.T8.9.10.5.1" rowspan="2"><span id="A2.T8.9.10.5.1.1"><span id="A2.T8.9.10.5.1.1.1"><span><span id="A2.T8.9.10.5.1.1.1.1">Trans.</span></span></span></span></td><td id="A2.T8.9.10.5.2">S. Anything&nbsp;<cite>(Kirillov et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib48" title="">2023</a>)</cite> (ViT-L)</td><td id="A2.T8.9.10.5.3">349</td><td id="A2.T8.9.10.5.4">53.2</td><td id="A2.T8.9.10.5.5">38.9</td><td id="A2.T8.9.10.5.6">0.140</td><td id="A2.T8.9.10.5.7">0.051</td></tr><tr id="A2.T8.9.11.6"><td id="A2.T8.9.11.6.1">S. Anything&nbsp;<cite>(Kirillov et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib48" title="">2023</a>)</cite> (ViT-H)</td><td id="A2.T8.9.11.6.2">365</td><td id="A2.T8.9.11.6.3">51.7</td><td id="A2.T8.9.11.6.4">41.1</td><td id="A2.T8.9.11.6.5">0.146</td><td id="A2.T8.9.11.6.6">0.050</td></tr><tr id="A2.T8.9.12.7"><td id="A2.T8.9.12.7.1"></td><td id="A2.T8.9.12.7.2">SWINv2-L&nbsp;<cite>(Liu et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib66" title="">2022a</a>)</cite> (window=24)</td><td id="A2.T8.9.12.7.3">272</td><td id="A2.T8.9.12.7.4">58.4</td><td id="A2.T8.9.12.7.5">33.1</td><td id="A2.T8.9.12.7.6">0.117</td><td id="A2.T8.9.12.7.7">0.028</td></tr><tr id="A2.T8.9.13.8"><td id="A2.T8.9.13.8.1" rowspan="4"><span id="A2.T8.9.13.8.1.1"><span id="A2.T8.9.13.8.1.1.1"><span><span id="A2.T8.9.13.8.1.1.1.1">ViT</span></span></span></span></td><td id="A2.T8.9.13.8.2">ViT-L CLIP&nbsp;<cite>(Radford et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib78" title="">2021</a>)</cite></td><td id="A2.T8.9.13.8.3">384</td><td id="A2.T8.9.13.8.4">92.2</td><td id="A2.T8.9.13.8.5">81.9</td><td id="A2.T8.9.13.8.6">0.157</td><td id="A2.T8.9.13.8.7"><span id="A2.T8.9.13.8.7.1">0.052</span></td></tr><tr id="A2.T8.9.14.9"><td id="A2.T8.9.14.9.1">ViT-L BeiTv2&nbsp;<cite>(Peng et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib74" title="">2022b</a>)</cite></td><td id="A2.T8.9.14.9.2">OOM</td><td id="A2.T8.9.14.9.3">90.4</td><td id="A2.T8.9.14.9.4"><span id="A2.T8.9.14.9.4.1">86.5</span></td><td id="A2.T8.9.14.9.5">0.149</td><td id="A2.T8.9.14.9.6">0.042</td></tr><tr id="A2.T8.9.15.10"><td id="A2.T8.9.15.10.1">ViT-L MAE&nbsp;<cite>(He et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib36" title="">2022</a>)</cite></td><td id="A2.T8.9.15.10.2">390</td><td id="A2.T8.9.15.10.3"><span id="A2.T8.9.15.10.3.1">92.7</span></td><td id="A2.T8.9.15.10.4">84.7</td><td id="A2.T8.9.15.10.5"><span id="A2.T8.9.15.10.5.1">0.163</span></td><td id="A2.T8.9.15.10.6"><span id="A2.T8.9.15.10.6.1">0.065</span></td></tr><tr id="A2.T8.9.16.11"><td id="A2.T8.9.16.11.1">ViT-L DINOv2&nbsp;<cite>(Oquab et&nbsp;al., <a href="https://arxiv.org/html/2410.02073v1#bib.bib72" title="">2024</a>)</cite></td><td id="A2.T8.9.16.11.2">392</td><td id="A2.T8.9.16.11.3"><span id="A2.T8.9.16.11.3.1">96.5</span></td><td id="A2.T8.9.16.11.4"><span id="A2.T8.9.16.11.4.1">90.3</span></td><td id="A2.T8.9.16.11.5"><span id="A2.T8.9.16.11.5.1">0.161</span></td><td id="A2.T8.9.16.11.6"><span id="A2.T8.9.16.11.6.1">0.065</span></td></tr><tr id="A2.T8.9.17.12"><td id="A2.T8.9.17.12.1"></td><td id="A2.T8.9.17.12.2">Depth Pro</td><td id="A2.T8.9.17.12.3">341</td><td id="A2.T8.9.17.12.4"><span id="A2.T8.9.17.12.4.1">96.1</span></td><td id="A2.T8.9.17.12.5"><span id="A2.T8.9.17.12.5.1">91.3</span></td><td id="A2.T8.9.17.12.6"><span id="A2.T8.9.17.12.6.1">0.177</span></td><td id="A2.T8.9.17.12.7"><span id="A2.T8.9.17.12.7.1">0.080</span></td></tr></tbody></table>

  

### B.3 Training objectives

To assess the efficacy of our training curriculum, we compare it to alternative training schedules. We first examine the different stages individually and then compare full curricula.

Stage 1 training objectives. We first evaluate loss combinations for the first stageÂ (Tab.Â [9](https://arxiv.org/html/2410.02073v1#A2.T9 "Table 9 â€£ B.3 Training objectives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second")).

Table 9: Comparison of stage 1 training objectives. 1A only applies the â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT to metric, and the â„’ğ‘†ğ‘†ğ¼âˆ’ğ‘€ğ´ğ¸subscriptâ„’ğ‘†ğ‘†ğ¼ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{SSI\\mathchar 45\\relax MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_SSI - italic\_MAE end\_POSTSUBSCRIPT to non-metric datasets. 1D additionally minimizes gradients on all datasets. 1B minimizes gradients only on synthetic datasets. We use 1C, which minimizes gradients with a scale-and-shift-invariant â„’ğ‘†ğ‘†ğ¼âˆ’ğ‘€ğ´ğºğ¸subscriptâ„’ğ‘†ğ‘†ğ¼ğ‘€ğ´ğºğ¸\\mathcal{L}\_{\\mathit{SSI\\mathchar 45\\relax MAGE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_SSI - italic\_MAGE end\_POSTSUBSCRIPT loss on all synthetic synthetic datasets irrespective of whether the dataset is metric.

Condition 1A only applies a mean absolute error loss to all datasets. For non-metric datasets, we use the scale-and-shift-invariant version. Condition 1B adds gradient losses to all synthetic datasets. We again use the scale-and-shift-invariant version for non-metric datasets. Following our observations from Sec.Â [3](https://arxiv.org/html/2410.02073v1#S3 "3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), we propose to apply an appropriate mean absolute error loss as in other conditions depending on a dataset being metric, but apply a scale-and-shift-invariant gradient loss irrespective of a dataset being metric or not (C). Although the intuition of applying a gradient loss is to sharpen the boundaries and improve edge-related metrics, it also improves other, non-boundary-related metrics. The key finding is that the gradient loss must be scale-and-shift-invariant, irrespective of the dataset being metric or not. Applying a regular gradient loss on metric datasets that early in training actually harms convergence.

Stage 2 training objectives.

Table 10: Comparison of stage 2 training objectives. We evaluate the efficacy of derivative-based losses for sharpening boundaries. Employing first- and second-order derivative losses (2A) yields the best results on balance as indicated by the average rank over metrics. More details in the text.

The second stage of our training curriculum focuses on sharpening depth boundaries. To that end, we only employ synthetic datasets due to their high quality ground truth. The obvious strategy for sharpening predictions is the application of gradient losses. We evaluate our combination of multiscale derivative-based losses in an ablation study. Condition 2A uses all of the losses, namely â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT, â„’ğ‘€ğ‘†ğ¸subscriptâ„’ğ‘€ğ‘†ğ¸\\mathcal{L}\_{\\mathit{MSE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MSE end\_POSTSUBSCRIPT ,â„’ğ‘€ğ´ğºğ¸subscriptâ„’ğ‘€ğ´ğºğ¸\\mathcal{L}\_{\\mathit{MAGE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAGE end\_POSTSUBSCRIPT, â„’ğ‘€ğ´ğ¿ğ¸subscriptâ„’ğ‘€ğ´ğ¿ğ¸\\mathcal{L}\_{\\mathit{MALE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MALE end\_POSTSUBSCRIPT, and â„’ğ‘€ğ‘†ğºğ¸subscriptâ„’ğ‘€ğ‘†ğºğ¸\\mathcal{L}\_{\\mathit{MSGE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MSGE end\_POSTSUBSCRIPT. Tab.Â [10](https://arxiv.org/html/2410.02073v1#A2.T10 "Table 10 â€£ B.3 Training objectives â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). 2B removes the second-order loss â„’ğ‘€ğ´ğ¿ğ¸subscriptâ„’ğ‘€ğ´ğ¿ğ¸\\mathcal{L}\_{\\mathit{MALE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MALE end\_POSTSUBSCRIPT. 2C further removes the squared first order losses â„’ğ‘€ğ‘†ğºğ¸subscriptâ„’ğ‘€ğ‘†ğºğ¸\\mathcal{L}\_{\\mathit{MSGE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MSGE end\_POSTSUBSCRIPT. 2D removes all derivative-based losses. 2E applies the â„’ğ‘€ğ´ğ¸subscriptâ„’ğ‘€ğ´ğ¸\\mathcal{L}\_{\\mathit{MAE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MAE end\_POSTSUBSCRIPT to all datasets. Removing â„’ğ‘€ğ´ğ¿ğ¸subscriptâ„’ğ‘€ğ´ğ¿ğ¸\\mathcal{L}\_{\\mathit{MALE}}caligraphic\_L start\_POSTSUBSCRIPT italic\_MALE end\_POSTSUBSCRIPT improves results on Apolloscape. Our combination of 0th- to 2nd-order derivative losses (2A) performs best across metrics and datasets in aggregate (e.g., in terms of the average rank across metrics).

### B.4 Full curricula

Table 11: Comparison of full curricula. We evaluate our curriculum (3A) against single stage training (3B), and pre-training on synthetic, fine-tuning with real data (3C).

<table id="A2.T11.8"><tbody><tr id="A2.T11.8.9.1"><td id="A2.T11.8.9.1.1"></td><td colspan="2" id="A2.T11.8.9.1.2">HRWSI</td><td colspan="3" id="A2.T11.8.9.1.3">Hypersim</td><td colspan="3" id="A2.T11.8.9.1.4">Apolloscape</td></tr><tr id="A2.T11.8.8"><td id="A2.T11.8.8.9">Cond.</td><td id="A2.T11.1.1.1">AbsRel<math alttext="\downarrow" display="inline" id="A2.T11.1.1.1.m1.1"><semantics id="A2.T11.1.1.1.m1.1a"><mo id="A2.T11.1.1.1.m1.1.1" stretchy="false" xref="A2.T11.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T11.1.1.1.m1.1b"><ci id="A2.T11.1.1.1.m1.1.1.cmml" xref="A2.T11.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.1.1.1.m1.1d">â†“</annotation></semantics></math></td><td id="A2.T11.2.2.2"><math alttext="\delta_{1}\!\uparrow" display="inline" id="A2.T11.2.2.2.m1.1"><semantics id="A2.T11.2.2.2.m1.1a"><mrow id="A2.T11.2.2.2.m1.1.1" xref="A2.T11.2.2.2.m1.1.1.cmml"><msub id="A2.T11.2.2.2.m1.1.1.2" xref="A2.T11.2.2.2.m1.1.1.2.cmml"><mi id="A2.T11.2.2.2.m1.1.1.2.2" xref="A2.T11.2.2.2.m1.1.1.2.2.cmml">Î´</mi><mn id="A2.T11.2.2.2.m1.1.1.2.3" xref="A2.T11.2.2.2.m1.1.1.2.3.cmml">1</mn></msub><mo id="A2.T11.2.2.2.m1.1.1.1" lspace="0.108em" stretchy="false" xref="A2.T11.2.2.2.m1.1.1.1.cmml">â†‘</mo><mi id="A2.T11.2.2.2.m1.1.1.3" xref="A2.T11.2.2.2.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T11.2.2.2.m1.1b"><apply id="A2.T11.2.2.2.m1.1.1.cmml" xref="A2.T11.2.2.2.m1.1.1"><ci id="A2.T11.2.2.2.m1.1.1.1.cmml" xref="A2.T11.2.2.2.m1.1.1.1">â†‘</ci><apply id="A2.T11.2.2.2.m1.1.1.2.cmml" xref="A2.T11.2.2.2.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T11.2.2.2.m1.1.1.2.1.cmml" xref="A2.T11.2.2.2.m1.1.1.2">subscript</csymbol><ci id="A2.T11.2.2.2.m1.1.1.2.2.cmml" xref="A2.T11.2.2.2.m1.1.1.2.2">ğ›¿</ci><cn id="A2.T11.2.2.2.m1.1.1.2.3.cmml" type="integer" xref="A2.T11.2.2.2.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="A2.T11.2.2.2.m1.1.1.3.cmml" xref="A2.T11.2.2.2.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.2.2.2.m1.1c">\delta_{1}\!\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.2.2.2.m1.1d">italic_Î´ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â†‘</annotation></semantics></math></td><td id="A2.T11.3.3.3">Log<math alttext="{}_{10}\!\downarrow" display="inline" id="A2.T11.3.3.3.m1.1"><semantics id="A2.T11.3.3.3.m1.1a"><mmultiscripts id="A2.T11.3.3.3.m1.1.1"><mo id="A2.T11.3.3.3.m1.1.1.2" stretchy="false">â†“</mo><mprescripts id="A2.T11.3.3.3.m1.1.1a"></mprescripts><mn id="A2.T11.3.3.3.m1.1.1.3">10</mn><mrow id="A2.T11.3.3.3.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A2.T11.3.3.3.m1.1b">{}_{10}\!\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.3.3.3.m1.1c">start_FLOATSUBSCRIPT 10 end_FLOATSUBSCRIPT â†“</annotation></semantics></math></td><td id="A2.T11.4.4.4">AbsRel<math alttext="\downarrow" display="inline" id="A2.T11.4.4.4.m1.1"><semantics id="A2.T11.4.4.4.m1.1a"><mo id="A2.T11.4.4.4.m1.1.1" stretchy="false" xref="A2.T11.4.4.4.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T11.4.4.4.m1.1b"><ci id="A2.T11.4.4.4.m1.1.1.cmml" xref="A2.T11.4.4.4.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.4.4.4.m1.1d">â†“</annotation></semantics></math></td><td id="A2.T11.5.5.5"><math alttext="\delta_{1}\!\uparrow" display="inline" id="A2.T11.5.5.5.m1.1"><semantics id="A2.T11.5.5.5.m1.1a"><mrow id="A2.T11.5.5.5.m1.1.1" xref="A2.T11.5.5.5.m1.1.1.cmml"><msub id="A2.T11.5.5.5.m1.1.1.2" xref="A2.T11.5.5.5.m1.1.1.2.cmml"><mi id="A2.T11.5.5.5.m1.1.1.2.2" xref="A2.T11.5.5.5.m1.1.1.2.2.cmml">Î´</mi><mn id="A2.T11.5.5.5.m1.1.1.2.3" xref="A2.T11.5.5.5.m1.1.1.2.3.cmml">1</mn></msub><mo id="A2.T11.5.5.5.m1.1.1.1" lspace="0.108em" stretchy="false" xref="A2.T11.5.5.5.m1.1.1.1.cmml">â†‘</mo><mi id="A2.T11.5.5.5.m1.1.1.3" xref="A2.T11.5.5.5.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T11.5.5.5.m1.1b"><apply id="A2.T11.5.5.5.m1.1.1.cmml" xref="A2.T11.5.5.5.m1.1.1"><ci id="A2.T11.5.5.5.m1.1.1.1.cmml" xref="A2.T11.5.5.5.m1.1.1.1">â†‘</ci><apply id="A2.T11.5.5.5.m1.1.1.2.cmml" xref="A2.T11.5.5.5.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T11.5.5.5.m1.1.1.2.1.cmml" xref="A2.T11.5.5.5.m1.1.1.2">subscript</csymbol><ci id="A2.T11.5.5.5.m1.1.1.2.2.cmml" xref="A2.T11.5.5.5.m1.1.1.2.2">ğ›¿</ci><cn id="A2.T11.5.5.5.m1.1.1.2.3.cmml" type="integer" xref="A2.T11.5.5.5.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="A2.T11.5.5.5.m1.1.1.3.cmml" xref="A2.T11.5.5.5.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.5.5.5.m1.1c">\delta_{1}\!\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.5.5.5.m1.1d">italic_Î´ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â†‘</annotation></semantics></math></td><td id="A2.T11.6.6.6">Log<math alttext="{}_{10}\!\downarrow" display="inline" id="A2.T11.6.6.6.m1.1"><semantics id="A2.T11.6.6.6.m1.1a"><mmultiscripts id="A2.T11.6.6.6.m1.1.1"><mo id="A2.T11.6.6.6.m1.1.1.2" stretchy="false">â†“</mo><mprescripts id="A2.T11.6.6.6.m1.1.1a"></mprescripts><mn id="A2.T11.6.6.6.m1.1.1.3">10</mn><mrow id="A2.T11.6.6.6.m1.1.1b"></mrow></mmultiscripts><annotation encoding="application/x-tex" id="A2.T11.6.6.6.m1.1b">{}_{10}\!\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.6.6.6.m1.1c">start_FLOATSUBSCRIPT 10 end_FLOATSUBSCRIPT â†“</annotation></semantics></math></td><td id="A2.T11.7.7.7">AbsRel<math alttext="\downarrow" display="inline" id="A2.T11.7.7.7.m1.1"><semantics id="A2.T11.7.7.7.m1.1a"><mo id="A2.T11.7.7.7.m1.1.1" stretchy="false" xref="A2.T11.7.7.7.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="A2.T11.7.7.7.m1.1b"><ci id="A2.T11.7.7.7.m1.1.1.cmml" xref="A2.T11.7.7.7.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.7.7.7.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.7.7.7.m1.1d">â†“</annotation></semantics></math></td><td id="A2.T11.8.8.8"><math alttext="\delta_{1}\!\uparrow" display="inline" id="A2.T11.8.8.8.m1.1"><semantics id="A2.T11.8.8.8.m1.1a"><mrow id="A2.T11.8.8.8.m1.1.1" xref="A2.T11.8.8.8.m1.1.1.cmml"><msub id="A2.T11.8.8.8.m1.1.1.2" xref="A2.T11.8.8.8.m1.1.1.2.cmml"><mi id="A2.T11.8.8.8.m1.1.1.2.2" xref="A2.T11.8.8.8.m1.1.1.2.2.cmml">Î´</mi><mn id="A2.T11.8.8.8.m1.1.1.2.3" xref="A2.T11.8.8.8.m1.1.1.2.3.cmml">1</mn></msub><mo id="A2.T11.8.8.8.m1.1.1.1" lspace="0.108em" stretchy="false" xref="A2.T11.8.8.8.m1.1.1.1.cmml">â†‘</mo><mi id="A2.T11.8.8.8.m1.1.1.3" xref="A2.T11.8.8.8.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T11.8.8.8.m1.1b"><apply id="A2.T11.8.8.8.m1.1.1.cmml" xref="A2.T11.8.8.8.m1.1.1"><ci id="A2.T11.8.8.8.m1.1.1.1.cmml" xref="A2.T11.8.8.8.m1.1.1.1">â†‘</ci><apply id="A2.T11.8.8.8.m1.1.1.2.cmml" xref="A2.T11.8.8.8.m1.1.1.2"><csymbol cd="ambiguous" id="A2.T11.8.8.8.m1.1.1.2.1.cmml" xref="A2.T11.8.8.8.m1.1.1.2">subscript</csymbol><ci id="A2.T11.8.8.8.m1.1.1.2.2.cmml" xref="A2.T11.8.8.8.m1.1.1.2.2">ğ›¿</ci><cn id="A2.T11.8.8.8.m1.1.1.2.3.cmml" type="integer" xref="A2.T11.8.8.8.m1.1.1.2.3">1</cn></apply><csymbol cd="latexml" id="A2.T11.8.8.8.m1.1.1.3.cmml" xref="A2.T11.8.8.8.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T11.8.8.8.m1.1c">\delta_{1}\!\uparrow</annotation><annotation encoding="application/x-llamapun" id="A2.T11.8.8.8.m1.1d">italic_Î´ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT â†‘</annotation></semantics></math></td></tr><tr id="A2.T11.8.10.2"><td id="A2.T11.8.10.2.1">3A (Ours)</td><td id="A2.T11.8.10.2.2"><span id="A2.T11.8.10.2.2.1">0.149</span></td><td id="A2.T11.8.10.2.3"><span id="A2.T11.8.10.2.3.1">83.6</span></td><td id="A2.T11.8.10.2.4"><span id="A2.T11.8.10.2.4.1">0.072</span></td><td id="A2.T11.8.10.2.5"><span id="A2.T11.8.10.2.5.1">0.235</span></td><td id="A2.T11.8.10.2.6"><span id="A2.T11.8.10.2.6.1">81.3</span></td><td id="A2.T11.8.10.2.7"><span id="A2.T11.8.10.2.7.1">0.092</span></td><td id="A2.T11.8.10.2.8"><span id="A2.T11.8.10.2.8.1">0.303</span></td><td id="A2.T11.8.10.2.9"><span id="A2.T11.8.10.2.9.1">72.9</span></td></tr><tr id="A2.T11.8.11.3"><td id="A2.T11.8.11.3.1">3B</td><td id="A2.T11.8.11.3.2"><span id="A2.T11.8.11.3.2.1">0.148</span></td><td id="A2.T11.8.11.3.3"><span id="A2.T11.8.11.3.3.1">83.9</span></td><td id="A2.T11.8.11.3.4"><span id="A2.T11.8.11.3.4.1">0.073</span></td><td id="A2.T11.8.11.3.5"><span id="A2.T11.8.11.3.5.1">0.245</span></td><td id="A2.T11.8.11.3.6"><span id="A2.T11.8.11.3.6.1">81.3</span></td><td id="A2.T11.8.11.3.7"><span id="A2.T11.8.11.3.7.1">0.095</span></td><td id="A2.T11.8.11.3.8"><span id="A2.T11.8.11.3.8.1">0.292</span></td><td id="A2.T11.8.11.3.9"><span id="A2.T11.8.11.3.9.1">72.1</span></td></tr><tr id="A2.T11.8.12.4"><td id="A2.T11.8.12.4.1">3C</td><td id="A2.T11.8.12.4.2"><span id="A2.T11.8.12.4.2.1">0.153</span></td><td id="A2.T11.8.12.4.3"><span id="A2.T11.8.12.4.3.1">83.6</span></td><td id="A2.T11.8.12.4.4"><span id="A2.T11.8.12.4.4.1">0.166</span></td><td id="A2.T11.8.12.4.5"><span id="A2.T11.8.12.4.5.1">0.386</span></td><td id="A2.T11.8.12.4.6"><span id="A2.T11.8.12.4.6.1">37.1</span></td><td id="A2.T11.8.12.4.7"><span id="A2.T11.8.12.4.7.1">0.586</span></td><td id="A2.T11.8.12.4.8"><span id="A2.T11.8.12.4.8.1">0.712</span></td><td id="A2.T11.8.12.4.9"><span id="A2.T11.8.12.4.9.1">0.5</span></td></tr></tbody></table>

Finally, we assess the efficacy of our complete training curriculum in comparison to alternatives. Condition 3A represents our two-stage curriculum. Condition 3B trains in a single stage and applies all the second-stage gradient losses throughout the whole training. Condition 3C reverses our two stages and represents the established strategy of pretraining on synthetic data first and fine-tuning with real-world data.

### B.5 Focal length estimation

Additional analysis of zero-shot focal length estimation accuracy. In Fig.Â [10](https://arxiv.org/html/2410.02073v1#A2.F10 "Figure 10 â€£ B.5 Focal length estimation â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), we present a more comprehensive analysis of our focal length predictorâ€™s performance compared to baseline models. To that end, we plot the percentage of samples below a certain absolute relative error for each method and dataset in our zero-shot evaluation set up. Depth Pro outperforms all approaches on all datasets.

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/x56.png)

Figure 10: Evaluation of focal length estimation. Each plot compares a number of methods on a given dataset. The xğ‘¥xitalic\_x axis represents the AbsRel error and the yğ‘¦yitalic\_y axis represents the percentage of samples whose error is below that magnitude.

Table 12: Ablation study on focal length estimation.

Controlled evaluation of network structures. We evaluate a number of choices for the focal length estimation head and report results in Tab.Â [12](https://arxiv.org/html/2410.02073v1#A2.T12 "Table 12 â€£ B.5 Focal length estimation â€£ Appendix B Controlled Experiments â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"). The models are evaluated on 500 images randomly sampled from FlickrÂ (Thomee etÂ al., [2016](https://arxiv.org/html/2410.02073v1#bib.bib99)). The first row, â€œDPT onlyâ€ shows the performance of a network with the frozen DPT depth feature extractorÂ (Ranftl etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib82)) and a convolutional head. The second row, â€œViT onlyâ€, demonstrates the performance of only using a small ViT network trained from scratchÂ (Dosovitskiy etÂ al., [2021](https://arxiv.org/html/2410.02073v1#bib.bib22)). The third row, â€œDPT & ViT in seriesâ€, uses the frozen DPT feature extractor followed by a small ViT network. The fourth row, â€œDPT & ViT in parallelâ€, represents our chosen architecture depicted in Fig.Â [3](https://arxiv.org/html/2410.02073v1#S3.F3 "Figure 3 â€£ 3.1 Network â€£ 3 Method â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second"), which utilizes frozen features from the depth network and task-specific features from a separate ViT image encoder in parallel.

We observe that â€œDPT & ViT in seriesâ€ exhibits similar performance to â€œDPT onlyâ€, suggesting that adding more computation on top of the frozen DPT feature in addition to our small convolutional head does not provide extra benefits despite the increased computation. When comparing â€œDPT & ViT in seriesâ€ with â€œDPT & ViT in parallelâ€, we observe an accuracy difference of 14.6 percentage points. This indicates that accurate focal length prediction requires extra task-specific knowledge in addition to depth information. Furthermore, â€œDPT & ViT in parallelâ€ outperforms â€œViT onlyâ€, which highlights the importance of features from the pretrained depth network for obtaining a high-performing focal length estimator.

## Appendix C Implementation, Training and Evaluation Details

In this section we provide additional details on the datasets used for training and evaluation, hyperparameter settings for our method, and details on the evaluation setup.

### C.1 Datasets

Tab.Â [13](https://arxiv.org/html/2410.02073v1#A3.T13 "Table 13 â€£ C.1 Datasets â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") provides a comprehensive summary of the datasets utilized in our study, detailing their respective licenses and specifying their roles (e.g., training or testing).

Table 13: Datasets used in this work.

### C.2 Training hyperparameters.

We specify the training hyperparameters in Tab.Â [14](https://arxiv.org/html/2410.02073v1#A3.T14 "Table 14 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") and Tab.Â [15](https://arxiv.org/html/2410.02073v1#A3.T15 "Table 15 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second").

Table 14: Depth Pro model training hyperparameters.

Table 15: Training loss functions for different datasets and stages.

Table 16: Dataset evaluation setup.For each metric depth dataset in our evaluation, we report the range of valid depth values, number of samples, and resolution of ground truth depth maps. Due to the large size of the validation set (approximately 35K samples), we used a randomly sampled subset of NuScenes.

### C.3 Baselines

Below we provide further detail on the setup of the baselines.

DepthAnything. Depth Anything v1 and v2 each released a general model for _relative_ depth, but their _metric_ depth models are tailored to specific domains (indoor vs.Â outdoor). For the metric depth evaluation, we match these models to datasets according to their domain, and for datasets containing both indoor and outdoor images, we select the model with the best performance. For qualitative results and the (scale and shift invariant) zero-shot boundary evaluation, we employ the relative depth models, since they yield better qualitative results and sharper boundaries than the metric models.

Metric3D. For Metric3D v1 and v2, we found that the crop size parameter strongly affects metric scale accuracy. In fact, using a fixed crop size consistently yielded very poor results on at least one metric dataset. In order to obtain acceptable results, we used different crop sizes for indoor (512, 1088) and outdoor (512, 992) datasets. As in the case of Depth Anything, we mark these results in gray to indicate that they are not strictly zero-shot. For Metric 3D v2, we use the largest (â€˜giantâ€™) model.

UniDepth. For UniDepth, we use the ViT-L version, which performs best on average among the UniDepth variants.

ZoeDepth. We use the model finetuned on both indoor and outdoor data (denoted ZoeD\_NK).

### C.4 Evaluation setup

In evaluating our approach and baselines, we found the range of valid depth values, the depth map resolution used for computing metrics, the resizing approach used for matching the resolution of the ground truth depth maps, and the choice of intrinsics to affect results, sometimes strongly. This is why we made an effort to setup and evaluate each baseline in the same, fair evaluation set up, which we detail below.

Tab.Â [16](https://arxiv.org/html/2410.02073v1#A3.T16 "Table 16 â€£ C.2 Training hyperparameters. â€£ Appendix C Implementation, Training and Evaluation Details â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") lists our evaluation datasets, the range of depth values used for evaluation, the number of samples, and the resolution of the ground truth depth maps. In case a method predicted depth maps at a different resolution, we resized predictions bilinearly to match the ground truth resolution.

Since several factors outlined above can affect the reported accuracy of a method, few baselines report sufficient detail on their evaluation setup, and the exact evaluation setups may differ across baselines, it is generally impossible to exactly reproduce reported results while guaranteeing fairness. We prioritized fair comparison and tried to evaluate all baselines in the same environment. We were able to match most reported results, with the following three notable differences. ZeroDepth reported better results on nuScenes, which we attribute to the use of a different validation set in their evaluation. UniDepth reported different results on ETH3D, which we attribute to the handling of raw images; specifically, in our setup, we use the raw images without any post-processing, and take the intrinsics from the accompanying EXIF data; we believe this best adheres to the zero-shot premise for single-image depth estimation. Finally, on SUN-RGBD, Depth Anything fairs better in our evaluation setup than in the evaluation reported in the original paper.

Evaluation metric for sharp boundaries. For both our depth-based and mask-based boundary metrics, we apply the same weighted-averaging strategy to account for multiple relative depth ratios. F1 values (depth-based metrics) and Recall values (mask-based metrics) are averaged across thresholds that range linearly from tmâ¢iâ¢n\=5subscriptğ‘¡ğ‘šğ‘–ğ‘›5t\_{min}=5italic\_t start\_POSTSUBSCRIPT italic\_m italic\_i italic\_n end\_POSTSUBSCRIPT = 5 to tmâ¢aâ¢x\=25subscriptğ‘¡ğ‘šğ‘ğ‘¥25t\_{max}=25italic\_t start\_POSTSUBSCRIPT italic\_m italic\_a italic\_x end\_POSTSUBSCRIPT = 25. Weights are computed as the normalized range of threshold values between tmâ¢iâ¢nsubscriptğ‘¡ğ‘šğ‘–ğ‘›t\_{min}italic\_t start\_POSTSUBSCRIPT italic\_m italic\_i italic\_n end\_POSTSUBSCRIPT and tmâ¢aâ¢xsubscriptğ‘¡ğ‘šğ‘ğ‘¥t\_{max}italic\_t start\_POSTSUBSCRIPT italic\_m italic\_a italic\_x end\_POSTSUBSCRIPT, such that stronger weights are given towards high threshold values.

## Appendix D Applications

Metric, sharp, and fast monocular depth estimation enables a variety of downstream applications. We showcase the utility of Depth Pro in two additional contexts beyond novel view synthesis: conditional image synthesis with ControlNet (Zhang etÂ al., [2023b](https://arxiv.org/html/2410.02073v1#bib.bib121)) and synthetic depth of field (Peng etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib73)).

Depth-conditioned image synthesis. In this application we stylize an image through a text prompt via ControlNetÂ (Zhang etÂ al., [2023b](https://arxiv.org/html/2410.02073v1#bib.bib121)). To retain the structure of the input image, we predict a depth map from the input image and use it for conditioning the image synthesis through a pre-trained depth-to-image ControlNet SD 1.5 model. FigureÂ [11](https://arxiv.org/html/2410.02073v1#A4.F11 "Figure 11 â€£ Appendix D Applications â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") shows the input image, prompt, and predicted depth maps and synthesis results for Depth Pro, Deoth Anything v2, Marigold, and Metric3D v2. We find that only Depth Pro accurately predicts the cables and sky region, resulting in a stylized image that retains the structure of the input image. Baselines either miss cables, causing the cable car to float mid-air (Depth Anything v2), or add a gradient to the sky (Marigold).

Figure 11: Comparison on conditional image synthesis. We use ControlNetÂ (Zhang etÂ al., [2023a](https://arxiv.org/html/2410.02073v1#bib.bib120)) to synthesize a stylized image given a prompt (top row, right) and a depth map. The depth map is predicted from the input imageÂ (Li etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib56)) (top row, left) via Depth Pro, and baselines. The left column shows depth maps, the right column the synthesized image. For the baselines, missing cables (Depth Anything v2 & Matric3D v2) or a spurious gradient in the sky (Marigold) alter the scene structure of the synthesized image.

Synthetic depth of field. Synthetic depth of field can be used to highlight the primary subject in a photo by deliberately blurring the surrounding areas. BokehMeÂ (Peng etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib73)) introduces a hybrid rendering framework that marries a neural renderer with a classical physically motivated renderer. This framework takes a single image along with a depth map as input. In this context, it is essential for the depth map to delineate objects well, such that the photoâ€™s subject is kept correctly in focus while other content is correctly blurred out. Furthermore, the depth map should correctly trace out the details of the subject, to deep these (and only these) details correctly in focus. Figure [12](https://arxiv.org/html/2410.02073v1#A4.F12 "Figure 12 â€£ Appendix D Applications â€£ Depth Pro: Sharp Monocular Metric Depth in Less Than a Second") shows the advantage afforded by Depth Pro in this application. (We keep the most salient object in focus by setting the refocused disparity (disp\_focus) hyperparameter of BokehMe as the disparity of the object.)

<table id="A4.F12.15"><tbody><tr id="A4.F12.15.1.1"><td id="A4.F12.15.1.1.1"></td><td id="A4.F12.15.1.1.2"></td></tr></tbody></table>

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_reference.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_disparity_depthpro.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_disparity_marigold.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_disparity_da.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_depthpro.jpg)

Depth Pro (Ours)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_marigold.jpg)

Marigold

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/366_out/366_da.jpg)

Depth Anything v2

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_reference.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_disparity_depthpro.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_disparity_marigold.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_disparity_da.jpg)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_depthpro.jpg)

Depth Pro (Ours)

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_marigold.jpg)

Marigold

![Refer to caption](chrome-extension://pcmpcfapbekmbjjkdalcgopdkipoggdi/html/2410.02073v1/extracted/5897170/fig/out-of-focus/415_out/415_da.jpg)

Depth Anything v2

Figure 12: Comparison on synthetic depth of field. We compare the synthetic depth of field produced by BokehMeÂ (Peng etÂ al., [2022a](https://arxiv.org/html/2410.02073v1#bib.bib73)) using depth maps from Depth Pro, MarigoldÂ (Ke etÂ al., [2024](https://arxiv.org/html/2410.02073v1#bib.bib45)), and Depth Anything v2Â (Yang etÂ al., [2024a](https://arxiv.org/html/2410.02073v1#bib.bib112)). Zoom in for detail.