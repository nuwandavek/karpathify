                 Preprint
                 DEPTHPRO:SHARPMONOCULARMETRIC
                 DEPTHINLESSTHANASECOND
                                ¨
                   AlekseiBochkovskiiAmaelDelaunoyHugoGermainMarcelSantos
                      YichaoZhouStephanR.RichterVladlenKoltun
                                   Apple
                                  ABSTRACT
                     Wepresentafoundationmodelforzero-shotmetricmonoculardepthestimation.
                     Ourmodel,DepthPro,synthesizeshigh-resolutiondepthmapswithunparalleled
                     sharpnessandhigh-frequencydetails.Thepredictionsaremetric,withabsolute
                     scale,withoutrelyingontheavailabilityofmetadatasuchascameraintrinsics.
                     Andthemodelisfast,producinga2.25-megapixeldepthmapin0.3secondsona
                     standardGPU.Thesecharacteristicsareenabledbyanumberoftechnicalcontribu-
                     tions,includinganefﬁcientmulti-scalevisiontransformerfordenseprediction,a
                     trainingprotocolthatcombinesrealandsyntheticdatasetstoachievehighmetric
                     accuracyalongsideﬁneboundarytracing,dedicatedevaluationmetricsforbound-
                     aryaccuracyinestimateddepthmaps,andstate-of-the-artfocallengthestimation
                     fromasingleimage.Extensiveexperimentsanalyzespeciﬁcdesignchoicesand
                     demonstratethatDepthProoutperformspriorworkalongmultipledimensions.We
                     releasecode&weightsathttps://github.com/apple/ml-depth-pro
                 1INTRODUCTION
                 Zero-shotmonoculardepthestimationunderpinsagrowingvarietyofapplications,suchasadvanced
                 imageediting,viewsynthesis,andconditionalimagegeneration.InspiredbyMiDaS(Ranftletal.,
                 2022)andmanyfollow-upworks(Ranftletal.,2021;Keetal.,2024;Yangetal.,2024a;Piccinelli
                 etal.,2024;Huetal.,2024),applicationsincreasinglyleveragetheabilitytoderiveadensepixelwise
                 depthmapforanyimage.
                 Ourworkismotivatedinparticularbynovelviewsynthesisfromasingleimage,anexciting
                 applicationthathasbeentransformedbyadvancesinmonoculardepthestimation(Hedmanetal.,
                 2017;Shihetal.,2020;Jampanietal.,2021;Khanetal.,2023).Applicationssuchasviewsynthesis
                 implyanumberofdesiderataformonoculardepthestimation.First,thedepthestimatorshould
         arXiv:2410.02073v1  [cs.CV]  2 Oct 2024workzero-shotonanyimage,notrestrictedtoaspeciﬁcdomain(Ranftletal.,2022;Yangetal.,
                 2024a).Furthermore,themethodshouldideallyproducemetricdepthmapsinthiszero-shotregime,
                 toaccuratelyreproduceobjectshapes,scenelayouts,andabsolutescales(Guizilinietal.,2023;Hu
                 etal.,2024).Forthebroadestapplicability‘inthewild’,themethodshouldproducemetricdepth
                 mapswithabsolutescaleevenifnocameraintrinsics(suchasfocallength)areprovidedwiththe
                 image(Piccinellietal.,2024).Thisenablesviewsynthesisscenariossuchas“Synthesizeaviewof
                 thisscenefrom63mmaway”foressentiallyarbitrarysingleimages(Dodgson,2004).
                 Second,forthemostcompellingresults,themonoculardepthestimatorshouldoperateathigh
                 resolutionandproduceﬁne-graineddepthmapsthatcloselyadheretoimagedetailssuchashair,fur,
                 andotherﬁnestructures(Miangolehetal.,2021;Keetal.,2024;Lietal.,2024a).Onebeneﬁtof
                 producingsharpdepthmapsthataccuratelytraceintricatedetailsistheeliminationof“ﬂyingpixels”,
                 whichcandegradeimagequalityinapplicationssuchasviewsynthesis(Jampanietal.,2021).
                 Third,formanyinteractiveapplicationscenarios,thedepthestimatorshouldoperateatlowla-
                 tency,processingahigh-resolutionimageinlessthanasecondtosupportinteractiveviewsynthe-
                 sis“queries”ondemand.Lowlatencyisacommoncharacteristicofmethodsthatreducezero-
                 shotmonoculardepthestimationtoasingleforwardpassthroughaneuralnetwork(Ranftletal.,
                                     1
                                       Preprint
                                          Input
                                          DepthPro
                                          Marigold
                                          DepthAnythingv2
                                          Metric3Dv2
                                       Figure1:ResultsonimagesfromtheAM-2k(Lietal.,2022a)(1st&3rdcolumn)andDIS-5k(Qin
                                       etal.,2022)(2ndcolumn)datasets.Inputimageontop,estimateddepthmapsfromDepthPro,
                                       Marigold(Keetal.,2024),DepthAnythingv2(Yangetal.,2024b),andMetric3Dv2(Huetal.,
                                       2024)below.DepthProproduceszero-shotmetricdepthmapswithabsolutescaleat2.25-megapixel
                                       nativeresolutionin0.3secondsonaV100GPU.
                                       2021;Yangetal.,2024a;Piccinellietal.,2024),butitisnotalwayssharedbymethodsthat
                                       employmorecomputationallydemandingmachineryattesttime(Keetal.,2024;Lietal.,2024a).
                                        Inthiswork,wepresentafoundationmodel
                                       forzero-shotmetricmonoculardepthestimation
                                                                                                                            DepthPro
                                                                                                                            DPT
                                                                                           15
                                       thatmeetsallofthesedesiderata.Ourmodel,
                                                                                          0.                                DepthAnything-v2
                                                                                                                            UniDepth
                                       DepthPro,producesmetricdepthmapswith
                                                                                          0.10                              Metric3D
                                                                                                                            Metric3D-v2
                                       absolutescaleonarbitraryimages‘inthewild’
                                                                                           .05                              Marigold
                                                                                          0                                 PatchFusion
                                       withoutrequiringmetadatasuchascamerain-
                                                                                         BoundaryRecall0.00
                                       trinsics.Itoperatesathighresolution,producing
                                                                                              0      1      2 5   75100
                                                                                                     Runtime(s)
                                       2.25-megapixeldepthmaps(withanativeout-
                                       putresolutionof1536×1536beforeoptional
                                                                                       Figure2:Boundaryrecallversusruntime.Depth
                                       furtherupsampling)in0.3secondsonaV100
                                                                                       Prooutperformspriorworkbyamultiplicative
                                       GPU.Fig.1showssomerepresentativeresults.
                                                                                       factorinboundaryaccuracywhilebeingorders
                                       DepthProdramaticallyoutperformsallprior
                                                                                       ofmagnitudefasterthanworksfocusingonﬁne-
                                       workinsharpdelineationofobjectboundaries,
                                                                                       grainedpredictions(e.g.,Marigold,PatchFusion).
                                       includingﬁnestructuressuchashair,fur,and
                                                                                      2
                 Preprint
                 vegetation.AsshowninFig.2,DepthProoffersunparalleledboundarytracing,outperformingall
                 priorworkbyamultiplicativefactorinboundaryrecall.(SeeSec.4foradditionaldetail.)Compared
                 tothepriorstateoftheartinboundaryaccuracy(Keetal.,2024;Lietal.,2024a),DepthProisone
                 totwoordersofmagnitudefaster,yieldsmuchmoreaccurateboundaries,andprovidesmetricdepth
                 mapswithabsolutescale.
                 DepthProisenabledbyanumberoftechnicalcontributions.First,wedesignanefﬁcientmulti-scale
                 ViT-basedarchitectureforcapturingtheglobalimagecontextwhilealsoadheringtoﬁnestructuresat
                 highresolution.Second,wederiveanewsetofmetricsthatenableleveraginghighlyaccuratematting
                 datasetsforquantifyingtheaccuracyofboundarytracinginevaluatingmonoculardepthmaps.Third,
                 wedeviseasetoflossfunctionsandatrainingcurriculumthatpromotesharpdepthestimateswhile
                 trainingonreal-worlddatasetsthatprovidecoarseandinaccuratesupervisionaroundboundaries,
                 alongwithsyntheticdatasetsthatofferaccuratepixelwisegroundtruthbutlimitedrealism.Fourth,
                 wecontributezero-shotfocallengthestimationfromasingleimagethatdramaticallyoutperforms
                 thepriorstateoftheart.
                 2RELATEDWORK
                 Earlyworkonmonoculardepthestimationfocusedontrainingonindividualdatasetsrecordedwitha
                 singlecamera(Saxenaetal.,2009;Eigenetal.,2014;Eigen&Fergus,2015).Althoughthissetup
                 directlyenabledmetricdepthpredictions,itwaslimitedtosingledatasetsandnarrowdomains.
                 Zero-shotdepthestimation.MegaDepth(Li&Snavely,2018)demonstratedthattrainingon
                 adiversedatasetallowsgeneralizingmonoculardepthpredictionbeyondaspeciﬁcdomain.Mi-
                 DaS(Ranftletal.,2022)advancedthisideabytrainingonalargemixofdiversedatasetswitha
                 scale-and-shift-invariantloss.Follow-upworksappliedthisrecipetotransformer-basedarchitec-
                 tures(Ranftletal.,2021;Birkletal.,2023)andfurtherexpandedthesetoffeasibledatasetsthrough
                 self-supervision(Spenceretal.,2023;Yangetal.,2024a).Alineofworkusesself-supervisionto
                 learnfromunlabeledimageandvideodata(Petrovai&Nedevschi,2022;Yangetal.,2024a).A
                 numberofrecentapproaches(Keetal.,2024;Guietal.,2024)harnessdiffusionmodelstosynthesize
                 relativedepthmaps.Althoughsomeofthesemethodsdemonstratedexcellentgeneralization,their
                 predictionsareambiguousinscaleandshift,whichprecludesdownstreamapplicationsthatrequire
                 accurateshapes,sizes,ordistances.
                 Zero-shotmetricdepth.Alineofworksoughttoimprovemetricdepthpredictionthroughaglobal
                 distributionofdepthvalues(Fuetal.,2018;Bhatetal.,2021;2022;Lietal.,2024b)andfurther
                 conditioningonscenetype(Bhatetal.,2023).Adifferentapproachdirectlytakesintoaccount
                              ´
                 cameraintrinsics.Cam-Convs(Faciletal.,2019)conditionedconvolutionsonthecameraintrinsics.
                 LeReS(Yinetal.,2021)trainsaseparatenetworkforundistortingpointcloudstorecoverscale
                 andshift,Metric3D(Yinetal.,2023)scalesimagesordepthmapstoacanonicalspaceandremaps
                 estimateddepthgiventhefocallength,andZeroDepth(Guizilinietal.,2023)learnscamera-speciﬁc
                 embedddingsinavariationalframework.DMD(Saxenaetal.,2023)conditionsadiffusionmodelon
                 theﬁeldofview.Metric3Dv2(Huetal.,2024)leveragessurfacenormalsasanauxilliaryoutputto
                 improvemetricdepth.Allofthesemethodsrequirethecameraintrinsicstobeknownandaccurate.
                 Morerecentworksattempttoreasonaboutunknowncameraintrinsicseitherthroughaseparate
                 network(Spenceretal.,2024)orbypredictingacameraembeddingforconditioningitsdepth
                 predictionsinasphericalspace(Piccinellietal.,2024).Akintotheserecentapproaches,ourmethod
                 doesnotrequirethefocallengthtobeprovidedasinput.Weproposetodirectlyestimatetheﬁeld
                 ofviewfromintermediatefeaturesofthedepthpredictionnetwork,andshowthatthissubstantially
                 outperformsthepriorstateoftheartinthetaskofcross-domainfocallengthestimation.
                 Sharpoccludingcontours.SharpNet(Ramamonjisoa&Lepetit,2019)incorporatesnormalsand
                 occludingcontourconstraints,butrequiresadditionalcontourandnormalsupervisionduringtraining.
                 BoostingDepth(Miangolehetal.,2021)obtainsdetailedpredictionsfromalow-resolutionnetwork
                 byapplyingitindependentlytoimagepatches.Sincethepatcheslackglobalcontext,BoostingDepth
                 fusesthemthroughasophisticatedmulti-steppipeline.PatchFusion(Lietal.,2024a)reﬁnesthis
                 conceptthroughimage-adaptivepatchsamplingandtailoredmodulesthatenableend-to-endtraining.
                 Arecentlineofworkleveragesdiffusionpriorstoenhancethesharpnessofocclusionboundaries(Gui
                 etal.,2024;Keetal.,2024).Theseapproachespredominantlyfocusonpredictingrelative(rather
                                     3
                    Preprint
                    thanmetric)depth.Weproposeasimplerarchitecturewithouttask-speciﬁcmodulesordiffusion
                    priorsanddemonstratethatevensharperandmoreaccurateresultscanbeobtainedwhileproducing
                    metricdepthmapsandreducingruntimebymorethantwoordersofmagnitude.
                    Guideddepthsuper-resolutionusestheinputimagetoupsamplelow-resolutiondepthpredic-
                    tions(Metzgeretal.,2023;Zhongetal.,2023).SMDNet(Tosietal.,2021)predictsbimodal
                    mixturedensitiestosharpenoccludingcontours.AndRamamonjisoaetal.(Ramamonjisoaetal.,
                    2020)introduceamoduleforlearningtosharpendepthboundariesofapretrainednetwork.These
                    worksareorthogonaltooursandcouldbeappliedtofurtherupsampleourhigh-resolutionpredictions.
                    Toevaluateboundarytracinginpredicteddepthmaps,Kochetal.(2018)introducetheiBims
                    datasetwithmanualannotationsofoccludingcontoursandcorrespondingmetrics.Theneedfor
                    manualannotationandhighlyaccuratedepthgroundtruthconstrainthebenchmarktoasmallset
                    ofindoorscenes.Wecontributemetricsbasedonsegmentationandmattingdatasetsthatprovide
                    acomplementaryviewbyenablingevaluationoncomplex,dynamicenvironmentsorsceneswith
                    exceedinglyﬁnedetailforwhichground-truthdepthisimpossibletoobtain.
                    Multi-scalevisiontransformers.Visiontransformers(ViTs)haveemergedasthedominantgeneral-
                    purposearchitectureforperceptiontasks,butoperateatlowresolution(Dosovitskiyetal.,2021).
                     ¨
                    Naıvelyscalingthearchitecturetohigherresolutionsisprohibitiveduetothecomputationalcom-
                    plexity.SeveralworksidentiﬁedtheattentionlayersasthemainobstacletoscalingupViTandhave
                    proposedalternatives(Zhuetal.,2021;Liuetal.,2021;Lietal.,2022c;Chuetal.,2021;Liuetal.,
                    2022a;2023;Caietal.,2023;Jaegleetal.,2022).
                    AnotherlineofworkmodiﬁedtheViTarchitecturetoproduceahierarchyoffeatures(Fanetal.,
                    2021;Xieetal.,2021;Yuanetal.,2021;Ranftletal.,2021;Chenetal.,2021;Leeetal.,2022).
                    RatherthanmodifyingtheViTarchitecture,whichrequirescomputationallyexpensiveretraining,
                    weproposeanetworkarchitecturethatappliesaplainViTbackboneatmultiplescalesandfuses
                    predictionsintoasinglehigh-resolutionoutput.Thisarchitecturebeneﬁtsfromongoingimprovements
                    inViTpretraining,asnewvariantscanbeeasilyswappedin(Oquabetal.,2024;Pengetal.,2022b;
                    Sunetal.,2023).
                    Pretrainedvisiontransformershavebeenadaptedforsemanticsegmenationandobjectdetection.
                    ViT-Adapter(Chenetal.,2023)andViT-CoMer(Xiaetal.,2024)supplementapretrainedViTwith
                    aconvolutionalnetworkfordenseprediction,whereasViT-Det(Lietal.,2022b)buildsafeature
                    pyramidontopofapretrainedViT.Distinctfromthese,wefusefeaturesfromtheViTappliedat
                    multiplescalestolearnglobalcontexttogetherwithlocaldetail.
                    3METHOD
                    3.1NETWORK
                    ThekeyideaofourarchitectureistoapplyplainViTencoders(Dosovitskiyetal.,2021)on
                    patchesextractedatmultiplescalesandfusethepatchpredictionsintoasinglehigh-resolutiondense
                    predictioninanend-to-endtrainablemodel.Fig.3illustratesthearchitecture.Forpredictingdepth,
                    weemploytwoViTencoders,apatchencoderandanimageencoder.Thepatchencoderisapplied
                    onpatchesextractedatmultiplescales.Intuitively,thismayallowlearningrepresentationsthatare
                    scale-invariantasweightsaresharedacrossscales.Theimageencoderanchorsthepatchpredictions
                    inaglobalcontext.Itisappliedtothewholeinputimage,downsampledtothebaseinputresolution
                    ofthechosenencoderbackbone(inourcase384×384).
                    Thewholenetworkoperatesataﬁxedresolutionof1536×1536,whichwaschosenasamultipleof
                    theViT’s384×384.Thisguaranteesasufﬁcientlylargereceptiveﬁeldandconstantruntimesforany
                    imagewhilepreventingout-of-memoryerrors(whichwerepeatedlyobservedforvariable-resolution
                    approachesonlargeimages).Conﬁrmingthisdesignchoice,theresultswereportinSec.4and
                    Tab.5demonstratethatDepthProisconsistentlyordersofmagnitudefasterthanvariable-resolution
                    approacheswhilebeingmoreaccurateandproducingsharperboundaries.Akeybeneﬁtofassembling
                    ourarchitecturefromplainViTencodersovercustomencodersistheabundanceofpretrainedViT-
                    basedbackbonesthatcanbeharnessed(Oquabetal.,2024;Pengetal.,2022b;Sunetal.,2023).
                                           4
                                    Preprint
                                    Figure3:Overviewofthenetworkarchitecture.Animageisdownsampledatseveralscales.At
                                    eachscale,itissplitintopatches,whichareprocessedbyaViT-basedpatchencoder,withweights
                                    sharedacrossscales.Patchesaremergedintofeaturemaps,upsampled,andfusedviaaDPTdecoder.
                                    Predictionsareanchoredbyaseparateimageencoderthatprovidesglobalcontext.
                                    Weevaluateseveralpretrainedbackbonesandcompareourarchitecturetootherhigh-resolution
                                    architecturesintheappendices(Tab.8andSec.B.2).
                                    Afterdownsamplingto1536×1536,theinputimageissplitintopatchesof384×384.Forthetwo
                                    ﬁnestscales,weletpatchesoverlaptoavoidseams.Ateachscale,thepatchesarefedintothepatch
                                    encoder,whichproducesafeaturetensoratresolution24× 24perinputpatch(features3–6in
                                    Fig.3).Attheﬁnestscalewefurtherextractintermediatefeatures(features1&2inFig.3)tocapture
                                    ﬁner-graineddetails.Wemergethefeaturepatchesintomaps,whicharefedintothedecodermodule,
                                    whichresemblestheDPTdecoder(Ranftletal.,2021).
                                    Inadditiontosharingrepresentationsacrossscales,thepatch-basedapplicationoftheencoder
                                    networkallowstrivialparallelizationaspatchescanbeprocessedindependently.Anothersource
                                    ofcomputationalefﬁciencycomesfromthelowercomputationalcomplexityofpatch-basedpro-
                                    cessingincomparisontoscalinguptheViTtohigherresolutions.Thereasonismulti-headself-
                                    attention(Vaswanietal.,2017),whosecomputationalcomplexityscalesquadraticallywiththe
                                    numberofinputpixels,andthusquarticallyinimagedimension.
                                    3.2SHARPMONOCULARDEPTHESTIMATION
                                    Trainingobjectives.ForeachinputimageI,ournetworkf predictsacanonicalinversedepth
                                    imageC =f(I).ToobtainadensemetricdepthmapD ,wescalebythehorizontalﬁeldofview,
                                                                                    m
                                                                                                       fpx
                                    representedbythefocallengthf  andthewidthw (Yinetal.,2023):D     =    .
                                                                px                                 m   wC
                                    Wetrainwithseveralobjectives,allbasedoncanonicalinversedepth,becausethisprioritizes
                                    areasclosetothecameraoverfartherareasorthewholescene,andthussupportsvisualqualityin
                                                                            ˆ
                                    applicationssuchasnovelviewsynthesis.LetC betheground-truthcanonicalinversedepth.Forall
                                    metricdatasetswecomputethemeanabsoluteerror(L       ,Eq.1)perpixeli,anddiscardpixels
                                                                                    MAE
                                    withanerrorinthetop20%perimageforreal-world(asopposedtosynthetic)datasets:
                                                                                  N
                                                                       ˆ       1 ! ˆ
                                                                L    (C,C)=|C −C|.                                     (1)
                                                                 MAEi i
                                                                              N i
                                    Forallnon-metricdatasets(i.e.,thosewithoutreliablecameraintrinsicsorinconsistentscale),we
                                    normalizepredictionsandgroundtruthviathemeanabsolutedeviationfromthemedian(Ranftl
                                    etal.,2022)beforeapplyingaloss.Wefurthercomputeerrorsontheﬁrstandsecondderivativesof
                                    (canoncial)inversedepthmapsatmultiplescales.Let∇ indicateaspatialderivativeoperator∗,such
                                                                                   ∗
                                    asScharr(S)(Scharretal.,1997)orLaplace(L),andp theerrornorm.Wedeﬁnethemulti-scale
                                    derivativelossoverM scalesas
                                                                           M      Nj
                                                                  ˆ     1 ! 1 !           j      ˆj p
                                                        L     (C,C)= |∇C −∇C|,                                         (2)
                                                          ∗,p,M                        ∗ i     ∗ i
                                                                       M j Nj i
                                                                              5
                                            Preprint
                                            wherethescalesj arecomputedbyblurringanddownsamplingtheinversedepthmapsbyafactorof
                                            2perscale.AsshorthandswedeﬁnetheMeanAbsoluteGradientErrorL                          =L       ,theMean
                                                                                                                        MAGES,1,6
                                            AbsoluteLaplaceErrorL             =L        ,andtheMeanSquaredGradientErrorL                   =L       .
                                                                       MALEL,1,6                                                   MSGES,2,6
                                            Trainingcurriculum.Weproposeatrainingcurriculummotivatedbythefollowingobservations.
                                            First,trainingonalargemixofreal-worldandsyntheticdatasetsimprovesgeneralizationasmeasured
                                            byzero-shotaccuracy(Ranftletal.,2022;2021;Yangetal.,2024a;Huetal.,2024).Second,
                                            syntheticdatasetsprovidepixel-accurategroundtruth,whereasreal-worlddatasetsoftencontain
                                            missingareas,mismatcheddepth,orfalsemeasurementsonobjectboundaries.Third,predictionsget
                                            sharperoverthecourseoftraining.
                                            Basedontheseobservations,wedesignatwo-stagetrainingcurriculum.Intheﬁrststage,weaim
                                            tolearnrobustfeaturesthatallowthenetworktogeneralizeacrossdomains.Tothatend,wetrain
                                            onamixofalllabeledtrainingsets.Speciﬁcally,weminimizeL                       onmetricdatasetsandits
                                                                                                                   MAE
                                            normalizedversiononnon-metricdatasets.L              ischosenforitsrobustnessinhandlingpotentially
                                                                                           MAE
                                            corruptedreal-worldgroundtruth.Tosteerthenetworktowardssharpboundaries,weaimtoalso
                                                                                                 ¨
                                            superviseongradientsofthepredictions.Donenaıvely,however,thiscanhinderoptimizationand
                                            slowdownconvergence.Wefoundthatascale-and-shift-invariantlossongradients,appliedonlyto
                                            syntheticdatasets,workedbest.Controlledexperimentsarereportedintheappendices.
                                            Thesecondstageoftrainingisdesignedtosharpenboundariesandrevealﬁnedetailsinthepredicted
                                            depthmaps.Tominimizetheeffectofinaccurategroundtruth,atthisstageweonlytrainonsynthetic
                                            datasetsthatprovidehigh-qualitypixel-accurategroundtruth.(Notethatthisinvertsthecommon
                                                                                                                                               ´
                                            practiceofﬁrsttrainingonsyntheticdataandthenﬁne-tuningonrealdata(Gaidonetal.,2016;Gomez
                                            etal.,2023;Sunetal.,2021).)Speciﬁcally,weagainminimizetheL                      andsupplementitwith
                                                                                                                      MAE
                                            aselectionoflossesontheﬁrst-andsecond-orderderivatives:L                   , L       ,andL         .We
                                                                                                                MAGEMALEMSGE
                                            provideadetailedspeciﬁcationofthelossfunctionsthatareappliedateachstageintheappendices.
                                            Evaluationmetricsforsharpboundaries.Applicationssuchasnovelviewsynthesisrequiredepth
                                            mapstoadheretoobjectboundaries.Thisisparticularlychallengingforthinstructures.Misaligned
                                            orblurryboundariescanmakeobjectsappeardistortedorsplitintoparts.Commonbenchmarksfor
                                            monoculardepthpredictionrarelytakeboundarysharpnessintoaccount.Thismaybeattributed
                                            inparttothelackofdiverseandrealisticdatasetswithprecisepixel-accurateground-truthdepth.
                                            Toaddressthisshortcoming,weproposeanewsetofmetricsspeciﬁcallyfortheevaluationof
                                            depthboundaries.Ourkeyobservationisthatwecanleverageexistinghigh-qualityannotationsfor
                                            matting,saliency,orsegmentationasgroundtruthfordepthboundaries.Wetreatannotationsfor
                                            thesetasksasbinarymaps,whichdeﬁneaforeground/backgroundrelationshipbetweenanobjectand
                                            itsenvironment.(Thisrelationshipmaynotholdineverycase,especiallyforsegmentationmasks.
                                            However,wecaneasilydiscardsuchproblematiccasesthroughmanualinspection.Itismucheasier
                                            toﬁlteroutasegmentationmaskthantoannotateit.)Toensurethattherelationshipholds,weonly
                                            considerpixelsaroundedgesinthebinarymap.
                                           Weﬁrstdeﬁnethemetricsfordepthmapsandlaterderivetheformulationforbinarysegmentation
                                            masks.Motivatedbytherankingloss(Chenetal.,2016),weusethepairwisedepthratioof
                                            neighboringpixelstodeﬁneaforeground/backgroundrelationship.Leti,jbethelocationsof
                                            twoneighboringpixels.Wethendeﬁneanoccludingcontourc derivedfromadepthmapd as
                                                       !                  "                                      d
                                                        d(j)           t
                                            cd(i,j)=>(1+) ,where[·]istheIversonbracket.Intuitively,thisindicatesthepresence
                                                         d(i)         100
                                            ofanoccludingcontourbetweenpixelsi andj iftheircorrespondingdepthdiffersbymorethant%.
                                            Forallpairsofneighboringpixels,wecanthencomputetheprecision(P)andrecall(R)as
                                                               #                                           #
                                                                          c (i,j) ∧ c (i,j)                           c (i,j) ∧ c (i,j)
                                                                  i,j∈N(i) d          ˆ                      i,j∈N(i) d           ˆ
                                                                    #                 d                         #                 d
                                                       P(t)=                                  andR(t)=                                   .        (3)
                                                                                c (i,j)                                    c (i,j)
                                                                       i,j∈N(i) d                                  i,j∈N(i)  ˆ
                                                                                                                             d
                                            NotethatbothP andRarescale-invariant.Inourexperiments,wereporttheF1score.Toaccount
                                            formultiplerelativedepthratios,wefurtherperformaweightedaveragingoftheF1valueswith
                                            thresholdsthatrangelinearlyfromt            =5tot          =25,withstrongerweightstowardshigh
                                                                                   minmax
                                            thresholdvalues.Comparedtootheredge-basedmetrics(suchastheedgeaccuracyandcompletion
                                            fromiBims(Kochetal.,2018)),ourmetricdoesnotrequireanymanualedgeannotation,butsimply
                                            pixelwisegroundtruth,whichiseasilyobtainedforsyntheticdatasets.
                                                                                               6
                              Preprint
                              Similarly,wecanalsoidentifyoccludingcontoursfrombinarylabelmapsthatcanbederivedfrom
                              real-worldsegmentation,saliency,andmattingdatasets.Givenabinarymaskb overtheimage,we
                              deﬁnethepresenceofanoccludingcontourc betweenpixelsi,jasc (i,j)=b(i)∧¬b(j).With
                                                               b                 b
                              thisdeﬁnitionathand,wecomputetherecallR(t) byreplacingtheoccludingcontoursfromdepth
                              mapsinEq.3withthosefrombinarymaps.Sincethebinarymapscommonlylabelwholeobjects,
                              wecannotobtainground-truthoccludingcontoursthatdonotalignwithobjectsilhouettes.Thus
                              theboundaryannotationisincomplete–somebutnotalloccludingcontoursareidentiﬁedbythis
                              procedure.Thereforewecanonlycomputetherecallbutnottheprecisionforbinarymaps.
                              Topenalizeblurryedges,wesuppressnon-maximumvaluesofc withinthevalidboundsofc (i,j)
                                                                             ˆ                    ˆ
                                                                            d                     d
                              connectedcomponents.Foradditionalexperimentsandqualitativeresultswerefertotheappendices.
                              3.3FOCALLENGTHESTIMATION
                              TohandleimagesthatmayhaveinaccurateormissingEXIFmetadata,wesupplementournetwork
                              withafocallengthestimationhead.Asmallconvolutionalheadingestsfrozenfeaturesfromthe
                              depthestimationnetworkandtask-speciﬁcfeaturesfromaseparateViTimageencodertopredictthe
                              horizontalangularﬁeld-of-view.WeuseL asthetrainingloss.Wetrainthefocallengthheadand
                                                             2
                              theViTencoderafterthedepthestimationtraining.Separatingthefocallengthtraininghasseveral
                              beneﬁtsoverjointtrainingwiththedepthnetwork.Itavoidsthenecessityofbalancingthedepth
                              andfocallengthtrainingobjectives.Italsoallowstrainingthefocallengthheadonadifferentsetof
                              datasets,excludingsomenarrow-domainsingle-cameradatasetsthatareusedintrainingthedepth
                              estimationnetwork,andaddinglarge-scaleimagedatasetsthatprovidefocallengthsupervisionbut
                              nodepthsupervision.Furtherdetailsareprovidedintheappendices.
                              4EXPERIMENTS
                              Thissectionsummarizesthekeyresults.Additionaldetailsandexperimentsarereportedintheap-
                              pendices,includingdetailsondatasets,hyperparameters,experimentalprotocols,andthecomparison
                              ofruntimes,whichissummarizedinFig.2.Theappendicesalsoreportcontrolledexperiments,
                              includingcontrolledstudiesonnetworkarchitectures,trainingobjectives,andtrainingcurricula.
                              HerewesummarizeanumberofkeycomparisonsofDepthProtostate-of-the-artmetricmonocular
                              depthestimationsystems.Onechallengeinconductingsuchacomparisonisthatmanyleading
                              recentsystemsaretrainedonbespokecombinationsofdatasets.Somesystemsuseproprietary
                              datasetsthatarenotpubliclyavailable,andsomeusedatasetsthatareonlyavailableunderrestrictive
                              licenses.Somerecentsystemsalsotrainonunlabeleddatasetsorincorporatepretrainedmodels(e.g.,
                              diffusionmodels)thatweretrainedonadditionalmassivedatasets.Thisrulesoutthepossibilityofa
                              comparisonthatcontrolsfortrainingdata(e.g.,onlycomparingtosystemsthatusethesamedatasets
                              wedo).Atthisstageofthisresearcharea,theonlyfeasiblecomparisontootherleadingcross-domain
                              monoculardepthestimationmodelsisonafullsystem-to-systembasis.Fullytrainedmodels(each
                              trainedonalarge,partiallyoverlappingandpartiallydistinctcollectionofdatasets)arecomparedto
                              eachotherzero-shotondatasetsthatnoneofthecomparedsystemstrainedon.
                              Zero-shotmetricdepth.Weevaluateourmethod’sabilitytopredictzero-shotmetricdepthand
                              compareagainstthestateoftheartinTab.1.OurbaselinesincludeDepthAnything(Yangetal.,
                              2024a),Metric3D(Yinetal.,2023),PatchFusion(Lietal.,2024a),UniDepth(Piccinellietal.,2024),
                              ZeroDepth(Guizilinietal.,2023)andZoeDepth(Bhatetal.,2023).Wealsoreportresultsforthe
                              veryrecentDepthAnythingv2(Yangetal.,2024b)andMetric3Dv2(Huetal.,2024).
                              Asanoverallsummarymeasureofmetricdepthaccuracy,Tab.1usestheδ metric(Ladickyetal.,
                                                                                     1
                              2014),whichiscommonlyusedforthispurpose(Yinetal.,2023;Yangetal.,2024a;Piccinellietal.,
                              2024).Itisdeﬁnedasthepercentageofinlierpixels,forwhichthepredictedandground-truthdepths
                              arewithin25%ofeachother.Wepickedthismetricforitsrobustness,withthestrictestthreshold
                              foundintheliterature(25%).
                              CorrespondingtablesforadditionalmetricscanbefoundinSec.A.2oftheappendices,including
                              AbsRel(Ladickyetal.,2014),Log, δ andδ scores,aswellaspoint-cloudmetrics(Spencer
                                                            2    3
                                                        10
                              etal.,2022).Tab.1alsoreportstheaveragerankofeachmethodacrossdatasets,acommonwayto
                              summarizecross-datasetperformance(Ranftletal.,2022).
                                                                  7
                         Preprint
                         WereportresultsonBooster(Ramirezetal.,2024),Middlebury(Scharsteinetal.,2014),Sun-
                                                    ¨
                         RGBD(Songetal.,2015),ETH3D(Schopsetal.,2017),nuScenes(Caesaretal.,2020),and
                         Sintel(Butleretal.,2012),because,toourknowledge,theywereneverusedintraininganyof
                         theevaluatedsystems.Despiteourbestefforts,wewerenotabletorunZeroDepthonBooster,
                         Middlebury,orSun-RGBDasitconsistentlyranoutofmemoryduetothehighimageresolutions.
                         MoredetailsonourevaluationsetupcanbefoundinSec.Coftheappendix.
                         TheresultsinTab.1conﬁrmtheﬁndingsofPiccinellietal.(2024),whoobservedconsiderable
                         domainbiasinsomeoftheleadingmetricdepthestimationmodels.Notably,DepthAnything
                         v1&v2focusonrelativedepthestimation;formetricdepth,theyprovidedifferentmodelsfor
                         differentdomains,ﬁne-tunedeitherforindoororforoutdoorscenes.Metric3Dv1&v2provide
                         domain-invariantmodels,buttheirperformancedependsstronglyoncarefulselectionofthecrop
                         sizeattesttime,whichisperformedperdomainintheirexperimentsandthusviolatesthezero-shot
                         premise.Wetriedsettingthecropsizeautomaticallybasedontheaspectratiooftheimage,but
                         thissubstantiallydegradedtheperformanceofMetric3D;forthisreason,weusetherecommended
                         non-zero-shotprotocol,withtherecommendedper-domaincropsizes.Sincedomain-speciﬁcmodels
                         andcropsizesviolatethestrictzero-shotpremisewe(andotherbaselines)operateunder,wemark
                         theDepthAnythingandMetric3DresultsingrayinTab.1.
                         WeﬁndthatDepthProdemonstratesthestrongestgeneralizationbyconsistentlyscoringamongthe
                         topapproachesperdatasetandobtainingthebestaveragerankacrossalldatasets.
                         Table1:Zero-shotmetricdepthaccuracy.Wereporttheδ scoreperdataset(higherisbetter)and
                                                              1
                         aggregateperformanceacrossdatasetsviatheaveragerank(lowerisbetter).Methodsingrayarenot
                         strictlyzero-shot.Resultsonadditionalmetricsanddatasetsarepresentedintheappendices.
                         MethodBoosterETH3DMiddleburyNuScenesSintelSun-RGBDAvg.Rank↓
                         DepthAnything(Yangetal.,2024a)52.39.339.335.46.985.05.7
                         DepthAnythingv2(Yangetal.,2024b)59.536.337.217.75.972.45.8
                         Metric3D(Yinetal.,2023)4.734.213.664.417.316.95.8
                         Metric3Dv2(Huetal.,2024)39.487.729.982.638.375.63.7
                         PatchFusion(Lietal.,2024a)22.651.849.920.414.053.65.2
                         UniDepth(Piccinellietal.,2024)27.625.331.983.616.595.84.2
                         ZeroDepth(Guizilinietal.,2023)OOMOOM46.564.312.9OOM4.6
                         ZoeDepth(Bhatetal.,2023)21.634.253.828.17.885.75.3
                         DepthPro(Ours)46.641.560.549.140.089.02.5
                         Zero-shotboundaries.Tab.2summarizestheevaluationofboundaryaccuracyforDepthProanda
                         numberofbaselines.Thisevaluationisconductedinazero-shotsetting:modelsareonlyevaluated
                         ondatasetsthatwerenotseenduringtraining.Sinceourboundarymetricsarescale-invariant,our
                         baselinesherealsoincludemethodsthatonlypredictrelative(ratherthanabsolutemetric)depth.Our
                         absolutebaselinesincludeMetric3D(Yinetal.,2023),Metric3Dv2(‘giant’model)(Huetal.,2024),
                         PatchFusion(Lietal.,2024a),UniDepth(Piccinellietal.,2024),andZoeDepth(Bhatetal.,2023).
                         WealsoreportresultsfortherelativevariantsofDepthAnythingv1&v2(Yangetal.,2024a;b)
                         becausetheyyieldsharperboundariesthantheirmetriccounterparts.Lastly,weincludeMarigold(Ke
                         etal.,2024),arecentdiffusion-basedrelativedepthmodelthatbecamepopularduetoitshigh-ﬁdelity
                         predictions.WeusetheboundarymetricsintroducedinSec.3.2,andreporttheaverageboundary
                         F1scorefordatasetswithground-truthdepth,andboundaryrecall(R)fordatasetswithmattingor
                         segmentationannotations.Forimagemattingdatasets,apixelismarkedasoccludingwhenthevalue
                         ofthealphamatteisabove0.1.
                         ThedatasetsincludeSintel(Butleretal.,2012)andSpring(Mehletal.,2023),whicharesynthetic.
                         WealsoincludetheiBimsdataset(Kochetal.,2018)whichisoftenusedspeciﬁcallytoevaluatedepth
                         boundaries,despitehavinglowresolution.WerefertotheappendicesforafullslateofiBims-speciﬁc
                         metrics.Toevaluatehigh-frequencystructuresencounteredinnaturalimages(suchashairorfur),
                         weuseAM-2k(Lietal.,2022a)andP3M-10k(Lietal.,2021),whicharehigh-resolutionimage
                         mattingdatasetsthatwereusedtoevaluateimagemattingmodels(Lietal.,2023).Additionally,we
                         furtherreportresultsontheDIS-5k(Qinetal.,2022)imagesegmentationdataset.Thisisanobject
                         segmentationdatasetthatprovideshighlyaccuratebinarymasksacrossdiverseimages.Wemanually
                         removesamplesinwhichthesegmentedobjectisoccludedbyforegroundobjects.Fig.2visually
                         summarizestheboundaryrecallmetricontheAM-2kdataset,asafunctionofruntime.
                                                       8
                                 Preprint
                                 Table2:Zero-shotboundaryaccuracy.WereporttheF1scorefordatasetwithground-truthdepth,
                                 andboundaryrecall(R)formattingandsegmentationdatasets.Qualitativeresultsareshownona
                                 samplefromtheAM-2kdataset(Lietal.,2022a).Higherisbetterforallmetrics.
                                        MethodSintelF1↑ SpringF1↑ iBimsF1↑ AMR↑ P3MR↑ DISR↑
                                        DPT(Ranftletal.,2021)0.1810.0290.1130.0550.0750.018
                                        Metric3D(Yinetal.,2023)0.0370.0000.0550.0030.0030.001
                                        Metric3Dv2(Huetal.,2024)0.3210.0240.0960.0240.0130.006
                                        ZoeDepth(Bhatetal.,2023)0.0270.0010.0350.0080.0040.002
                                      AbsolutePatchFusion(Lietal.,2024a)0.3120.0320.1340.0610.1090.068
                                        UniDepth(Piccinellietal.,2024)0.3160.0000.0390.0010.0030.000
                                        DepthAnything(Yangetal.,2024a)0.2610.0450.1270.0580.0940.023
                                      Rel.DepthAnythingv2(Yangetal.,2024b)0.2280.0560.1110.1070.1310.056
                                        Marigold(Keetal.,2024)0.0680.0320.1490.0640.1010.049
                                        DepthPro(Ours)0.4090.0790.1760.1730.1680.077
                                      ImageAlphaMatteDepthPro(Ours)DepthAnythingv2PatchFusionMarigold
                                 WeﬁndthatDepthProproducesmoreaccurateboundariesthanallbaselinesonalldatasets,bya
                                 signiﬁcantmargin.AscanbeobservedinFig.1,intheimagesinTab.2,andtheadditionalresultsin
                                 Sec.A,thecompetitivemetricaccuracyoftherecentMetric3Dv2andDepthAnythingv2models
                                 doesnotimplysharpboundaries.DepthProhasaconsistentlyhigherrecallforthinstructureslike
                                 hairandfurandyieldssharperboundaries.Thisisalsotrueincomparisontothediffusion-based
                                 Marigold,whichleveragesapriortrainedonbillionsofreal-wordimages,aswellasPatchFusion,
                                 whichoperatesatvariableresolution.NotethattheruntimeofDepthProisordersofmagnitude
                                 fasterthanMarigoldandPatchFusion(seeFig.2&Tab.5).Fig.4demonstratesthebeneﬁtsofsharp
                                 boundarypredictionfornovelviewsynthesisfromasingleimage.
                                 Focallengthestimation.Previouswork(Piccinellietal.,2024;Kocabasetal.,2021;Baradad&
                                 Torralba,2020)doesnotprovidecomprehensivesystematicevaluationsoffocallengthestimators
                                 onin-the-wildimages.Toaddressthis,wecuratedazero-shottestdataset.Tothisend,weselected
                                 diversedatasetswithintactEXIFdata,enablingreliableassessmentoffocallengthestimation
                                 accuracy.FiveK(Bychkovskyetal.,2011),DDDP(Abuolaim&Brown,2020),andRAISE(Dang-
                                 Nguyenetal.,2015)contributeprofessional-gradephotographstakenwithSLRcameras.SPAQ(Fang
                                 etal.,2020)providescasualphotographsfrommobilephones.PPR10K(Liangetal.,2021)provides
                                 high-qualityportraitimages.Finally,ZOOM(Zhangetal.,2019)includessetsofscenescapturedat
                                 variousopticalzoomlevels.
                                 Table3:Comparisononfocallengthestimation.Wereportδ   andδ    foreachdataset,i.e.,the
                                                                                   25%50%
                                 percentageofimageswithrelativeerror(focallengthinmm)lessthan25%and50%,respectively.
                                                            DDDPFiveKPPR10KRAISESPAQZOOM
                                                         δ   δ    δ    δ    δ   δ    δ   δ    δ    δ    δ   δ
                                                         25%50%25%50%25%50%25%50%25%50%25%50%
                                 UniDepth(Piccinellietal.,2024)6.840.324.856.213.844.235.474.844.277.420.445.4
                                 SPEC(Kocabasetal.,2021)14.646.330.256.634.667.049.278.650.082.223.243.6
                                 im2pcl(Baradad&Torralba,2020)7.329.628.060.024.261.451.875.226.655.022.442.8
                                 DepthPro(Ours)66.985.874.292.464.688.884.296.468.485.269.891.6
                                 Tab.3comparesDepthProagainststate-of-the-artfocallengthestimatorsandshowsthepercentage
                                 ofimageswithrelativeestimationerrorunder25%and50%,respectively.DepthProisthemost
                                 accurateacrossalldatasets.Forexample,onPPR10K,adatasetofhumanportraits,ourmethodleads
                                 with64.6%oftheimageshavingafocallengtherrorbelow25%,whilethesecond-bestmethod,
                                 SPEC,onlyachieves34.6%onthismetric.Weattributethissuperiorperformancetoournetwork
                                 designandtrainingprotocol,whichdecoupletrainingofthefocallengthestimatorfromthedepth
                                                                        9
                   Preprint
                      Input
                      DepthPro
                      DepthAnythingv2
                      Marigold
                      Metric3Dv2
                   Figure4:Impactonnovelviewsynthesis.WeplugdepthmapsproducedbyDepthPro,Marigold(Ke
                   etal.,2024),DepthAnythingv2(Yangetal.,2024b),andMetric3Dv2(Huetal.,2024)intoarecent
                   publiclyavailablenovelviewsynthesissystem(Khanetal.,2023).Wedemonstrateresultsonimages
                   fromAM-2k(Lietal.,2022a)(1st&3rdcolumn)andDIS-5k(Qinetal.,2022)(2ndcolumn).Depth
                   Proproducessharperandmoreaccuratedepthmaps,yieldingcleanersynthesizedviews.Depth
                   Anythingv2andMetric3Dv2sufferfrommisalignmentbetweentheinputimagesandestimated
                   depthmaps,resultinginforegroundpixelsbleedingintothebackground.Marigoldisconsiderably
                   slowerthanDepthProandproduceslessaccurateboundaries,yieldingartifactsinsynthesizedimages.
                   Zoominfordetail.
                   network,enablingustousedifferenttrainingsetsforthesetwotasks.Furthercontrolledexperiments
                   arereportedintheappendices.
                   5CONCLUSION& LIMITATIONS
                   DepthProproduceshigh-resolutionmetricdepthmapswithhigh-frequencydetailatsub-second
                   runtimes.Ourmodelachievesstate-of-the-artzero-shotmetricdepthestimationaccuracywithout
                   requiringmetadatasuchascameraintrinsics,andtracesoutocclusionboundariesinunprecedented
                   detail,facilitatingapplicationssuchasnovelviewsynthesisfromsingleimages‘inthewild’.While
                   DepthProoutperformspriorworkalongmultipledimensions,itisnotwithoutlimitations.For
                   example,themodelislimitedindealingwithtranslucentsurfacesandvolumetricscattering,where
                   thedeﬁnitionofsinglepixeldepthisill-posedandambiguous.
                                        10
                      Preprint
                      REFERENCES
                      AbdullahAbuolaimandMichaelSBrown.Defocusdeblurringusingdual-pixeldata.InECCV,
                       2020.
                      ManelBaradadandAntonioTorralba.Heightanduprightnessinvariancefor3Dpredictionfroma
                       singleview.InCVPR,2020.
                      ZuriaBauer,FranciscoGomez-Donoso,EdmanuelCruz,SergioOrts-Escolano,andMiguelCazorla.
                       UASOL,alarge-scalehigh-resolutionoutdoorstereodataset.ScientiﬁcData,6,2019.
                      ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka.AdaBins:Depthestimationusing
                       adaptivebins.InCVPR,2021.
                      ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka.LocalBins:Improvingdepthestimation
                       bylearninglocaldistributions.InECCV,2022.
                                                                 ¨
                      ShariqFarooqBhat,ReinerBirkl,DianaWofk,PeterWonka,andMatthiasMuller.ZoeDepth:
                       Zero-shottransferbycombiningrelativeandmetricdepth.arXiv,2023.
                                             ¨
                      ReinerBirkl,DianaWofk,andMatthiasMuller.MiDaSv3.1-Amodelzooforrobustmonocular
                       relativedepthestimation.arXiv,2023.
                      MichaelJ.Black,PriyankaPatel,JoachimTesch,andJinlongYang.BEDLAM:Asyntheticdataset
                       ofbodiesexhibitingdetailedlifelikeanimatedmotion.InCVPR,2023.
                      DanielJ.Butler,JonasWulff,GarrettB.Stanley,andMichaelJ.Black.Anaturalisticopensource
                       movieforopticalﬂowevaluation.InECCV,2012.
                                                   ´
                      VladimirBychkovsky,SylvainParis,EricChan,andFredoDurand.Learningphotographicglobal
                       tonaladjustmentwithadatabaseofinput/outputimagepairs.InCVPR,2011.
                      HolgerCaesar,VarunBankiti,AlexH.Lang,SourabhVora,VeniceErinLiong,QiangXu,Anush
                       Krishnan,YuPan,GiancarloBaldan,andOscarBeijbom.nuScenes:Amultimodaldatasetfor
                       autonomousdriving.InCVPR,2020.
                      HanCai,JunyanLi,MuyanHu,ChuangGan,andSongHan.EfﬁcientViT:Lightweightmulti-scale
                       attentionforhigh-resolutiondenseprediction.InICCV,2023.
                      Chun-Fu(Richard)Chen,QuanfuFan,andRameswarPanda.CrossViT:Cross-attentionmulti-scale
                       visiontransformerforimageclassiﬁcation.InICCV,2021.
                      WeifengChen,ZhaoFu,DaweiYang,andJiaDeng.Single-imagedepthperceptioninthewild.In
                       NIPS,2016.
                      ZheChen,YuchenDuan,WenhaiWang,JunjunHe,TongLu,JifengDai,andYuQiao.Vision
                       transformeradapterfordensepredictions.InICLR,2023.
                      XiangxiangChu,ZhiTian,YuqingWang,BoZhang,HaibingRen,XiaolinWei,HuaxiaXia,and
                       ChunhuaShen.Twins:Revisitingthedesignofspatialattentioninvisiontransformers.InNeurIPS,
                       2021.
                      AngelaDai,AngelX.Chang,ManolisSavva,MaciejHalber,ThomasA.Funkhouser,andMatthias
                       Nießner.ScanNet:Richly-annotated3Dreconstructionsofindoorscenes.InCVPR,2017.
                      Duc-TienDang-Nguyen,CeciliaPasquini,ValentinaConotter,andGiuliaBoato.RAISE:Araw
                       imagesdatasetfordigitalimageforensics.InMMSys,2015.
                      AfshinDehghan,GiladBaruch,ZhuoyuanChen,YuriFeigin,PeterFu,ThomasGebauer,Daniel
                       Kurz,TalDimry,BrandonJoffe,ArikSchwartz,andEladShulman.ARKitScenes:Adiverse
                       real-worlddatasetfor3DindoorsceneunderstandingusingmobileRGB-Ddata.InNeurIPS
                       Datasets&Benchmarks,2021.
                      JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.ImageNet:Alarge-scale
                       hierarchicalimagedatabase.InCVPR,2009.
                                               11
                     Preprint
                     NeilA.Dodgson.Variationandextremaofhumaninterpupillarydistance.InStereoscopicDisplays
                      andVirtualRealitySystemsXI,volume5291,pp.36–46,2004.
                     AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
                      Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,
                      andNeilHoulsby.Animageisworth16x16words:Transformersforimagerecognitionatscale.
                      InICLR,2021.
                     DavidEigenandRobFergus.Predictingdepth,surfacenormalsandsemanticlabelswithacommon
                      multi-scaleconvolutionalarchitecture.InICCV,2015.
                     DavidEigen,ChristianPuhrsch,andRobFergus.Depthmappredictionfromasingleimageusinga
                      multi-scaledeepnetwork.InNIPS,2014.
                     FacebookResearch.fvcore.https://github.com/facebookresearch/fvcore,2022.
                       ´  ´
                     JoseM.Facil,BenjaminUmmenhofer,HuizhongZhou,LuisMontesano,ThomasBrox,andJavier
                      Civera.CAM-Convs:Camera-awaremulti-scaleconvolutionsforsingle-viewdepth.InCVPR,
                      2019.
                     HaoqiFan,BoXiong,KarttikeyaMangalam,YanghaoLi,ZhichengYan,JitendraMalik,and
                      ChristophFeichtenhofer.Multiscalevisiontransformers.InICCV,2021.
                     YumingFang,HanweiZhu,YanZeng,KedeMa,andZhouWang.Perceptualqualityassessmentof
                      smartphonephotography.InCVPR,2020.
                     HuanFu,MingmingGong,ChaohuiWang,KayhanBatmanghelich,andDachengTao.Deepordinal
                      regressionnetworkformonoculardepthestimation.InCVPR,2018.
                     AdrienGaidon,QiaoWang,YohannCabon,andEleonoraVig.Virtualworldsasproxyformulti-
                      objecttrackinganalysis.InCVPR,2016.
                     AndreasGeiger,PhilipLenz,ChristophStiller,andRaquelUrtasun.Visionmeetsrobotics:The
                      KITTIdataset.IJRR,32(11),2013.
                           ´                     `  `            ´    ´
                     JoseLuisGomez,ManuelSilva,AntonioSeoane,AgnesBorras,MarioNoriega,GermanRos,Jose
                                  ´          ´
                      AntonioIglesiasGuitian,andAntonioM.Lopez.Allforone,andoneforall:UrbanSyndataset,
                      thethirdmusketeerofsyntheticdrivingscenes.arXiv,2023.
                     MingGui,JohannesS.Fischer,UlrichPrestel,PingchuanMa,DmytroKotovenko,OlgaGrebenkova,
                                                ¨
                      StefanAndreasBaumann,VincentTaoHu,andBjornOmmer.DepthFM:Fastmonoculardepth
                      estimationwithﬂowmatching.arXiv,2024.
                     VitorGuizilini,RaresAmbrus,SudeepPillai,AllanRaventos,andAdrienGaidon.3Dpackingfor
                      self-supervisedmonoculardepthestimation.InCVPR,2020.
                     VitorGuizilini,IgorVasiljevic,DianChen,RaresAmbrus,andAdrienGaidon.Towardszero-shot
                      scale-awaremonoculardepthestimation.InICCV,2023.
                                                      ´
                     KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollar,andRossGirshick.Masked
                      autoencodersarescalablevisionlearners.InCVPR,2022.
                     PeterHedman,SuhibAlsisan,RichardSzeliski,andJohannesKopf.Casual3Dphotography.ACM
                      Trans.Graph.,36(6),2017.
                     MuHu,WeiYin,ChiZhang,ZhipengCai,XiaoxiaoLong,HaoChen,KaixuanWang,GangYu,
                      ChunhuaShen,andShaojieShen.Metric3Dv2:Aversatilemonoculargeometricfoundation
                      modelforzero-shotmetricdepthandsurfacenormalestimation.arXiv,2024.
                     Yuan-TingHu,JiahongWang,RaymondA.Yeh,andAlexanderG.Schwing.SAIL-VOS3D:A
                      syntheticdatasetandbaselinesforobjectdetectionand3Dmeshreconstructionfromvideodata.
                      InCVPR,2021.
                     Po-HanHuang,KevinMatzen,JohannesKopf,NarendraAhuja,andJia-BinHuang.DeepMVS:
                      Learningmulti-viewstereopsis.InCVPR,2018.
                                             12
                      Preprint
                      XinyuHuang,PengWang,XinjingCheng,DingfuZhou,QichuanGeng,andRuigangYang.The
                        ApolloScapeopendatasetforautonomousdrivinganditsapplication.TPAMI,42(10):2702–2719,
                        2020.
                      AndrewJaegle,SebastianBorgeaud,Jean-BaptisteAlayrac,CarlDoersch,CatalinIonescu,David
                                                                         ´
                        Ding,SkandaKoppula,DanielZoran,AndrewBrock,EvanShelhamer,OlivierJ.Henaff,
                                                             ˜
                        MatthewM.Botvinick,AndrewZisserman,OriolVinyals,andJoaoCarreira.PerceiverIO:
                        Ageneralarchitectureforstructuredinputs&outputs.InICLR,2022.
                      VarunJampani,HuiwenChang,KyleSargent,AbhishekKar,RichardTucker,MichaelKrainin,
                        DominikKaeser,WilliamT.Freeman,DavidSalesin,BrianCurless,andCeLiu.SLIDE:Single
                        image3Dphotographywithsoftlayeringanddepth-awareinpainting.InICCV,2021.
                      NikitaKaraev,IgnacioRocco,BenjaminGraham,NataliaNeverova,AndreaVedaldi,andChristian
                        Rupprecht.DynamicStereo:Consistentdynamicdepthfromstereovideos.InCVPR,2023.
                      BingxinKe,AntonObukhov,ShengyuHuang,NandoMetzger,RodrigoCayeDaudt,andKonrad
                        Schindler.Repurposingdiffusion-basedimagegeneratorsformonoculardepthestimation.In
                        CVPR,2024.
                      NumairKhan,LeiXiao,andDouglasLanman.Tiledmultiplaneimagesforpractical3Dphotography.
                        InICCV,2023.
                      YoungjungKim,BumsubHam,ChangjaeOh,andKwanghoonSohn.Structureselectivedepth
                        superresolutionforRGB-Dcameras.TIP,25(11):5527–38,2016.
                      AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
                        Xiao,SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDollar,andRossGirshick.
                        Segmentanything.InICCV,2023.
                                                         ¨
                      MuhammedKocabas,Chun-HaoP.Huang,JoachimTesch,LeaMuller,OtmarHilliges,andMichaelJ.
                        Black.SPEC:Seeingpeopleinthewildwithanestimatedcamera.InICCV,2021.
                                                          ¨
                      TobiasKoch,LukasLiebel,FriedrichFraundorfer,andMarcoKorner.EvaluationofCNN-based
                        single-imagedepthestimationmethods.InECCVWorkshops,2018.
                      AnastasiiaKornilova,MarselFaizullin,KonstantinPakulev,AndreySadkov,DenisKukushkin,Azat
                        Akhmetyanov,TimurAkhtyamov,HekmatTaherinejad,andGonzaloFerrer.SmartPortraits:Depth
                        poweredhandheldsmartphonedatasetofhumanportraitsforstateestimation,reconstructionand
                        synthesis.InCVPR,2022.
                      LuborLadicky,JianboShi,andMarcPollefeys.Pullingthingsoutofperspective.InCVPR,2014.
                      Hoang-AnLe,ThomasMensink,ParthaDas,SezerKaraoglu,andTheoGevers.EDEN:Multimodal
                        syntheticdatasetofenclosedgardenscenes.InWACV,2021.
                      YoungwanLee,JongheeKim,JeffreyWillette,andSungJuHwang.MPViT:Multi-pathvision
                        transformerfordenseprediction.InCVPR,2022.
                      JizhiziLi,SihanMa,JingZhang,andDachengTao.Privacy-preservingportraitmatting.InACM-MM,
                        2021.
                      JizhiziLi,JingZhang,StephenJMaybank,andDachengTao.Bridgingcompositeandreal:Towards
                        end-to-enddeepimagematting.IJCV,130(2):246–266,2022a.
                      JizhiziLi,JingZhang,andDachengTao.Deepimagematting:Acomprehensivesurvey.arXiv,
                        2023.
                      YanghaoLi,HanziMao,RossGirshick,andKaimingHe.Exploringplainvisiontransformer
                        backbonesforobjectdetection.InECCV,2022b.
                      YanghaoLi,Chao-YuanWu,HaoqiFan,KarttikeyaMangalam,BoXiong,JitendraMalik,and
                        ChristophFeichtenhofer.MViTv2:Improvedmultiscalevisiontransformersforclassiﬁcationand
                        detection.InCVPR,2022c.
                                                13
                      Preprint
                      ZhengqiLiandNoahSnavely.MegaDepth:Learningsingle-viewdepthpredictionfrominternet
                       photos.InCVPR,2018.
                      ZhenyuLi,ShariqFarooqBhat,andPeterWonka.PatchFusion:Anend-to-endtile-basedframework
                       forhigh-resolutionmonocularmetricdepthestimation.InCVPR,2024a.
                      ZhenyuLi,XuyangWang,XianmingLiu,andJunjunJiang.BinsFormer:Revisitingadaptivebins
                       formonoculardepthestimation.TIP,33:3964–3976,2024b.
                      JieLiang,HuiZeng,MiaomiaoCui,XuansongXie,andLeiZhang.PPR10K:Alarge-scaleportrait
                       photoretouchingdatasetwithhuman-regionmaskandgroup-levelconsistency.InCVPR,2021.
                      XinyuLiu,HouwenPeng,NingxinZheng,YuqingYang,HanHu,andYixuanYuan.EfﬁcientViT:
                       Memoryefﬁcientvisiontransformerwithcascadedgroupattention.InCVPR,2023.
                      ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
                       Swintransformer:Hierarchicalvisiontransformerusingshiftedwindows.InICCV,2021.
                      ZeLiu,HanHu,YutongLin,ZhuliangYao,ZhendaXie,YixuanWei,JiaNing,YueCao,Zheng
                       Zhang,LiDong,FuruWei,andBainingGuo.SwintransformerV2:Scalingupcapacityand
                       resolution.InCVPR,2022a.
                      ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie.
                       AConvNetforthe2020s.InCVPR,2022b.
                                                                ´
                      LukasMehl,JennySchmalfuss,AzinJahedi,YaroslavaNalivayko,andAndresBruhn.Spring:A
                       high-resolutionhigh-detaildatasetandbenchmarkforsceneﬂow,opticalﬂowandstereo.InCVPR,
                       2023.
                      NandoMetzger,RodrigoCayeDaudt,andKonradSchindler.Guideddepthsuper-resolutionbydeep
                       anisotropicdiffusion.InCVPR,2023.
                      S.MahdiH.Miangoleh,SebastianDille,LongMai,SylvainParis,andYagizAksoy.Boostingmonoc-
                       ulardepthestimationmodelstohigh-resolutionviacontent-adaptivemulti-resolutionmerging.In
                       CVPR,2021.
                      SimonNiklaus,LongMai,JimeiYang,andFengLiu.3DKenBurnseffectfromasingleimage.In
                       SIGGRAPH,2019.
                                   ´       ´
                      MaximeOquab,TimotheeDarcet,TheoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,
                       PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,MahmoudAssran,Nico-
                       lasBallas,WojciechGaluba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,Michael
                                                    ´
                       Rabbat,VasuSharma,GabrielSynnaeve,HuXu,HerveJegou,JulienMairal,PatrickLabatut,Ar-
                       mandJoulin,andPiotrBojanowski.DINOv2:Learningrobustvisualfeatureswithoutsupervision.
                       TMLR,2024.
                      JuewenPeng,ZhiguoCao,XianruiLuo,HaoLu,KeXian,andJianmingZhang.BokehMe:When
                       neuralrenderingmeetsclassicalrendering.InCVPR,2022a.
                      ZhiliangPeng,LiDong,HangboBao,QixiangYe,andFuruWei.BEiTv2:Maskedimagemodeling
                       withvector-quantizedvisualtokenizers.arXiv,2022b.
                      AndraPetrovaiandSergiuNedevschi.Exploitingpseudolabelsinaself-supervisedlearningframe-
                       workforimprovedmonoculardepthestimation.InCVPR,2022.
                                                         `
                      LuigiPiccinelli,Yung-HsuYang,ChristosSakaridis,MattiaSegu,SiyuanLi,LucVanGool,and
                       FisherYu.UniDepth:Universalmonocularmetricdepthestimation.InCVPR,2024.
                      XuebinQin,HangDai,XiaobinHu,Deng-PingFan,LingShao,andLucVanGool.Highlyaccurate
                       dichotomousimagesegmentation.InECCV,2022.
                      AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
                       GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever.
                       Learningtransferablevisualmodelsfromnaturallanguagesupervision.PMLR,2021.
                                                14
                     Preprint
                         ¨
                     MichaelRamamonjisoaandVincentLepetit.SharpNet:Fastandaccuraterecoveryofoccluding
                       contoursinmonoculardepthestimation.InICCVWorkshop,2019.
                         ¨
                     MichaelRamamonjisoa,YumingDu,andVincentLepetit.Predictingsharpandaccurateocclusion
                       boundariesinmonoculardepthestimationusingdisplacementﬁelds.InCVPR,2020.
                     PierluigiZamaRamirez,AlexCostanzino,FabioTosi,MatteoPoggi,SamueleSalti,StefanoMattoc-
                       cia,andLuigiDiStefano.Booster:Abenchmarkfordepthfromimagesofspecularandtransparent
                       surfaces.TPAMI,46(1):85–102,2024.
                        ´
                     ReneRanftl,AlexeyBochkovskiy,andVladlenKoltun.Visiontransformersfordenseprediction.In
                       ICCV,2021.
                        ´
                     ReneRanftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun.Towardsrobust
                       monoculardepthestimation:Mixingdatasetsforzero-shotcross-datasettransfer.TPAMI,44(3),
                       2022.
                                                            ´
                     MikeRoberts,JasonRamapuram,AnuragRanjan,AtulitKumar,MiguelAngelBautista,Nathan
                       Paczan,RussWebb,andJoshuaM.Susskind.Hypersim:Aphotorealisticsyntheticdatasetfor
                       holisticindoorsceneunderstanding.InICCV,2021.
                     AshutoshSaxena,MinSun,andAndrewY.Ng.Make3D:Learning3Dscenestructurefromasingle
                       stillimage.TPAMI,31(5),2009.
                     SaurabhSaxena,JunhwaHur,CharlesHerrmann,DeqingSun,andDavidJ.Fleet.Zero-shotmetric
                       depthwithaﬁeld-of-viewconditioneddiffusionmodel.arXiv,2023.
                                  ¨         ¨
                     HannoScharr,StefanKorkel,andBerndJahne.Numerischeisotropieoptimierungvonﬁr-ﬁltern
                              ¨
                       mittelsquerglattung.InDAGM-Symposium,1997.
                                       ¨
                     DanielScharstein,HeikoHirschmuller,YorkKitajima,GregKrathwohl,NeraNesic,XiWang,and
                       PorterWestling.High-resolutionstereodatasetswithsubpixel-accurategroundtruth.InGCPR,
                       2014.
                            ¨           ¨
                     ThomasSchops,JohannesL.Schonberger,S.Galliani,TorstenSattler,KonradSchindler,Marc
                       Pollefeys,andAndreasGeiger.Amulti-viewstereobenchmarkwithhigh-resolutionimagesand
                       multi-cameravideos.InCVPR,2017.
                     Meng-LiShih,Shih-YangSu,JohannesKopf,andJia-BinHuang.3Dphotographyusingcontext-
                       awarelayereddepthinpainting.InCVPR,2020.
                     NathanSilberman,DerekHoiem,PushmeetKohli,andRobFergus.Indoorsegmentationandsupport
                       inferencefromRGBDimages.InECCV,2012.
                     ShuranSong,SamuelP.Lichtenberg,andJianxiongXiao.SUNRGB-D:ARGB-Dsceneunder-
                       standingbenchmarksuite.InCVPR,2015.
                     JaimeSpencer,ChrisRussell,SimonHadﬁeld,andRichardBowden.Deconstructingself-supervised
                       monocularreconstruction:Thedesigndecisionsthatmatter.TMLR,2022.
                     JaimeSpencer,SimonHadﬁeld,ChrisRussell,andRichardBowden.Kickback&relax:Learningto
                       reconstructtheworldbywatchingSlowTV.InICCV,2023.
                     JaimeSpencer,ChrisRussell,SimonHadﬁeld,andRichardBowden.Kickback&relax++:Scaling
                       beyondground-truthdepthwithSlowTV&CribsTV.arXiv,2024.
                     DeqingSun,DanielVlasic,CharlesHerrmann,VarunJampani,MichaelKrainin,HuiwenChang,
                       RaminZabih,WilliamT.Freeman,andCeLiu.AutoFlow:Learningabettertrainingsetfor
                       opticalﬂow.InCVPR,2021.
                     QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao.EVA-CLIP:Improvedtraining
                       techniquesforCLIPatscale.arXiv,2023.
                     MingxingTanandQuocLe.EfﬁcientNetV2:Smallermodelsandfastertraining.InICML,2021.
                                              15
                                                  Preprint
                                                  BartThomee,DavidAShamma,GeraldFriedland,BenjaminElizalde,KarlNi,DouglasPoland,
                                                     DamianBorth,andLi-JiaLi.YFCC100M:Thenewdatainmultimediaresearch.Communications
                                                     oftheACM,59(2):64–73,2016.
                                                  FabioTosi,YiyiLiao,CarolinSchmitt,andAndreasGeiger.SMD-Nets:Stereomixturedensity
                                                     networks.InCVPR,2021.
                                                                                                   ´  ´
                                                  HugoTouvron,MatthieuCord,andHerveJegou.DeiTIII:RevengeoftheViT.InECCV,2022.
                                                  AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
                                                     Kaiser,andIlliaPolosukhin.Attentionisallyouneed.InNIPS,2017.
                                                  QiangWang,ShizhenZheng,QingsongYan,FeiDeng,KaiyongZhao,andXiaowenChu.IRS:A
                                                     largesyntheticindoorroboticsstereodatasetfordisparityandsurfacenormalestimation.arXiv,
                                                     2019.
                                                  WenshanWang,DelongZhu,XiangweiWang,YaoyuHu,YuhengQiu,ChenWang,YafeiHu,Ashish
                                                     Kapoor,andSebastianA.Scherer.TartanAir:AdatasettopushthelimitsofvisualSLAM.In
                                                     IROS,2020.
                                                  RossWightman.Pytorchimagemodels.https://github.com/rwightman/
                                                     pytorch-image-models,2019.
                                                  SanghyunWoo,ShoubhikDebnath,RonghangHu,XinleiChen,ZhuangLiu,InSoKweon,and
                                                     SainingXie.ConvNeXtV2:Co-DesigningandScalingConvNetsWithMaskedAutoencoders.In
                                                     CVPR,2023.
                                                  MagnusWrenningeandJonasUnger.Synscapes:Aphotorealisticsyntheticdatasetforstreetscene
                                                     parsing.arXiv,2018.
                                                  ChunlongXia,XinliangWang,FengLv,XinHao,andYifengShi.ViT-CoMer:Visiontransformer
                                                     withconvolutionalmulti-scalefeatureinteractionfordensepredictions.InCVPR,2024.
                                                  KeXian,ChunhuaShen,ZhiguoCao,HaoLu,YangXiao,RuiboLi,andZhenboLuo.Monocular
                                                     relativedepthperceptionwithwebstereodatasupervision.InCVPR,2018.
                                                  KeXian,JianmingZhang,OliverWang,LongMai,ZheLin,andZhiguoCao.Structure-guided
                                                     rankinglossforsingleimagedepthprediction.InCVPR,2020.
                                                                                                                                ´      ´
                                                  EnzeXie,WenhaiWang,ZhidingYu,AnimaAnandkumar,JoseM.Alvarez,andPingLuo.Seg-
                                                     Former:Simpleandefﬁcientdesignforsemanticsegmentationwithtransformers.InNeurIPS,
                                                     2021.
                                                  LiheYang,BingyiKang,ZilongHuang,XiaogangXu,JiashiFeng,andHengshuangZhao.Depth
                                                     anything:Unleashingthepoweroflarge-scaleunlabeleddata.InCVPR,2024a.
                                                  LiheYang,BingyiKang,ZilongHuang,ZhenZhao,XiaogangXu,JiashiFeng,andHengshuang
                                                     Zhao.Depthanythingv2.arXiv,2024b.
                                                  YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,LeiZhou,TianFang,andLongQuan.
                                                     BlendedMVS:Alarge-scaledatasetforgeneralizedmulti-viewstereonetworks.InCVPR,2020.
                                                  WeiYin,JianmingZhang,OliverWang,SimonNiklaus,LongMai,SimonChen,andChunhuaShen.
                                                     Learningtorecover3Dsceneshapefromasingleimage.InCVPR,2021.
                                                  WeiYin,ChiZhang,HaoChen,ZhipengCai,GangYu,KaixuanWang,XiaozhiChen,andChunhua
                                                     Shen.Metric3D:Towardszero-shotmetric3Dpredictionfromasingleimage.InICCV,2023.
                                                  WeihaoYu,ChenyangSi,PanZhou,MiLuo,YichenZhou,JiashiFeng,ShuichengYan,andXinchao
                                                     Wang.Metaformerbaselinesforvision.TPAMI,46(2),2024.
                                                  YuhuiYuan,RaoFu,LangHuang,WeihongLin,ChaoZhang,XilinChen,andJingdongWang.
                                                     HRFormer:High-resolutiontransformerfordenseprediction.InNeurIPS,2021.
                                                                                                            16
                                   Preprint
                                   XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer.Sigmoidlossforlanguage
                                      imagepre-training.InICCV,2023.
                                   ChiZhang,WeiYin,GangYu,ZhibinWang,TaoChen,BinFu,JoeyTianyiZhou,andChunhua
                                      Shen.Robustgeometry-preservingdepthestimationusingdifferentiablerendering.InICCV,
                                      2023a.
                                   LvminZhang,AnyiRao,andManeeshAgrawala.Addingconditionalcontroltotext-to-image
                                      diffusionmodels.InICCV,2023b.
                                   XuanerZhang,QifengChen,RenNg,andVladlenKoltun.Zoomtolearn,learntozoom.InCVPR,
                                      2019.
                                   ZhiweiZhong,XianmingLiu,JunjunJiang,DebinZhao,andXiangyangJi.Guideddepthmap
                                      super-resolution:Asurvey.ACMComputingSurveys,2023.
                                   XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,andJifengDai.DeformableDETR:
                                      Deformabletransformersforend-to-endobjectdetection.InICLR,2021.
                                    SUPPLEMENTALMATERIAL
                                   InSectionA,weprovideadditionalresultsandexperiments.Sec.A.1presentsfurtherqualitative
                                   comparisonstobaselines,Sec.A.2presentsamoredetailedzero-shotevaluation,Sec.A.3lists
                                   runtimesforallevaluatedmethods,andSec.A.4presentsadditionalexperimentsonboundaryaccuracy.
                                   SectionBshowcasesaselectionofcontrolledexperimentsonDepthProthathelpedguidearchitectural
                                   choices(Sec.B.1,Sec.B.2,andSec.B.5),trainingobjectivedesign(Sec.B.3),andcurriculumtraining
                                   (Sec.B.4).InSectionC,weprovideadditionalimplementation,trainingandevaluationdetails,
                                   includingacompletesummaryofthedatasetsthatwereinvolvedinthispaper.Finally,SectionD
                                   providesadditionalmaterialondownstreamapplications.
                                    AADDITIONALRESULTS
                                    A.1QUALITATIVERESULTS
                                   WeprovideadditionalqualitativeresultsofDepthPro,Marigold(Keetal.,2024),Metric3Dv2(Hu
                                   etal.,2024),andDepthAnythingv2(Yangetal.,2024b)onin-the-wildimagesfromAM-2k(Li
                                                                                  1
                                   etal.,2022a),DIS-5k(Qinetal.,2022),andUnsplashinFig.5,Fig.6,andFig.7.Finedetailsare
                                   repeatedlymissedbyMetric3Dv2andDepthAnythingv2.Marigoldreproducesﬁnerdetailsthan
                                   Metric3Dv2andDepthAnythingv2,butcommonlyyieldsnoisypredictions.
                                    A.2ZERO-SHOTMETRICDEPTH
                                   ExpandingonthesummaryinTab.1,weprovideadditionalresultsforzero-shotmetricdepth
                                   estimationinTab.4.WereportresultsonBooster(Ramirezetal.,2024),Middlebury(Scharstein
                                                                                      ¨
                                   etal.,2014),Sun-RGBD(Songetal.,2015),ETH3D(Schopsetal.,2017),nuScenes(Caesaretal.,
                                   2020),andSintel(Butleretal.,2012).OurbaselinesincludeDepthAnything(Yangetal.,2024a)
                                   andDepthAnythingv2(Yangetal.,2024b),Metric3D(Yinetal.,2023)andMetric3Dv2(Huetal.,
                                   2024),PatchFusion(Lietal.,2024a),UniDepth(Piccinellietal.,2024),ZeroDepth(Guizilinietal.,
                                   2023),andZoeDepth(Bhatetal.,2023).Topreservethezero-shotsetting,wedonotreportresults
                                   formodelsthatweretrainedonthesamedatasetastheevaluationdataset.Wereportcommonlyused
                                   metricsinthedepthestimationliterature,namelyAbsRel, Log(Saxenaetal.,2009),δ , δ andδ
                                                                                         10                    1  2     3
                                   scores(Ladickyetal.,2014),aswellaspoint-cloudmetrics(Spenceretal.,2022).Duetothehigh
                                   resolutionofBoosterimages,wewerenotabletoobtainpoint-cloudmetricsinreasonabletime.
                                      1https://www.unsplash.com
                                                                             17
                        Preprint
                           InputImageDepthPro(Ours)DepthAnythingv2MarigoldMetric3Dv2
                        Figure5:Zero-shotresultsofDepthPro,Marigold(Keetal.,2024),Metric3Dv2(Huetal.,2024),
                        andDepthAnythingv2(Yangetal.,2024b)onimagesfromUnsplash(Lietal.,2022a),AM-2k(Li
                        etal.,2022a),andDIS-5k(Qinetal.,2022).
                                                    18
                        Preprint
                           InputImageDepthPro(Ours)DepthAnythingv2MarigoldMetric3Dv2
                        Figure6:Zero-shotresultsofDepthPro,Marigold(Keetal.,2024),Metric3Dv2(Huetal.,2024),
                        andDepthAnythingv2(Yangetal.,2024b)onimagesfromUnsplash(Lietal.,2022a),AM-2k(Li
                        etal.,2022a),andDIS-5k(Qinetal.,2022).
                                                    19
                        Preprint
                           InputImageDepthPro(Ours)DepthAnythingv2MarigoldMetric3Dv2
                        Figure7:Zero-shotresultsofDepthPro,Marigold(Keetal.,2024),Metric3Dv2(Huetal.,2024),
                        andDepthAnythingv2(Yangetal.,2024b)onimagesfromUnsplash(Lietal.,2022a),AM-2k(Li
                        etal.,2022a),andDIS-5k(Qinetal.,2022).
                                                    20
                                          Preprint
                                         Table4:Additionalzero-shotmetricdepthevaluation.Wereportadditionalmetricsusedinthe
                                          depthestimationliterature,namelyAbsRel(Ladickyetal.,2014),Log, δ andδ scores,aswellas
                                                                                                                    2      3
                                                                                                               10
                                          point-cloudmetrics(Spenceretal.,2022)onBooster(Ramirezetal.,2024),Middlebury(Scharstein
                                                                                                     ¨
                                          etal.,2014),Sun-RGBD(Songetal.,2015),ETH3D(Schopsetal.,2017),nuScenes(Caesaretal.,
                                          2020),andSintel(Butleretal.,2012).Forfaircomparison,allreportedresultswerereproducedin
                                          ourenvironment.
                                           NuScenesAbsRel↓ Log↓ δ ↑ δ ↑                                     SI-Log↓  PC-CD↓   PC-F↑   PC-IoU↑
                                                                                      10 2           3
                                           DepthAnything(Yangetal.,2024a)0.4530.15173.87690.30128.15324.1460.0070.004
                                           DepthAnythingv2(Yangetal.,2024b)0.6140.32631.83747.26529.73737.5160.0080.004
                                           Metric3D(Yinetal.,2023)0.4220.13277.22083.60533.82729.2840.0070.004
                                           Metric3Dv2(Huetal.,2024)0.1970.08093.25295.73627.03214.8760.0080.004
                                           PatchFusion(Lietal.,2024a)0.3920.22648.74276.03531.17120.8360.0060.003
                                           UniDepth(Piccinellietal.,2024)0.1380.06093.00696.41521.80111.6290.0090.004
                                           ZeroDepth(Guizilinietal.,2023)0.2370.12182.59689.90830.70323.3480.0070.004
                                           ZoeDepth(Bhatetal.,2023)0.4980.18264.94782.70431.50139.1830.0060.003
                                           DepthPro(Ours)0.2870.16473.83684.25229.54822.4800.0100.005
                                           Sintel AbsRel↓ Log↓ δ ↑ δ ↑ SI-Log↓ PC-CD↓ PC-F↑ PC-IoU↑
                                                                                      10 2           3
                                           DepthAnything(Yangetal.,2024a)3.9730.55915.41827.28135.77138.5920.0570.030
                                           DepthAnythingv2(Yangetal.,2024b)2.2260.49418.69633.82041.92354.9310.0570.031
                                           Metric3D(Yinetal.,2023)1.7330.38732.37544.79348.60545.8580.0560.031
                                           Metric3Dv2(Huetal.,2024)0.3700.21662.91576.86625.31234.7900.0910.051
                                           PatchFusion(Lietal.,2024a)0.6170.39135.51551.44336.80644.6150.0770.045
                                           UniDepth(Piccinellietal.,2024)0.8690.30135.72257.25642.83732.3380.0980.057
                                           ZeroDepth(Guizilinietal.,2023)0.7030.49125.62937.07650.83976.2740.0520.029
                                           ZoeDepth(Bhatetal.,2023)0.9460.39222.69844.96940.21752.3010.0850.049
                                           DepthPro(Ours)0.5080.23059.24771.13827.49441.9680.1210.073
                                           Sun-RGBDAbsRel↓ Log↓ δ ↑ δ ↑                                     SI-Log↓  PC-CD↓   PC-F↑   PC-IoU↑
                                                                                      10 2           3
                                           DepthAnything(Yangetal.,2024a)0.1140.05398.81199.7708.0380.0340.1600.090
                                           DepthAnythingv2(Yangetal.,2024b)0.1820.07097.64599.4628.3900.0450.1690.096
                                           Metric3D(Yinetal.,2023)1.7120.38226.99934.11620.2620.5060.0600.032
                                           Metric3Dv2(Huetal.,2024)0.1560.07696.34899.5487.4330.0250.1790.102
                                           PatchFusion(Lietal.,2024a)0.4660.96160.14560.65169.647331.4770.0520.027
                                           UniDepth(Piccinellietal.,2024)0.0870.03799.33099.8046.9680.0200.2940.183
                                           ZoeDepth(Bhatetal.,2023)0.1230.05397.95499.5058.9640.0480.1350.075
                                           DepthPro(Ours)0.1130.04998.50699.5477.8410.0390.1790.103
                                           ETH3DAbsRel↓ Log↓ δ ↑ δ ↑ SI-Log↓ PC-CD↓ PC-F↑ PC-IoU↑
                                                                                      10 2           3
                                           DepthAnything(Yangetal.,2024a)1.6820.38019.78431.05710.9030.0720.1720.114
                                           DepthAnythingv2(Yangetal.,2024b)0.3700.17364.65786.2569.6830.0420.3300.233
                                           Metric3D(Yinetal.,2023)0.8590.24049.29157.57314.5410.0720.3030.219
                                           Metric3Dv2(Huetal.,2024)0.1240.05399.55399.9006.1970.0830.4660.358
                                           PatchFusion(Lietal.,2024a)0.2560.10688.37897.30611.0230.0420.2090.135
                                           UniDepth(Piccinellietal.,2024)0.4570.18657.67081.4837.7290.0310.4090.305
                                           ZoeDepth(Bhatetal.,2023)0.5000.17664.45281.43413.2500.0780.1270.082
                                           DepthPro(Ours)0.3270.19361.30971.22810.1700.0940.4870.398
                                           MiddleburyAbsRel↓ Log↓ δ ↑                               δ ↑     SI-Log↓  PC-CD↓   PC-F↑   PC-IoU↑
                                                                                      10 2           3
                                           DepthAnything(Yangetal.,2024a)0.2730.14969.61986.06012.4200.1020.1030.055
                                           DepthAnythingv2(Yangetal.,2024b)0.2620.14172.07490.5499.6390.0630.1270.069
                                           Metric3D(Yinetal.,2023)1.2510.30537.52858.73312.0910.0690.0690.036
                                           Metric3Dv2(Huetal.,2024)0.4500.15273.32188.6105.5190.0220.2150.122
                                           PatchFusion(Lietal.,2024a)0.2500.10887.16698.15414.6410.3190.0840.044
                                           UniDepth(Piccinellietal.,2024)0.3240.12780.04799.6217.3790.1130.2210.129
                                           ZeroDepth(Guizilinietal.,2023)0.3770.17967.06078.95214.4820.2320.0520.027
                                           ZoeDepth(Bhatetal.,2023)0.2140.11577.68390.86010.4480.0690.1140.062
                                           DepthPro(Ours)0.2510.08993.16996.4018.6100.1070.1610.091
                                                        BoosterAbsRel↓ Log↓ δ ↑ δ ↑ SI-Log↓
                                                                                                   10 2           3
                                                        DepthAnything(Yangetal.,2024a)0.3170.11479.61595.22810.507
                                                        DepthAnythingv2(Yangetal.,2024b)0.3150.11076.23994.2767.056
                                                        Metric3D(Yinetal.,2023)1.3320.34613.07333.97510.631
                                                        Metric3Dv2(Huetal.,2024)0.4170.14075.78392.8333.932
                                                        PatchFusion(Lietal.,2024a)0.7190.21349.38772.89214.128
                                                        UniDepth(Piccinellietal.,2024)0.5000.16660.90489.2137.436
                                                        ZoeDepth(Bhatetal.,2023)0.6100.19552.65575.50810.551
                                                        DepthPro(Ours)0.3360.11879.42996.5244.616
                                                                                          21
                                 Preprint
                                 A.3RUNTIME
                                 Toassessthelatencyofourapproachincomparisontobaselines,wetestallapproachesonimagesof
                                 varyingsizesandreportresultsinTab.5.Wepickcommonimageresolutions(VGA:640×480,HD:
                                 1920×1080,4K:4032×3024)andmeasureeachmethod’saverageruntimeforprocessinganimage
                                 ofthegivensize.Allreportedruntimesarereproducedinourenvironmentandincludepreprocessing,
                                 eventualresizing(formethodsoperatingataﬁxedinternalresolution),andinferenceofeachmodel.
                                 Wefurtherreporttheparametercountsandﬂops(atHDresolution)foreachmethodasmeasured
                                 withthefvcorepackage.
                                 Amongallapproacheswithaﬁxedoutputresolution,DepthProhasthehighestnativeoutput
                                 resolution,processingmorethan3timesasmanypixelsasthenexthighest,Metric3Dv2(Huetal.,
                                 2024).YetDepthProhaslessthanhalftheparametercountandrequiresonlyathirdoftheruntime
                                 comparedtoMetric3Dv2.
                                 Thevariable-resolutionapproaches(PatchFusion(Lietal.,2024a)andZeroDepth(Guizilinietal.,
                                 2023))haveconsiderablylargerruntime,withthefastermodel,ZeroDepth,takingalmost4timesas
                                 longasDepthPro,evenforsmallVGAimages.
                                 Table5:Modelperformance,measuredonaV100-32GGPU.Wereportruntimesinmilliseconds
                                 (ms)onimagesofmultiplesizes,aswellasmodelparametercountsandﬂops.Forfairness,the
                                 reportedruntimesarereproducedinourenvironment.Entriesaresortedbythenativeoutput
                                 resolution.
                                                                     Nativeoutput
                                                 Parameter
                                    MethodFlops↓                             ↑      t  (ms)↓ t  (ms)↓ t  (ms)↓
                                                            HD                      VGAHD 4K
                                                  countresolution
                                    DPT123M-384×384=0.15MP33.230.627.8
                                    ZoeDepth340M-384×512=0.20MP235.7235.1235.4
                                    DepthAnythingv2335M1827G518×518=0.27MP90.991.191.2
                                    UniDepth347M630G462×616=0.28MP178.5183.0198.1
                                    Metric3D203M477G480×1216=0.58MP217.9263.8398.1
                                    Marigold949M-768×768=0.59MP5174.34433.64977.6
                                    Metric3Dv21.378G6830G616×1064=0.66MP1299.61299.71390.2
                                    PatchFusion203M-Original(tile-based)84012.084029.984453.9
                                    ZeroDepth233M10862GOriginal1344.38795.734992.2
                                    DepthPro504M4370G1536×1536=2.36MP341.3341.3341.3
                                 A.4BOUNDARYEXPERIMENTS
                                 Boundarymetricsempiricalstudy.Toillustratehowourboundarymetricswork,wereport
                                 additionalqualitativeedgemetricresultsinFig.8.Inparticular,weshowtheoccludingcontours
                                 derivedfromtheground-truthandpredicteddepth,whichillustratehowincorrectdepthboundary
                                 predictionscanimpactthemetric.Furthermore,toillustratethebehavioroftheboundaryprecision
                                 andrecallmeasurementsundervariousimageperturbationswealsoprovideanempiricalstudyin
                                 Fig.9.WereportbothquantitativeandqualitativeresultsonsamplesfromtheUnrealStereo4K
                                 dataset(Tosietal.,2021).Ourresultsempiricallydemonstratethecorrelationbetweenerroneous
                                 depthedgepredictionsandlowprecisionandrecallvalues.
                                 ResultsontheiBimsdataset(Kochetal.,2018).Wesupplementourboundaryevaluationbyresults
                                 ontheiBimsdataset,whichiscommonlyusedforevaluatingdepthboundaries.iBimsconsistsof
                                 imagesofindoorscenesthathavebeenlaser-scanned.Theimagesareat640×480resolutionand
                                 havebeensupplementedwithmanuallyannotatedocclusionboundarymapstofacilitateevaluation.
                                 TheiBimsbenchmarkusesDepthDirectedErrors(DDE),whichevaluateoverallmetricdepth
                                 accuracy,DepthBoundaryErrors(DBE),whicharesimilarinspirittoourproposedboundarymetric
                                 butrequiremanualannotation,andPlanarErrors,whichevaluatetheaccuracyofplanesderived
                                 fromthedepthmaps.
                                 WeﬁndthatDepthProisonparwiththestateoftheartaccordingtotheDDEandPEmetrics,and
                                 signiﬁcantlyoutperformsallpriorworkaccordingtotheboundarymetrics.
                                                                        22
                                    Preprint
                                                                                ˆ            !               !
                                              Imaged                            d                c (i,j)         c (i,j)
                                                                                                 d                ˆ
                                                                                                                  d
                                                                                           (i,j)∈N          (i,j)∈N
                                       Sintel(Butleretal.,2012)
                                       Spring(Mehletal.,2023)
                                       AM-2k(Lietal.,2022a)
                                       P3M-10k(Lietal.,2021)
                                       DIS-5k(Qinetal.,2022)
                                    Figure8:Evaluationmetricsforsharpboundaries.Weproposenovelmetricstoevaluatethe
                                    sharpnessofocclusionboundaries.Themetricscanbecomputedonground-truthdepthmaps(ﬁrst
                                    tworows),andbinarymapsthatcanbederivedfrommattingorsegmentationdatasets(subsequent
                                    rows).Eachrowshowsasampleimage,thegroundtruthforderivingocclusionboundaries,our
                                    prediction,ground-truthoccludingcontours,andoccludingcontoursfromtheprediction.Forthese
                                    visualizationswesett =15.
                                    BCONTROLLEDEXPERIMENTS
                                    Weconductseveralcontrolledexperimentstoinvestigatetheimpactofvariouscomponentsand
                                    designdecisionsinDepthPro.Speciﬁcally,weaimtoassessthecontributionofkeycomponentsin
                                    thenetworkarchitecture(Sec.B.1),trainingobjective(Sec.B.3),trainingcurriculum(Sec.B.4),and
                                    thefocallengthestimationhead(Sec.B.5).
                                    B.1NETWORKBACKBONE
                                    Webeginbyevaluatingvariouscandidateimageencoderbackboneswithinournetworkarchitecture.
                                    Toassesstheirperformance,weconductacomparativeanalysisutilizingoff-the-shelfmodels
                                    availablefromtheTIMMlibrary(Wightman,2019).Usingthepretrainedweights,wetraineach
                                    backboneat384×384resolutionacrossﬁveRGB-Ddatasets(Keystone,HRWSI,RedWeb,TartanAir,
                                    andHypersim)andevaluatetheirperformanceintermsofmetricdepthaccuracyacrossmultiple
                                                                              23
                                                 Preprint
                                                                                                                               ˆ          !               !
                                                                                                                d              d               c (i,j)         c (i,j)
                                                                                                                                                d                ˆ
                                                                                                                                                                d
                                                                                                                                        (i,j)∈N        (i,j)∈N
                                                            ,-./0102314.5678190
                                                            :;</6=>?@A.BC.D0E:;</6=>?@F151/.BCE
                                                            :;</6=>?@F1GHI1GEJBHDD1B/?F5HC
                                                                                                      GaussianBlur
                                                        '
                                                       !"+
                                                       !"&                                             Pixel-Shift
                                                       !"*
                                                    )#$*++!"%
                                                                                                      Up-Down(nearest)
                                                       !")
                                                       !"$
                                                       !"(                                            Up-Down(bilinear)
                                                       !"#
                                                        !!"#!"$!"%!"&'
                                                                       !"#$%&%'(
                                                                                                      Up-Down(bicubic)
                                                 Figure9:Boundaryevaluationmetricsempiricalstudy.Wedemonstratehowvarioustypesof
                                                 imageperturbationsimpactourproposededgemetrics.Wereportquantitativeandqualitativeresults
                                                 formultipleground-truthperturbations,suchassimpleimageshifts,downsamplingfollowedby
                                                 upsamplings,andGaussianblurring.Wereportbothground-truthandperturbedoccludingcontours,
                                                 usedtoderiveourF1scores.Ourresultsempiricallydemonstratethecorrelationbetweenerroneous
                                                 depthedgepredictionsandlowprecisionandrecallvalues.
                                                 Table6:Zero-shotmetricdepthevalutionontheiBimsdataset(Kochetal.,2018).Wereport
                                                 theiBims-speciﬁcDepthDirectedErrors(DDE),DepthBoundaryErrors(DBE)andPlanarErrors
                                                 (PE).Forfairness,allreportedresultswerereproducedinourenvironment.PleaseseeSec.A.4
                                                                                                                                                         ◦
                                                              MethodDDE(in%)DBE(inpx)PE(inm/)
                                                                                                           −         +                 compplan
                                                                                                  0                           acc                        orie
                                                                                                 ε   ↑    ε    ↓    ε   ↓    ε   ↓    ε    ↓    ε  ↓    ε   ↓
                                                                                                  DDE                         DBE                        PE
                                                                                                           DDEDDE                      DBEPE
                                                            DPT(Ranftletal.,2021)58.74441.2550.0003.58039.3720.13831.837
                                                            Metric3D(Yinetal.,2023)88.6081.33710.0542.07319.0110.10022.451
                                                            Metric3Dv2(Huetal.,2024)84.7210.54614.7321.84310.0620.09519.561
                                                            ZoeDepth(Bhatetal.,2023)85.60013.8740.5251.96018.1660.10320.108
                                                            DepthAnything(Yangetal.,2024a)88.95110.7410.3082.08119.1720.10620.680
                                                            DepthAnythingv2(Yangetal.,2024b)91.7731.6196.6071.9598.3500.09519.406
                                                            PatchFusion(Lietal.,2024a)85.76512.6021.6331.71120.7220.11723.926
                                                            Marigold(Keetal.,2024)58.73841.2610.0001.85512.7420.16833.734
                                                            UniDepth(Piccinellietal.,2024)73.0200.04126.9391.99914.2340.09819.114
                                                            DepthPro(Ours)89.7251.8098.4641.68010.1380.09518.776
                                                 datasets,includingBooster,Hypersim,Middlebury,andNYUv2,utilizingmetricssuchasAbsRelfor
                                                 afﬁne-invariantdepthandLogformetricdepthinTab.7.WeﬁndthatViT-LDINOv2(Oquabetal.,
                                                                                     10
                                                 2024)outperformsallotherbackbonesbyasigniﬁcantmarginandconcludethatthecombination
                                                 ofbackboneandpretrainingstrategyconsiderablyaffectsdownstreamperformance.Followingthis
                                                 analysis,wepickViT-LDINOv2forbothourencoderbackbones.
                                                  B.2HIGH-RESOLUTIONALTERNATIVES
                                                 Wefurtherevaluatealternativehigh-resolution1536×1536networkstructuresanddifferentpre-
                                                 trainedweights(Tab.8).Todothis,wetestgeneralizationaccuracybytrainingonatrainsplitof
                                                 somedatasetsandtestingonavalortestsplitofotherdatasets,followingtheStage1protocolfor
                                                                                                            24
                            Preprint
                            allmodelsinaccordancewithTab.14andTab.15.AllViTmodelsuseapatchsizeof16×16.For
                            weightspretrainedwithapatchsizeof14×14weapplybicubicinterpolationtotheweightsofthe
                            convolutionalpatchembeddinglayerandscaletheseweightsinverselytothenumberofpixels(i.e.,
                            theweightsarereducedbyafactorof1.3).AllViTmodelsuseresolution1536×1536,forthiswe
                            applybicubicinterpolationtopositionalembeddingspriortotraining.TheDepthProapproachinall
                            casesusesViTwithresolution384×384andpatchsize16×16forboththepatchencoderandthe
                            imageencoder.SWINv2andconvolutionalmodelsarepretrainedonImageNet(Dengetal.,2009).
                            Othermodelsusedifferentpretrainingapproachesdescribedintheirpapers:CLIP(Radfordetal.,
                            2021),MAE(Heetal.,2022),BeiTv2(Pengetal.,2022b),andDINOv2(Oquabetal.,2024).For
                            theSegmentAnythingmodelweusepubliclyavailablepretrainedweights,whichwereinitialized
                            usingMAEpretraining(Heetal.,2022)andsubsequentlytrainedforsegmentationasdescribedin
                            theirpaper(Kirillovetal.,2023).
                            WeﬁndthatthepresentedDepthProapproachisfasterandmoreaccurateforobjectboundariesthan
                            theplainViT,withcomparablemetricdepthaccuracy.Incomparisontoothertransformer-basedand
                            convolutionalmodels,DepthProhascomparablelatency,severaltimeslowermetricdeptherror,and
                            severaltimeshigherrecallaccuracyforobjectboundaries.
                            Table7:Comparisonofimageencoderbackbonescandidates.Wetraineachbackboneat384×384
                            resolutionacrossﬁveRGB-Ddatasets:Keystone,HRWSI,RedWeb,TartanAir,andHypersim.To
                            ensurefaircomparison,weselectbackbonecandidateswithcomparablecomputationalcomplexity,
                            measuredinFlopsusingthefvcorelibrary(FacebookResearch,2022),andanequivalentnumberof
                            parameters.WeidentifyViT-LDINOv2(Oquabetal.,2024)astheoptimalchoiceforourimage
                            encoderbackbone,givenitssuperiordepthaccuracyperformance.
                              BackboneFlops(G)Params(M)AbsRel↓ Log↓
                                                                                      10
                              ViT-LDINOv2-reg4(Oquabetal.,2024)2483450.0390.138
                              ViT-LDINOv2(Oquabetal.,2024)2473450.0400.129
                              ViT-LMAE(Heetal.,2022)2473430.0410.150
                              ViT-LBeiTv2(Pengetal.,2022b)2423360.0420.134
                              ViT-LBeiT(Bhatetal.,2023)2593360.0480.147
                              ViT-LSO400m-siglip(Zhaietal.,2023)3114710.0510.174
                              ViT-LCLIP-quickgelu(Radfordetal.,2021)2473440.0530.166
                              ViT-LCLIP(Radfordetal.,2021)2473450.0570.156
                              ViT-L(Dosovitskiyetal.,2021)2473450.0610.163
                              ConvNext-XXL(Liuetal.,2022b)5148670.0750.216
                              ViT-LDeiT-3(Touvronetal.,2022)2473450.0780.176
                              ConvNext-L-mlp(Liuetal.,2022b)1622140.0810.222
                              ConvNextv2-H(Wooetal.,2023)4056800.0850.242
                              SegAnythingViT-L(Kirillovetal.,2023)2453300.0870.311
                              SWINv2-L(Liuetal.,2022a)1772120.0910.240
                              CAFormer-B36(Yuetal.,2024)1241080.0910.248
                              EfﬁcientViT-L3(Liuetal.,2023)––0.1090.303
                            B.3TRAININGOBJECTIVES
                            Toassesstheefﬁcacyofourtrainingcurriculum,wecompareittoalternativetrainingschedules.We
                            ﬁrstexaminethedifferentstagesindividuallyandthencomparefullcurricula.
                            Stage1trainingobjectives.Weﬁrstevaluatelosscombinationsfortheﬁrststage(Tab.9).Condition
                            1Aonlyappliesameanabsoluteerrorlosstoalldatasets.Fornon-metricdatasets,weusethescale-
                            and-shift-invariantversion.Condition1Baddsgradientlossestoallsyntheticdatasets.Weagainuse
                            thescale-and-shift-invariantversionfornon-metricdatasets.FollowingourobservationsfromSec.3,
                            weproposetoapplyanappropriatemeanabsoluteerrorlossasinotherconditionsdependingona
                            datasetbeingmetric,butapplyascale-and-shift-invariantgradientlossirrespectiveofadatasetbeing
                            metricornot(C).Althoughtheintuitionofapplyingagradientlossistosharpentheboundaries
                            andimproveedge-relatedmetrics,italsoimprovesother,non-boundary-relatedmetrics.Thekey
                            ﬁndingisthatthegradientlossmustbescale-and-shift-invariant,irrespectiveofthedatasetbeing
                                                            25
                                              Preprint
                                              Table8:High-resolutionalternatives.Generalizationaccuracyofalternativehigh-resolution
                                              1536×1536modelsanddifferentpretrainedweights.AllmodelsaretrainedidenticallyusingStage
                                              1inaccordancewithTab.14andTab.15.LatencymeasuredonasingleGPUV100withFP16
                                              precisionusingbatch=1.AllViTmodelsuseapatchsizeof16×16.DepthProemploysaViT-L
                                              DINOv2(Oquabetal.,2024)fortheimageandpatchencoders.
                                                                                                              MetricdepthaccuracyBoundaryaccuracy
                                                       MethodLatency,ms↓ NYUv2δ ↑ iBimsδ ↑ iBimsF1↑ DISR↑
                                                                                                                    1           1
                                                       EfﬁcientNetV2-XL(Tan&Le,2021)1184.47.00.0050.000
                                                    Conv.ConvNext-XXL(Liuetal.,2022b)30468.038.30.1340.031
                                                       ConvNextv2-H(Wooetal.,2023)28770.056.60.1310.044
                                                       S.Anything(Kirillovetal.,2023)(ViT-L)34953.238.90.1400.051
                                                       S.Anything(Kirillovetal.,2023)(ViT-H)36551.741.10.1460.050
                                                    Trans.SWINv2-L(Liuetal.,2022a)(window=24)27258.433.10.1170.028
                                                       ViT-LCLIP(Radfordetal.,2021)38492.281.90.1570.052
                                                       ViT-LBeiTv2(Pengetal.,2022b)OOM90.486.50.1490.042
                                                    ViTViT-LMAE(Heetal.,2022)39092.784.70.1630.065
                                                       ViT-LDINOv2(Oquabetal.,2024)39296.590.30.1610.065
                                                       DepthPro34196.191.30.1770.080
                                              Table9:Comparisonofstage1trainingobjectives.1AonlyappliestheL                               tometric,andthe
                                                                                                                                    MAE
                                              L            tonon-metricdatasets.1Dadditionallyminimizesgradientsonalldatasets.1Bminimizes
                                                SSI-MAE
                                              gradientsonlyonsyntheticdatasets.Weuse1C,whichminimizesgradientswithascale-and-shift-
                                              invariantL                lossonallsyntheticsyntheticdatasetsirrespectiveofwhetherthedatasetis
                                                           SSI-MAGE
                                              metric.
                                                                           HRWSIHypersimApolloscape
                                                               Cond.AbsRel↓      δ ↑    Log↓      AbsRel↓    δ ↑    Log↓      AbsRel↓   δ ↑
                                                                                  1        10                 1        10                1
                                                               1A0.16682.10.0830.25975.40.1560.33945.6
                                                               1D0.13885.10.0770.24678.40.1280.42460.6
                                                               1B0.15683.30.0780.24977.30.1520.30047.3
                                                               1C0.15083.70.0740.23579.90.0840.23575.6
                                              metricornot.Applyingaregulargradientlossonmetricdatasetsthatearlyintrainingactuallyharms
                                              convergence.
                                              Stage2trainingobjectives.Thesecondstageofourtrainingcurriculumfocusesonsharpening
                                              Table10:Comparisonofstage2trainingobjectives.Weevaluatetheefﬁcacyofderivative-based
                                              lossesforsharpeningboundaries.Employingﬁrst-andsecond-orderderivativelosses(2A)yieldsthe
                                              bestresultsonbalanceasindicatedbytheaveragerankovermetrics.Moredetailsinthetext.
                                                                                    HRWSIHypersimApolloscape
                                                       ConditionLMSELMAGELMSGELMALEAbsRel↓ δ ↑    Log↓      AbsRel↓   δ ↑    Log↓      AbsRel↓   δ ↑
                                                                                            1        10                1        10                1
                                                     2A!!!!0.14983.60.0720.23581.30.0920.30372.9
                                                     2B!!!0.14883.70.0720.23081.00.0920.29973.1
                                                     2C!!0.15083.70.0720.23580.80.0910.30073.2
                                                     2D!                        0.15083.40.0740.23979.80.0960.34972.8
                                                     2E                         0.15982.70.0740.24280.60.0960.34673.3
                                              depthboundaries.Tothatend,weonlyemploysyntheticdatasetsduetotheirhighqualityground
                                              truth.Theobviousstrategyforsharpeningpredictionsistheapplicationofgradientlosses.We
                                              evaluateourcombinationofmultiscalederivative-basedlossesinanablationstudy.Condition2A
                                              usesallofthelosses,namelyL                , L      ,L        , L       ,andL           .Tab.10.2Bremoves
                                                                                  MAEMSEMAGEMALEMSGE
                                              thesecond-orderlossL             .2CfurtherremovesthesquaredﬁrstorderlossesL                     .2Dremoves
                                                                        MALE                                                            MSGE
                                              allderivative-basedlosses.2EappliestheL                toalldatasets.RemovingL                improvesresults
                                                                                               MAE MALE
                                              onApolloscape.Ourcombinationof0th-to2nd-orderderivativelosses(2A)performsbestacross
                                              metricsanddatasetsinaggregate(e.g.,intermsoftheaveragerankacrossmetrics).
                                                                                                     26
                                              Preprint
                                               B.4FULLCURRICULA
                                              Table11:Comparisonoffullcurricula.Weevaluateourcurriculum(3A)againstsinglestage
                                              training(3B),andpre-trainingonsynthetic,ﬁne-tuningwithrealdata(3C).
                                                                            HRWSIHypersimApolloscape
                                                             Cond.AbsRel↓          δ ↑    Log↓      AbsRel↓   δ ↑    Log↓      AbsRel↓    δ ↑
                                                                                    1        10                1        10                 1
                                                             3A(Ours)0.14983.60.0720.23581.30.0920.30372.9
                                                             3B 0.14883.90.0730.24581.30.0950.29272.1
                                                             3C 0.15383.60.1660.38637.10.5860.7120.5
                                              Finally,weassesstheefﬁcacyofourcompletetrainingcurriculumincomparisontoalternatives.
                                              Condition3Arepresentsourtwo-stagecurriculum.Condition3Btrainsinasinglestageandapplies
                                              allthesecond-stagegradientlossesthroughoutthewholetraining.Condition3Creversesourtwo
                                              stagesandrepresentstheestablishedstrategyofpretrainingonsyntheticdataﬁrstandﬁne-tuning
                                              withreal-worlddata.
                                               B.5FOCALLENGTHESTIMATION
                                              Additionalanalysisofzero-shotfocallengthestimationaccuracy.InFig.10,wepresentamore
                                              comprehensiveanalysisofourfocallengthpredictor’sperformancecomparedtobaselinemodels.To
                                              thatend,weplotthepercentageofsamplesbelowacertainabsoluterelativeerrorforeachmethod
                                              anddatasetinourzero-shotevaluationsetup.DepthProoutperformsallapproachesonalldatasets.
                                              Figure10:Evaluationoffocallengthestimation.Eachplotcomparesanumberofmethodsona
                                              givendataset.Thex axisrepresentstheAbsRelerrorandthey axisrepresentsthepercentageof
                                              sampleswhoseerrorisbelowthatmagnitude.
                                                                                                                          Table12:Ablationstudyonfo-
                                               Controlledevaluationofnetworkstructures.Weevaluate
                                                                                                                          callengthestimation.
                                              anumberofchoicesforthefocallengthestimationheadand
                                              reportresultsinTab.12.Themodelsareevaluatedon500images
                                                                                                                           Architectureδ                   ↑
                                                                                                                                                       25%
                                              randomlysampledfromFlickr(Thomeeetal.,2016).Theﬁrst
                                                                                                                           DPTonly60.0
                                              row,“DPTonly”showstheperformanceofanetworkwiththe
                                                                                                                           ViTonly74.4
                                              frozenDPTdepthfeatureextractor(Ranftletal.,2021)anda
                                                                                                                           DPT&ViTinseries63.6
                                              convolutionalhead.Thesecondrow,“ViTonly”,demonstrates
                                                                                                                           DPT&ViTinparallel78.2
                                              theperformanceofonlyusingasmallViTnetworktrainedfrom
                                                                                                     27
                  Preprint
                  scratch(Dosovitskiyetal.,2021).Thethirdrow,“DPT&ViTinseries”,usesthefrozenDPTfeature
                  extractorfollowedbyasmallViTnetwork.Thefourthrow,“DPT&ViTinparallel”,representsour
                  chosenarchitecturedepictedinFig.3,whichutilizesfrozenfeaturesfromthedepthnetworkand
                  task-speciﬁcfeaturesfromaseparateViTimageencoderinparallel.
                  Weobservethat“DPT&ViTinseries”exhibitssimilarperformanceto“DPTonly”,suggestingthat
                  addingmorecomputationontopofthefrozenDPTfeatureinadditiontooursmallconvolutional
                  headdoesnotprovideextrabeneﬁtsdespitetheincreasedcomputation.Whencomparing“DPT&
                  ViTinseries”with“DPT&ViTinparallel”,weobserveanaccuracydifferenceof14.6percentage
                  points.Thisindicatesthataccuratefocallengthpredictionrequiresextratask-speciﬁcknowledge
                  inadditiontodepthinformation.Furthermore,“DPT&ViTinparallel”outperforms“ViTonly”,
                  whichhighlightstheimportanceoffeaturesfromthepretraineddepthnetworkforobtaininga
                  high-performingfocallengthestimator.
                  CIMPLEMENTATION,TRAININGANDEVALUATIONDETAILS
                  Inthissectionweprovideadditionaldetailsonthedatasetsusedfortrainingandevaluation,hyperpa-
                  rametersettingsforourmethod,anddetailsontheevaluationsetup.
                  C.1DATASETS
                  Tab.13providesacomprehensivesummaryofthedatasetsutilizedinourstudy,detailingtheir
                  respectivelicensesandspecifyingtheirroles(e.g.,trainingortesting).
                  C.2TRAININGHYPERPARAMETERS.
                  WespecifythetraininghyperparametersinTab.14andTab.15.
                  C.3BASELINES
                  Belowweprovidefurtherdetailonthesetupofthebaselines.
                  DepthAnything.DepthAnythingv1andv2eachreleasedageneralmodelforrelativedepth,but
                  theirmetricdepthmodelsaretailoredtospeciﬁcdomains(indoorvs.outdoor).Forthemetricdepth
                  evaluation,wematchthesemodelstodatasetsaccordingtotheirdomain,andfordatasetscontaining
                  bothindoorandoutdoorimages,weselectthemodelwiththebestperformance.Forqualitative
                  resultsandthe(scaleandshiftinvariant)zero-shotboundaryevaluation,weemploytherelativedepth
                  models,sincetheyyieldbetterqualitativeresultsandsharperboundariesthanthemetricmodels.
                  Metric3D.ForMetric3Dv1andv2,wefoundthatthecropsizeparameterstronglyaffectsmetric
                  scaleaccuracy.Infact,usingaﬁxedcropsizeconsistentlyyieldedverypoorresultsonatleastone
                  metricdataset.Inordertoobtainacceptableresults,weuseddifferentcropsizesforindoor(512,
                  1088)andoutdoor(512,992)datasets.AsinthecaseofDepthAnything,wemarktheseresultsin
                  graytoindicatethattheyarenotstrictlyzero-shot.ForMetric3Dv2,weusethelargest(‘giant’)
                  model.
                  UniDepth.ForUniDepth,weusetheViT-Lversion,whichperformsbestonaverageamongthe
                  UniDepthvariants.
                  ZoeDepth.Weusethemodelﬁnetunedonbothindoorandoutdoordata(denotedZoeDNK).
                  C.4EVALUATIONSETUP
                  Inevaluatingourapproachandbaselines,wefoundtherangeofvaliddepthvalues,thedepthmap
                  resolutionusedforcomputingmetrics,theresizingapproachusedformatchingtheresolutionofthe
                  groundtruthdepthmaps,andthechoiceofintrinsicstoaffectresults,sometimesstrongly.Thisis
                  whywemadeanefforttosetupandevaluateeachbaselineinthesame,fairevaluationsetup,which
                  wedetailbelow.
                                       28
                        Preprint
                                          Table13:Datasetsusedinthiswork.
                         DatasetURLLicenseUsage
                         3DKenBurns(Niklausetal.,2019)https://github.com/sniklaus/3d-ken-burnsCC-BY-NC-SA4.0Train
                         AM-2K(Lietal.,2022a)https://github.com/JizhiziLi/GFMCustomTesting
                         Apolloscape(Huangetal.,2020)https://apolloscape.auto/CustomVal
                         ARKitScenes(Dehghanetal.,2021)https://github.com/apple/ARKitScenesCustomTrain
                         Bedlam(Blacketal.,2023)https://bedlam.is.tue.mpg.de/#dataCustomTrain
                         BlendedMVG(Yaoetal.,2020)https://github.com/YoYo000/BlendedMVSCCBY4.0Train
                         Booster(Ramirezetal.,2024)https://cvlab-unibo.github.io/booster-web/CCBYNC4.0Test
                         DDAD(Guizilinietal.,2020)https://github.com/TRI-ML/DDADCC-BY-NC-SA4.0Testing
                         DIML(indoor)(Kimetal.,2016)https://dimlrgbd.github.io/CustomTrain
                         DIS5K(Qinetal.,2022)https://xuebinqin.github.io/dis/index.htmlCustomTest
                         DPDD(Abuolaim&Brown,2020)https://github.com/Abdullah-Ab...pixelMITTesting
                         DynamicReplica(Karaevetal.,2023)https://github.com/facebookres...stereoCCBY-NC4.0Train
                         EDEN(Leetal.,2021)https://lhoangan.github.io/eden/CustomTrain
                              ¨
                         ETH3D(Schopsetal.,2017)https://www.eth3d.net/CC-BY-NC-SA4.0Testing
                         FiveK(Bychkovskyetal.,2011)https://data.csail.mit.edu/graphics/fivek/CustomTesting
                         HRWSI(Xianetal.,2020)https://kexianhust.github....Ranking-Loss/CustomTrain,Val
                         Hypersim(Robertsetal.,2021)https://github.com/apple/ml-hypersimCustomTrain,Val
                         iBims(Kochetal.,2018)https://www.asg.ed.tum.de/lmf/ibims1/CustomTest
                         IRS(Wangetal.,2019)https://github.com/HKBU-HPML/IRSCustomTrain
                         KITTI(Geigeretal.,2013)https://www.cvlibs.net/datasets/kitti/CC-BY-NC-SA3.0Testing
                         Middlebury(Scharsteinetal.,2014)https://vision.middlebury.edu/stereo/data/CustomTesting
                         MVS-Synth(Huangetal.,2018)https://phuang17....mvs-synth.htmlCustomTrain
                         NYUv2(Silbermanetal.,2012)https://cs.nyu.edu/...v2.htmlCustomTesting
                         nuScenes(Caesaretal.,2020)https://www.nuscenes.org/CustomTesting
                         P3M-10k(Lietal.,2021)https://github.com/JizhiziLi/P3MCustomTesting
                         PPR10K(Liangetal.,2021)https://github.com/csjliang/PPR10KApache2.0Testing
                         RAISE(Dang-Nguyenetal.,2015)http://loki...download.htmlCustomTesting
                         ReDWeb(Xianetal.,2018)https://sites.google.com/site/redwebcvpr18/CustomTrain
                         SAILVOS3D(Huetal.,2021)https://sailvos.web.illin...index.htmlCustomTrain
                         ScanNet(Daietal.,2017)http://www.scan-net.org/CustomTrain
                         Sintel(Butleretal.,2012)http://sintel.is.tue.mpg.de/CustomTesting
                         SmartPortraits(Kornilovaetal.,2022)https://mobile...SmartPortraits/CustomTrain
                         SPAQ(Fangetal.,2020)https://github.com/h4nwei/SPAQCustomTesting
                         Spring(Mehletal.,2023)https://spring-benchmark.org/CCBY4.0Testing
                         Sun-RGBD(Songetal.,2015)https://rgbd.cs.princeton.edu/CustomTesting
                         Synscapes(Wrenninge&Unger,2018)https://synscapes.on.liu.se/CustomTrain
                         TartanAir(Wangetal.,2020)https://theairlab.org/tartanair-dataset/CCBY4.0Train
                         UASOL(Baueretal.,2019)https://osf.io/64532/CCBY4.0Train
                         UnrealStereo4K(Tosietal.,2021)https://github.com/fabiotosi92/SMD-NetsCustomTrain
                         Unsplashhttps://unsplash.com/dataCustomTesting
                              ´
                         UrbanSyn(Gomezetal.,2023)https://www.urbansyn.org/CCBY-SA4.0Train
                         VirtualKITTI2(Gaidonetal.,2016)https://europe.naverlabs.com...-worlds/CCBY-NC-SA3.0Train
                         ZOOM(Zhangetal.,2019)https://github.com/ceciliav...inference-Testing
                                                    29
                   Preprint
                             Table14:DepthPromodeltraininghyperparameters.
                                         Stage1Stage2
                                         250 100
                     Epochs
                     Epochlength72000
                     Schedule1%warmup,80%constantLR,19%×0.1LR
                     LRforEncoder1.28e-5
                     LRforDecoder1.28e-4
                     Batchsize128
                     OptimizerAdam
                     Weightdecay0
                     Clipgradientnorm0.2
                     PretrainedLayerNormFrozen
                     Randomcolorchangeprobability75%
                     Randomblurprobability30%
                     CentercropprobabilityforFOV-augmentation50%
                     MetricdepthnormalizationCSTM-label(Yinetal.,2023)
                     NumberofchannelsforDecoder256
                     Resolution1536×1536
                     DepthPromodelstructure:
                     Image-Encoderresolution384×384
                     Patch-Encoderresolution384×384
                     Numberof384×384patchesinDepthPro35
                     Intersectionof384×384patchesinDepthPro25%
                           Table15:Traininglossfunctionsfordifferentdatasetsandstages.
                     LossfunctionDatasets
                     Stage1
                     MAE Hypersim,Tartanair,Synscapes,Urbansyn,Dy-
                     SSI-MAGEnamicReplica,Bedlam,IRS,VirtualKitti2,Sail-
                                          vos3d
                     MAE(trimmed=20%)ARKitScenes,DimlIndoor,Scannet,SmartPor-
                                          traits
                     SSI-MAEUnrealStereo4k,3DKenBurns,Eden,MVS
                     SSI-MAGESynth
                     SSI-MAE(trimmed=20%)HRWSI,BlendedMVG
                     Stage2
                     MAE,MSE,MAGE,MALE,MSGEHypersim,Tartanair,Synscapes,Urbansyn,Dy-
                                          namicReplica,Bedlam,IRS,VirtualKitti2,Sail-
                                          vos3d
                   Tab.16listsourevaluationdatasets,therangeofdepthvaluesusedforevaluation,thenumberof
                   samples,andtheresolutionofthegroundtruthdepthmaps.Incaseamethodpredicteddepthmapsat
                   adifferentresolution,weresizedpredictionsbilinearlytomatchthegroundtruthresolution.
                   Sinceseveralfactorsoutlinedabovecanaffectthereportedaccuracyofamethod,fewbaselines
                   reportsufﬁcientdetailontheirevaluationsetup,andtheexactevaluationsetupsmaydifferacross
                   baselines,itisgenerallyimpossibletoexactlyreproducereportedresultswhileguaranteeingfairness.
                   Weprioritizedfaircomparisonandtriedtoevaluateallbaselinesinthesameenvironment.We
                   wereabletomatchmostreportedresults,withthefollowingthreenotabledifferences.ZeroDepth
                   reportedbetterresultsonnuScenes,whichweattributetotheuseofadifferentvalidationsetintheir
                                         30
                                     Preprint
                                     Table16:Datasetevaluationsetup.Foreachmetricdepthdatasetinourevaluation,wereportthe
                                     rangeofvaliddepthvalues,numberofsamples,andresolutionofgroundtruthdepthmaps.Dueto
                                     thelargesizeofthevalidationset(approximately35Ksamples),weusedarandomlysampledsubset
                                     ofNuScenes.
                                       DatasetMinimumdistance(m)Maximumdistance(m)NumberofSamplesDepthResolution(px)
                                       Booster0.001102283008×4112
                                       ETH3D0.1002004544032×6048
                                       iBims0.10010100480×640
                                       Middlebury0.00110151988×2952
                                       NuScenes0.00180881900×1600
                                       Sintel0.010801064436×1024
                                       Sun-RGBD0.001105050530×730
                                     evaluation.UniDepthreporteddifferentresultsonETH3D,whichweattributetothehandlingofraw
                                     images;speciﬁcally,inoursetup,weusetherawimageswithoutanypost-processing,andtakethe
                                     intrinsicsfromtheaccompanyingEXIFdata;webelievethisbestadherestothezero-shotpremisefor
                                     single-imagedepthestimation.Finally,onSUN-RGBD,DepthAnythingfairsbetterinourevaluation
                                     setupthanintheevaluationreportedintheoriginalpaper.
                                     Evaluationmetricforsharpboundaries.Forbothourdepth-basedandmask-basedboundary
                                     metrics,weapplythesameweighted-averagingstrategytoaccountformultiplerelativedepth
                                     ratios.F1values(depth-basedmetrics)andRecallvalues(mask-basedmetrics)areaveragedacross
                                     thresholdsthatrangelinearlyfromt    =5tot       =25.Weightsarecomputedasthenormalized
                                                                     minmax
                                     rangeofthresholdvaluesbetweent      andt    ,suchthatstrongerweightsaregiventowardshigh
                                                                     minmax
                                     thresholdvalues.
                                     DAPPLICATIONS
                                     Metric,sharp,andfastmonoculardepthestimationenablesavarietyofdownstreamapplications.We
                                     showcasetheutilityofDepthProintwoadditionalcontextsbeyondnovelviewsynthesis:conditional
                                     imagesynthesiswithControlNet(Zhangetal.,2023b)andsyntheticdepthofﬁeld(Pengetal.,
                                     2022a).
                                     Depth-conditionedimagesynthesis.Inthisapplicationwestylizeanimagethroughatextprompt
                                     viaControlNet(Zhangetal.,2023b).Toretainthestructureoftheinputimage,wepredictadepth
                                     mapfromtheinputimageanduseitforconditioningtheimagesynthesisthroughapre-trained
                                     depth-to-imageControlNetSD1.5model.Figure11showstheinputimage,prompt,andpredicted
                                     depthmapsandsynthesisresultsforDepthPro,DeothAnythingv2,Marigold,andMetric3Dv2.We
                                     ﬁndthatonlyDepthProaccuratelypredictsthecablesandskyregion,resultinginastylizedimage
                                     thatretainsthestructureoftheinputimage.Baselineseithermisscables,causingthecablecarto
                                     ﬂoatmid-air(DepthAnythingv2),oraddagradienttothesky(Marigold).
                                     Syntheticdepthofﬁeld.Syntheticdepthofﬁeldcanbeusedtohighlighttheprimarysubjectin
                                     aphotobydeliberatelyblurringthesurroundingareas.BokehMe(Pengetal.,2022a)introduces
                                     ahybridrenderingframeworkthatmarriesaneuralrendererwithaclassicalphysicallymotivated
                                     renderer.Thisframeworktakesasingleimagealongwithadepthmapasinput.Inthiscontext,itis
                                     essentialforthedepthmaptodelineateobjectswell,suchthatthephoto’ssubjectiskeptcorrectly
                                     infocuswhileothercontentiscorrectlyblurredout.Furthermore,thedepthmapshouldcorrectly
                                     traceoutthedetailsofthesubject,todeepthese(andonlythese)detailscorrectlyinfocus.Figure12
                                     showstheadvantageaffordedbyDepthProinthisapplication.(Wekeepthemostsalientobjectin
                                     focusbysettingtherefocuseddisparity(dispfocus)hyperparameterofBokehMeasthedisparityof
                                     theobject.)
                                                                                 31
                   Preprint
                                            Prompt:Awatercolor
                         Input              imageofacable-car.
                              PredicteddepthSynthesizedimage
                         DepthPro
                         DepthAnythingv2
                         Marigold
                         Metric3Dv2
                   Figure11:Comparisononconditionalimagesynthesis.WeuseControlNet(Zhangetal.,2023a)
                   tosynthesizeastylizedimagegivenaprompt(toprow,right)andadepthmap.Thedepthmapis
                   predictedfromtheinputimage(Lietal.,2022a)(toprow,left)viaDepthPro,andbaselines.The
                   leftcolumnshowsdepthmaps,therightcolumnthesynthesizedimage.Forthebaselines,missing
                   cables(DepthAnythingv2&Matric3Dv2)oraspuriousgradientinthesky(Marigold)alterthe
                   scenestructureofthesynthesizedimage.
                                        32
                            Preprint
                               DepthPro(Ours)MarigoldDepthAnythingv2DepthPro(Ours)MarigoldDepthAnythingv2
                            Figure12:Comparisononsyntheticdepthofﬁeld.Wecomparethesyntheticdepthofﬁeld
                            producedbyBokehMe(Pengetal.,2022a)usingdepthmapsfromDepthPro,Marigold(Keetal.,
                            2024),andDepthAnythingv2(Yangetal.,2024a).Zoominfordetail.
                                                            33
