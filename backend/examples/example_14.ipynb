{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97107eb0",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction to Monocular Depth Estimation and Depth Pro\n",
    "\n",
    "In this lesson, we will introduce the concept of **monocular depth estimation**, understand the challenges associated with it, and get an overview of the **Depth Pro** model. Finally, we'll run a simple inference using Depth Pro on a sample image.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Monocular Depth Estimation?\n",
    "\n",
    "**Monocular depth estimation** is the task of predicting the depth of each pixel in an image captured by a single camera. Unlike stereo vision, which relies on multiple images from different viewpoints, monocular depth estimation aims to infer depth information using only one image.\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "- **Ambiguity**: A single image lacks direct 3D information, making depth estimation inherently ambiguous.\n",
    "- **Scale Recovery**: Without knowledge of the camera parameters (like focal length), it's difficult to recover the absolute scale of objects in the scene.\n",
    "- **Generalization**: Models trained on specific datasets may not perform well on unseen images due to varying scene types and camera settings.\n",
    "\n",
    "---\n",
    "\n",
    "## Introducing Depth Pro\n",
    "\n",
    "**Depth Pro** is a state-of-the-art model for **zero-shot metric monocular depth estimation**. It addresses the challenges mentioned above by:\n",
    "\n",
    "- Producing **sharp and detailed** depth maps with high-frequency details.\n",
    "- Estimating **metric depth** with **absolute scale** without relying on camera intrinsics.\n",
    "- Being **fast and efficient**, capable of generating high-resolution depth maps quickly.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Efficient Multi-scale Vision Transformer (ViT) Architecture**: Utilizes ViTs for dense prediction tasks.\n",
    "- **Training Protocol with Real and Synthetic Data**: Combines datasets to improve generalization and accuracy.\n",
    "- **Boundary Accuracy Metrics**: Introduces new metrics to evaluate and improve boundary sharpness.\n",
    "- **Focal Length Estimation**: Predicts camera focal length from a single image to achieve metric scale.\n",
    "\n",
    "---\n",
    "\n",
    "## Running an Inference with Depth Pro\n",
    "\n",
    "In this section, we will:\n",
    "\n",
    "- Install the required libraries.\n",
    "- Load the pre-trained Depth Pro model.\n",
    "- Run inference on a sample image.\n",
    "- Visualize the estimated depth map.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch and other dependencies if not already installed\n",
    "# Uncomment and run the following line if needed:\n",
    "\n",
    "# !pip install torch torchvision pillow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf00fc",
   "metadata": {},
   "source": [
    "*Note:* Ensure that you have PyTorch installed in your environment. The above command installs PyTorch and other required libraries.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431743e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: Load the Depth Pro Model\n",
    "\n",
    "We'll use the `create_model_and_transforms` function from the `depth_pro` module (provided in the repository) to load the pre-trained model and the necessary data transforms.\n",
    "\n",
    "First, ensure that the repository code is accessible. You can clone it or copy the `depth_pro` module into your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the 'depth_pro' module in your working directory\n",
    "from depth_pro import create_model_and_transforms\n",
    "\n",
    "# Determine the device to use (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model and the data transforms\n",
    "model, transform = create_model_and_transforms(device=device)\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92d146f",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We import `create_model_and_transforms` from the `depth_pro` module.\n",
    "- We check if a GPU is available and set the device accordingly.\n",
    "- We call `create_model_and_transforms` to load the pre-trained Depth Pro model and the necessary data transforms.\n",
    "- We set the model to evaluation mode using `model.eval()` since we're not training it.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Load a Sample Image\n",
    "\n",
    "Select an image to test the model. You can use any RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the sample image\n",
    "image_path = 'sample_image.jpg'  # Replace with your image path\n",
    "\n",
    "# Load the image using PIL\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.title('Input Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd63343",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We specify the path to the image and load it using `Image.open`.\n",
    "- We convert the image to RGB to ensure it has three channels.\n",
    "- We display the image using `matplotlib`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Preprocess the Image and Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the necessary transforms to the image\n",
    "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the tensor to the appropriate device\n",
    "input_tensor = input_tensor.to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    prediction = model.infer(input_tensor)\n",
    "\n",
    "# Extract the depth map from the prediction\n",
    "depth_map = prediction['depth'].cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c0c13b",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- We apply the transforms to the image to prepare it for the model.\n",
    "    - The transforms include resizing, normalization, and conversion to a tensor.\n",
    "- We add a batch dimension since the model expects a batched input.\n",
    "- We move the input tensor to the same device as the model.\n",
    "- We use `torch.no_grad()` to prevent PyTorch from tracking gradients during inference.\n",
    "- We call the `infer` method of the model to get the prediction dictionary.\n",
    "- We extract the depth map from the prediction and convert it to a NumPy array for visualization.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Visualize the Estimated Depth Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a490c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the depth map for visualization\n",
    "depth_min = depth_map.min()\n",
    "depth_max = depth_map.max()\n",
    "depth_normalized = (depth_map - depth_min) / (depth_max - depth_min)\n",
    "\n",
    "# Display the depth map\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(depth_normalized, cmap='viridis')\n",
    "plt.axis('off')\n",
    "plt.title('Estimated Depth Map')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1d3e5",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Depth values can vary widely, so we normalize them to the range [0, 1] for visualization.\n",
    "- We use the `viridis` colormap for better visual contrast.\n",
    "- We display the normalized depth map using `matplotlib` and add a colorbar to show the scale.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we:\n",
    "\n",
    "- Introduced the concept of monocular depth estimation and its challenges.\n",
    "- Learned about Depth Pro and its key features.\n",
    "- Ran an inference using Depth Pro on a sample image.\n",
    "- Visualized the estimated depth map.\n",
    "\n",
    "Now that we've seen Depth Pro in action, in the next lesson, we'll delve into the basics of Vision Transformers (ViT) and how they're applied in Depth Pro.\n",
    "\n",
    "---\n",
    "\n",
    "# End of Lesson 1\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 2: Vision Transformers (ViT) in Depth Pro\n",
    "\n",
    "In this lesson, we'll introduce **Vision Transformers (ViT)**, understand their key concepts, and explore how Depth Pro utilizes ViTs in its architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Vision Transformers (ViT)\n",
    "\n",
    "### Background\n",
    "\n",
    "- Originally, **Transformers** were introduced for sequence modeling tasks in NLP (e.g., machine translation).\n",
    "- **ViTs** adapt the Transformer architecture for image data.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Patch Embedding**:\n",
    "   - The input image is divided into fixed-size patches (e.g., 16x16 pixels).\n",
    "   - Each patch is flattened and mapped to a vector (embedding) of a fixed size.\n",
    "\n",
    "2. **Positional Encoding**:\n",
    "   - Since Transformers lack inherent positional information, positional encodings are added to the patch embeddings to retain spatial information.\n",
    "\n",
    "3. **Self-Attention Mechanism**:\n",
    "   - Allows the model to weigh the importance of different patches when making predictions.\n",
    "   - Captures global dependencies in the image.\n",
    "\n",
    "### Benefits for Computer Vision\n",
    "\n",
    "- **Global Context**: ViTs can capture relationships across the entire image.\n",
    "- **Scalability**: Can be scaled to larger datasets and models effectively.\n",
    "- **Flexibility**: Can be applied to various vision tasks, including classification, segmentation, and depth estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## ViT in Depth Pro\n",
    "\n",
    "Depth Pro leverages ViTs for both the **patch encoder** and the **image encoder**. The model processes the image at multiple scales to capture both local details and global context.\n",
    "\n",
    "### Multi-scale Approach\n",
    "\n",
    "- **Patch Encoder**:\n",
    "  - Processes overlapping patches at different scales.\n",
    "  - Shared weights across scales to enforce scale invariance.\n",
    "\n",
    "- **Image Encoder**:\n",
    "  - Processes the downsampled whole image to provide global context.\n",
    "\n",
    "### Code Exploration\n",
    "\n",
    "Let's explore how ViTs are integrated into Depth Pro by examining key code snippets.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Creating the ViT Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: depth_pro.py\n",
    "\n",
    "def create_backbone_model(preset: ViTPreset) -> Tuple[nn.Module, ViTPreset]:\n",
    "    ...\n",
    "    if preset in VIT_CONFIG_DICT:\n",
    "        config = VIT_CONFIG_DICT[preset]\n",
    "        model = create_vit(preset=preset, use_pretrained=False)\n",
    "    else:\n",
    "        raise KeyError(f\"Preset {preset} not found.\")\n",
    "    ...\n",
    "    return model, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b3eb2",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Presets**: Pre-defined configurations for different ViT models are stored in `VIT_CONFIG_DICT`.\n",
    "- **create_vit**: A function that creates a ViT model based on the preset.\n",
    "- **Backbone Models**: Both the patch encoder and image encoder are created using this function.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Understanding `create_vit` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeebea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: network/vit_factory.py\n",
    "\n",
    "def create_vit(...):\n",
    "    ...\n",
    "    model = timm.create_model(\n",
    "        config.timm_preset, pretrained=use_pretrained, dynamic_img_size=True\n",
    "    )\n",
    "    model = make_vit_b16_backbone(\n",
    "        model,\n",
    "        encoder_feature_dims=config.encoder_feature_dims,\n",
    "        encoder_feature_layer_ids=config.encoder_feature_layer_ids,\n",
    "        vit_features=config.embed_dim,\n",
    "        use_grad_checkpointing=use_grad_checkpointing,\n",
    "    )\n",
    "    ...\n",
    "    return model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69052a",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **timm Library**: Third-party library `timm` is used to create the ViT model.\n",
    "- **Dynamic Image Size**: Allows the model to adjust to different input sizes.\n",
    "- **make_vit_b16_backbone**: Wraps the model to adapt it for Depth Pro's requirements, including extracting features from specific layers.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: ViT in Depth Pro Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: network/encoder.py\n",
    "\n",
    "class DepthProEncoder(nn.Module):\n",
    "    def __init__(...):\n",
    "        ...\n",
    "        # Patch and image encoders\n",
    "        self.patch_encoder = patch_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3d01ae",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Patch Encoder**: Processes image patches at multiple scales.\n",
    "- **Image Encoder**: Processes the downsampled full image.\n",
    "- **Shared Weights**: The same ViT architecture (and possibly weights) can be used for both encoders.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380a8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.Tensor) -> list[torch.Tensor]:\n",
    "    ...\n",
    "    # Split the image into patches at different scales\n",
    "    x_pyramid_patches = torch.cat(\n",
    "        (x0_patches, x1_patches, x2_patches), dim=0\n",
    "    )\n",
    "    # Encode the patches using the ViT patch encoder\n",
    "    x_pyramid_encodings = self.patch_encoder(x_pyramid_patches)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448edda0",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Multi-scale Patches**: The image is split into patches with overlapping regions to capture fine details.\n",
    "- **Encoding**: The ViT model processes these patches to produce feature representations.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: ViT Architecture Details\n",
    "\n",
    "While the low-level implementation details are abstracted away, it's important to understand that the ViT model consists of:\n",
    "\n",
    "- **Multi-Head Self-Attention Layers**: Capture relationships between patches.\n",
    "- **Feedforward Networks**: Apply non-linear transformations to the features.\n",
    "- **Layer Normalization and Residual Connections**: Help with training stability and model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualizing the ViT Process\n",
    "\n",
    "Here's a simplified diagram of how ViT processes an image:\n",
    "\n",
    "1. **Input Image**: The image is divided into patches.\n",
    "2. **Patch Embedding**: Each patch is flattened and passed through a linear layer.\n",
    "3. **Positional Encoding**: Added to the patch embeddings to retain spatial information.\n",
    "4. **Transformer Encoder**: Processes the sequence of embeddings through self-attention layers.\n",
    "5. **Output Features**: The encoded features represent the input image.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we:\n",
    "\n",
    "- Introduced the concept of Vision Transformers and their key components.\n",
    "- Explored how ViTs are integrated into Depth Pro as patch and image encoders.\n",
    "- Reviewed code snippets to understand how the ViTs are created and used in the model.\n",
    "\n",
    "Understanding ViTs is crucial for grasping how Depth Pro effectively captures both local and global information in images for depth estimation.\n",
    "\n",
    "In the next lesson, we'll delve deeper into Depth Pro's architecture, focusing on how it processes multi-scale features and fuses them to produce accurate depth maps.\n",
    "\n",
    "---\n",
    "\n",
    "# End of Lesson 2\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 3: Depth Pro's Multi-scale Architecture\n",
    "\n",
    "In this lesson, we'll explore Depth Pro's multi-scale architecture in detail, focusing on the **encoder**, **decoder**, and how the model fuses features from different scales to produce high-resolution depth maps.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview of Depth Pro's Architecture\n",
    "\n",
    "![Depth Pro Architecture](https://raw.githubusercontent.com/your-repo/your-image-path/architecture.png)\n",
    "\n",
    "*Note: Replace the image link with the actual architecture image from the paper or repository.*\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Input Image Pyramid**: The input image is downsampled to create an image pyramid with multiple scales.\n",
    "2. **Patch Encoder**: Processes patches from different scales using a shared ViT encoder.\n",
    "3. **Image Encoder**: Processes the downsampled full image to provide global context.\n",
    "4. **Feature Fusion**: Combines features from different scales in the decoder.\n",
    "5. **Decoder**: Generates the high-resolution depth map from fused features.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-scale Encoding\n",
    "\n",
    "### Creating the Image Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c42879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: network/encoder.py\n",
    "\n",
    "def _create_pyramid(self, x: torch.Tensor) -> tuple:\n",
    "    # Original resolution (e.g., 1536x1536)\n",
    "    x0 = x\n",
    "\n",
    "    # Downsampled by a factor of 2 (e.g., 768x768)\n",
    "    x1 = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Downsampled by a factor of 4 (e.g., 384x384)\n",
    "    x2 = F.interpolate(x, scale_factor=0.25, mode='bilinear', align_corners=False)\n",
    "\n",
    "    return x0, x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51641a92",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **x0**: Original high-resolution image.\n",
    "- **x1**: Medium-resolution image.\n",
    "- **x2**: Low-resolution image.\n",
    "- The pyramid allows the model to capture features at different scales.\n",
    "\n",
    "### Splitting Images into Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb94291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(self, x: torch.Tensor, overlap_ratio: float) -> torch.Tensor:\n",
    "    patch_size = 384  # Size of patches\n",
    "    patch_stride = int(patch_size * (1 - overlap_ratio))\n",
    "    image_size = x.shape[-1]  # Assuming square images\n",
    "\n",
    "    steps = (image_size - patch_size) // patch_stride + 1\n",
    "    patches = []\n",
    "\n",
    "    for j in range(steps):\n",
    "        for i in range(steps):\n",
    "            x0 = i * patch_stride\n",
    "            y0 = j * patch_stride\n",
    "            x1 = x0 + patch_size\n",
    "            y1 = y0 + patch_size\n",
    "            patch = x[:, :, y0:y1, x0:x1]\n",
    "            patches.append(patch)\n",
    "\n",
    "    return torch.cat(patches, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97262c",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Overlap**: Controlled by `overlap_ratio` to capture continuous features.\n",
    "- **Patches**: Extracted from the image and concatenated to form a batch.\n",
    "\n",
    "### Encoding Patches with ViT Patch Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ab4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode patches\n",
    "x_pyramid_patches = torch.cat([x0_patches, x1_patches, x2_patches], dim=0)\n",
    "x_pyramid_encodings = self.patch_encoder(x_pyramid_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c02c65",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- Patches from different scales are concatenated.\n",
    "- The shared `patch_encoder` processes all patches.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Fusion and Decoding\n",
    "\n",
    "### Merging Encoded Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(self, x: torch.Tensor, batch_size: int, padding: int) -> torch.Tensor:\n",
    "    # Calculate the number of steps (patches) along each dimension\n",
    "    steps = int(np.sqrt(x.shape[0] // batch_size))\n",
    "    # ...\n",
    "    # Reconstruct the feature map from the patches\n",
    "    # ...\n",
    "    return merged_feature_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b288eb",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Purpose**: Reconstruct the spatial arrangement of features.\n",
    "- **Padding**: Removed to ensure seamless merging.\n",
    "\n",
    "### Decoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: network/decoder.py\n",
    "\n",
    "class MultiresConvDecoder(nn.Module):\n",
    "    def __init__(self, dims_encoder: Iterable[int], dim_decoder: int):\n",
    "        ...\n",
    "        # Convolutional layers to adjust dimensions\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(dim_in, dim_decoder, kernel_size=1, bias=False)\n",
    "            for dim_in in dims_encoder\n",
    "        ])\n",
    "        # Fusion blocks\n",
    "        self.fusions = nn.ModuleList([\n",
    "            FeatureFusionBlock2d(dim_decoder) for _ in dims_encoder\n",
    "        ])\n",
    "\n",
    "    def forward(self, encodings: List[torch.Tensor]) -> torch.Tensor:\n",
    "        features = encodings[-1]\n",
    "        for i in reversed(range(len(encodings) - 1)):\n",
    "            features = self.fusions[i](features, self.convs[i](encodings[i]))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c1131",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Convs**: Adjust the number of channels in the encoded features.\n",
    "- **Fusions**: Merge features from different scales.\n",
    "\n",
    "### Feature Fusion Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusionBlock2d(nn.Module):\n",
    "    def __init__(self, num_features: int):\n",
    "        super().__init__()\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(num_features, num_features, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip_connection: torch.Tensor = None) -> torch.Tensor:\n",
    "        if skip_connection is not None:\n",
    "            x = x + skip_connection\n",
    "        x = self.residual_block(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff99bd0",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Skip Connections**: Adds features from previous layers (like in ResNet).\n",
    "- **Residual Block**: Helps in learning identity mappings, improving gradient flow.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Depth Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b29f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: depth_pro.py\n",
    "\n",
    "class DepthPro(nn.Module):\n",
    "    def __init__(...):\n",
    "        ...\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(dim_decoder, dim_decoder // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(dim_decoder // 2, dim_decoder // 2, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_decoder // 2, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "        features = self.decoder(encodings)\n",
    "        depth = self.head(features)\n",
    "        return depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb785f1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Head**: Transforms the fused features into the final depth map.\n",
    "- **ConvTranspose2d**: Upsamples the feature map to the desired resolution.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we:\n",
    "\n",
    "- Explored Depth Pro's multi-scale architecture.\n",
    "- Learned how the model processes images at different scales and patches.\n",
    "- Understood how features are fused in the decoder to produce the depth map.\n",
    "- Reviewed code snippets to see how these components are implemented.\n",
    "\n",
    "By leveraging multi-scale processing and effective feature fusion, Depth Pro achieves high-resolution depth estimation with sharp details.\n",
    "\n",
    "In the next lesson, we'll delve into how Depth Pro is trained, including the loss functions used and the training protocol.\n",
    "\n",
    "---\n",
    "\n",
    "# End of Lesson 3\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 4: Training Depth Pro - Loss Functions and Training Protocol\n",
    "\n",
    "In this lesson, we'll discuss how Depth Pro is trained, focusing on the **loss functions** and the **training curriculum** that combines real and synthetic datasets to achieve high accuracy and sharp boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions Used in Depth Pro\n",
    "\n",
    "To train Depth Pro effectively, several loss functions are employed to handle different aspects of depth estimation.\n",
    "\n",
    "### 1. Mean Absolute Error (MAE)\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MAE}}(\\hat{C}, C) = \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{C}_i - C_i|\n",
    "\\]\n",
    "\n",
    "- \\( \\hat{C} \\): Predicted canonical inverse depth.\n",
    "- \\( C \\): Ground truth canonical inverse depth.\n",
    "- \\( N \\): Number of valid pixels.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Penalizes the absolute differences between predicted and ground truth depths.\n",
    "- Robust to outliers.\n",
    "\n",
    "### 2. Multi-scale Gradient Losses\n",
    "\n",
    "To encourage sharpness and preserve edges, gradient-based losses are applied at multiple scales.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{*, p, M}(C, \\hat{C}) = \\frac{1}{M} \\sum_{j=1}^{M} \\frac{1}{N_j} \\sum_{i=1}^{N_j} |\\nabla_* C_i^j - \\nabla_* \\hat{C}_i^j|^p\n",
    "\\]\n",
    "\n",
    "- \\( \\nabla_* \\): Spatial derivative operator (e.g., gradient, Laplacian).\n",
    "- \\( p \\): Norm (usually 1 for MAE or 2 for MSE).\n",
    "- \\( M \\): Number of scales.\n",
    "- \\( j \\): Scale index.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "- Encourages the model to focus on edges and fine details.\n",
    "- Improves boundary sharpness in the predicted depth maps.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Curriculum\n",
    "\n",
    "### Stage 1: Generalization\n",
    "\n",
    "- **Datasets**: A mix of real-world and synthetic datasets.\n",
    "- **Objective**: Learn robust features that generalize across different domains.\n",
    "- **Losses**:\n",
    "  - **MAE Loss**: Applied to metric datasets.\n",
    "  - **Scale-and-Shift-Invariant Loss**: Applied to non-metric datasets (datasets without absolute scale).\n",
    "\n",
    "### Stage 2: Refinement\n",
    "\n",
    "- **Datasets**: Only high-quality synthetic datasets.\n",
    "- **Objective**: Enhance the sharpness and accuracy of depth boundaries.\n",
    "- **Losses**:\n",
    "  - **MAE Loss**: Enforces overall depth accuracy.\n",
    "  - **Multi-scale Gradient Losses**: Focuses on preserving edges and details.\n",
    "\n",
    "**Rationale:**\n",
    "\n",
    "- **Stage 1**: Real-world datasets provide diversity, while synthetic datasets offer precise ground truth.\n",
    "- **Stage 2**: Synthetic datasets help refine the model to capture fine details, leveraging their accurate annotations.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Exploration: Loss Function Implementation\n",
    "\n",
    "While the training code isn't provided in the repository snippet, we can discuss how the loss functions might be implemented in PyTorch.\n",
    "\n",
    "### Implementing MAE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52865ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_mae_loss(predicted_depth, ground_truth):\n",
    "    # Assume both inputs are of shape [Batch, 1, Height, Width]\n",
    "    loss = F.l1_loss(predicted_depth, ground_truth, reduction='mean')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d9ed3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Computing Gradients for Gradient Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92df0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(image):\n",
    "    # Computes image gradients using Sobel operator\n",
    "    gradient_x = image[:, :, :, :-1] - image[:, :, :, 1:]\n",
    "    gradient_y = image[:, :, :-1, :] - image[:, :, 1:, :]\n",
    "    return gradient_x, gradient_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174f4b7",
   "metadata": {},
   "source": [
    "### Implementing Multi-scale Gradient Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c362133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_scale_gradient_loss(predicted_depth, ground_truth, scales=[1, 0.5, 0.25]):\n",
    "    total_loss = 0\n",
    "    for scale in scales:\n",
    "        if scale != 1:\n",
    "            # Downsample the predicted and ground truth depths\n",
    "            predicted_scaled = F.interpolate(predicted_depth, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "            ground_truth_scaled = F.interpolate(ground_truth, scale_factor=scale, mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            predicted_scaled = predicted_depth\n",
    "            ground_truth_scaled = ground_truth\n",
    "        \n",
    "        # Compute gradients\n",
    "        pred_grad_x, pred_grad_y = gradient(predicted_scaled)\n",
    "        gt_grad_x, gt_grad_y = gradient(ground_truth_scaled)\n",
    "        \n",
    "        # Compute gradient loss\n",
    "        loss_x = F.l1_loss(pred_grad_x, gt_grad_x, reduction='mean')\n",
    "        loss_y = F.l1_loss(pred_grad_y, gt_grad_y, reduction='mean')\n",
    "        \n",
    "        total_loss += (loss_x + loss_y)\n",
    "    \n",
    "    return total_loss / len(scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548294f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Total Loss Function\n",
    "\n",
    "Combining both MAE loss and multi-scale gradient loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ce7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(predicted_depth, ground_truth):\n",
    "    mae = compute_mae_loss(predicted_depth, ground_truth)\n",
    "    grad_loss = multi_scale_gradient_loss(predicted_depth, ground_truth)\n",
    "    return mae + grad_loss_weight * grad_loss  # grad_loss_weight is a hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9741d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Training Loop Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    images, depths = batch\n",
    "    images = images.to(device)\n",
    "    depths = depths.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    predicted_depth = model(images)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = total_loss(predicted_depth, depths)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f67c52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we:\n",
    "\n",
    "- Discussed the loss functions used to train Depth Pro, focusing on MAE and multi-scale gradient losses.\n",
    "- Understood the two-stage training curriculum that combines real and synthetic data.\n",
    "- Explored how these loss functions encourage both accuracy and sharpness in the depth maps.\n",
    "\n",
    "In the next lesson, we'll look at how Depth Pro evaluates boundary accuracy and how it estimates focal length from a single image.\n",
    "\n",
    "---\n",
    "\n",
    "# End of Lesson 4\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 5: Evaluating Boundary Accuracy and Focal Length Estimation\n",
    "\n",
    "In this final lesson, we'll explore how Depth Pro evaluates boundary accuracy using new metrics and how it estimates focal length from a single image to produce metric depth maps without relying on camera intrinsics.\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluating Boundary Accuracy\n",
    "\n",
    "### Importance of Boundary Sharpness\n",
    "\n",
    "- **Visual Quality**: Sharp boundaries in depth maps lead to better visualizations and applications like 3D rendering.\n",
    "- **Downstream Tasks**: Accurate boundaries improve the performance of tasks like segmentation and object recognition.\n",
    "\n",
    "### New Metrics Introduced\n",
    "\n",
    "Depth Pro introduces metrics that leverage existing datasets with accurate masks (e.g., matting datasets) to evaluate boundary accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### Occluding Contours in Depth Maps\n",
    "\n",
    "An **occluding contour** between two neighboring pixels \\( i \\) and \\( j \\) is identified if the depth difference exceeds a threshold:\n",
    "\n",
    "\\[\n",
    "c_d(i, j) = \\left[ \\frac{d(j)}{d(i)} > (1 + \\frac{t}{100}) \\right]\n",
    "\\]\n",
    "\n",
    "- \\( d(i) \\): Depth at pixel \\( i \\).\n",
    "- \\( t \\): Threshold percentage (e.g., 5%).\n",
    "\n",
    "### Precision and Recall Calculations\n",
    "\n",
    "- **Precision** (\\( P \\)) and **Recall** (\\( R \\)) are calculated based on the predicted and ground truth occluding contours.\n",
    "\n",
    "\\[\n",
    "P(t) = \\frac{\\sum_{i, j} c_d(i, j) \\land c_{\\hat{d}}(i, j)}{\\sum_{i, j} c_d(i, j)}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "R(t) = \\frac{\\sum_{i, j} c_d(i, j) \\land c_{\\hat{d}}(i, j)}{\\sum_{i, j} c_{\\hat{d}}(i, j)}\n",
    "\\]\n",
    "\n",
    "- \\( c_d \\): Ground truth occluding contours.\n",
    "- \\( c_{\\hat{d}} \\): Predicted occluding contours.\n",
    "- \\( \\land \\): Logical AND operation.\n",
    "\n",
    "### F1 Score\n",
    "\n",
    "- Combines precision and recall to provide a balanced metric:\n",
    "\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{P \\times R}{P + R}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Code Exploration: Boundary Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: eval/boundary_metrics.py\n",
    "\n",
    "def boundary_f1(pr: np.ndarray, gt: np.ndarray, t: float) -> float:\n",
    "    # Compute occluding contours for predicted and ground truth depths\n",
    "    ap, bp, cp, dp = fgbg_depth(pr, t)\n",
    "    ag, bg, cg, dg = fgbg_depth(gt, t)\n",
    "\n",
    "    # Calculate recall\n",
    "    r = 0.25 * (\n",
    "        np.count_nonzero(ap & ag) / max(np.count_nonzero(ag), 1) +\n",
    "        np.count_nonzero(bp & bg) / max(np.count_nonzero(bg), 1) +\n",
    "        np.count_nonzero(cp & cg) / max(np.count_nonzero(cg), 1) +\n",
    "        np.count_nonzero(dp & dg) / max(np.count_nonzero(dg), 1)\n",
    "    )\n",
    "\n",
    "    # Calculate precision\n",
    "    p = 0.25 * (\n",
    "        np.count_nonzero(ap & ag) / max(np.count_nonzero(ap), 1) +\n",
    "        np.count_nonzero(bp & bg) / max(np.count_nonzero(bp), 1) +\n",
    "        np.count_nonzero(cp & cg) / max(np.count_nonzero(cp), 1) +\n",
    "        np.count_nonzero(dp & dg) / max(np.count_nonzero(dp), 1)\n",
    "    )\n",
    "\n",
    "    # Compute F1 score\n",
    "    if r + p == 0:\n",
    "        return 0.0\n",
    "    return 2 * (r * p) / (r + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e7b0eb",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **fgbg_depth**: Computes foreground/background relationships between neighboring pixels.\n",
    "- **Directional Calculations**: Evaluates in four directions (left, top, right, bottom).\n",
    "- **Normalization**: Ensures division by zero is avoided using `max(count, 1)`.\n",
    "\n",
    "---\n",
    "\n",
    "## Focal Length Estimation from a Single Image\n",
    "\n",
    "### Motivation\n",
    "\n",
    "- **Metric Depth**: To recover absolute depth values (metric), the camera's focal length is usually required.\n",
    "- **Applicability**: Estimating the focal length directly from the image allows the model to work without camera intrinsics.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1773e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: network/fov.py\n",
    "\n",
    "class FOVNetwork(nn.Module):\n",
    "    def __init__(self, num_features: int, fov_encoder: Optional[nn.Module] = None):\n",
    "        super().__init__()\n",
    "        # Simplified for illustration\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(num_features, num_features // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_features // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, features: torch.Tensor) -> torch.Tensor:\n",
    "        # Use features from the encoder\n",
    "        fov = self.head(features)\n",
    "        return fov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebe5af",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Input Features**: Uses features from the encoder or decoder.\n",
    "- **Prediction**: Outputs a single scalar representing the estimated focal length or field of view.\n",
    "\n",
    "### Incorporating FOV Estimation into Depth Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be45c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: depth_pro.py\n",
    "\n",
    "class DepthPro(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        ...\n",
    "        # Get depth predictions\n",
    "        depth = self.head(features)\n",
    "        # Estimate focal length\n",
    "        fov_deg = self.fov(x, features)\n",
    "        # Adjust depth to metric using estimated focal length\n",
    "        if fov_deg is not None:\n",
    "            f_px = compute_focal_length_in_pixels(fov_deg, image_width)\n",
    "            depth = adjust_depth_to_metric(depth, f_px)\n",
    "        return depth, fov_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642874eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lesson, we:\n",
    "\n",
    "- Learned about the new metrics introduced in Depth Pro to evaluate boundary accuracy.\n",
    "- Understood how these metrics focus on occluding contours and edges in depth maps.\n",
    "- Explored how Depth Pro estimates the focal length from a single image, eliminating the need for camera intrinsics.\n",
    "\n",
    "---\n",
    "\n",
    "# End of Lesson 5\n",
    "\n",
    "---\n",
    "\n",
    "Congratulations! You've completed all five lessons on Depth Pro. You've gained an understanding of:\n",
    "\n",
    "- Monocular depth estimation challenges and Depth Pro's contributions.\n",
    "- The use of Vision Transformers in Depth Pro's architecture.\n",
    "- How Depth Pro processes multi-scale features and fuses them for depth prediction.\n",
    "- The training process, loss functions, and training curriculum.\n",
    "- Evaluation metrics for boundary accuracy and focal length estimation.\n",
    "\n",
    "Feel free to explore the code further and experiment with Depth Pro on your own images!\n",
    "\n",
    "# The End\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
