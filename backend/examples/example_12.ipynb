{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0905cd1",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction to Zero-Shot Metric Monocular Depth Estimation\n",
    "\n",
    "*Understanding the basics of monocular depth estimation and the challenges in achieving zero-shot metric predictions.*\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Monocular depth estimation involves predicting a depth map from a single RGB image. This depth map assigns a distance value to each pixel, indicating how far that part of the scene is from the camera. It's a fundamental task in computer vision with applications in 3D reconstruction, robotics, augmented reality, and more.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "- **Monocular Depth Estimation**: Estimating depth from a single image without stereo information.\n",
    "- **Zero-Shot Learning**: The model can generalize to new, unseen data without additional training.\n",
    "- **Metric Depth**: Depth estimates with absolute scale, meaning the distances correspond to real-world measurements.\n",
    "\n",
    "### Challenges:\n",
    "\n",
    "- **Ambiguity**: Inferring 3D structure from 2D images is inherently ambiguous.\n",
    "- **Generalization**: Models often struggle to generalize to new domains or scenes.\n",
    "- **Absolute Scale**: Estimating metric depth without camera parameters (like focal length) is challenging.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"Our model, Depth Pro, produces metric depth maps with absolute scale on arbitrary images ‘in the wild’ without requiring metadata such as camera intrinsics.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand the fundamentals of monocular depth estimation.\n",
    "- Recognize the challenges in achieving zero-shot metric depth estimation.\n",
    "- Prepare the environment for running code examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up the necessary environment. We'll be using Python and PyTorch for our implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758009a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Uncomment the line below if running in a new environment\n",
    "# !pip install torch torchvision matplotlib pillow requests\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5926b98",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading and Displaying an Image\n",
    "\n",
    "We'll work with a sample image throughout these lessons. Let's load and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a945205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an example image\n",
    "image_url = 'https://example.com/sample_image.jpg'  # Replace with a valid image URL\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(image_url)\n",
    "img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img)\n",
    "plt.title('Sample Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Optionally, save the image for later use\n",
    "img.save('sample_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfebbfe",
   "metadata": {},
   "source": [
    "*Note: Replace `'https://example.com/sample_image.jpg'` with the URL of an actual image or use an image from your local machine.*\n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Monocular Depth Estimation\n",
    "\n",
    "In monocular depth estimation, our goal is to predict a depth map \\( D \\) from a single RGB image \\( I \\).\n",
    "\n",
    "### Why is it challenging?\n",
    "\n",
    "- **Lack of Stereo Information**: Without multiple viewpoints, depth cues are limited.\n",
    "- **Scale Ambiguity**: Objects can appear differently based on perspective.\n",
    "- **Generalization**: Scenes in the wild have diverse characteristics.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"Zero-shot monocular depth estimation underpins a growing variety of applications... Our model, Depth Pro, produces metric depth maps with absolute scale on arbitrary images ‘in the wild’ without requiring metadata such as camera intrinsics.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this lesson, we've established the foundational knowledge required to understand the problem Depth Pro aims to solve. In the next lesson, we'll delve into the architecture of Depth Pro and how it leverages Vision Transformers to overcome these challenges.\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 2: Depth Pro's Multi-Scale Vision Transformer Architecture\n",
    "\n",
    "*Exploring how Depth Pro leverages a multi-scale Vision Transformer to produce high-resolution depth maps with sharp boundaries.*\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Depth Pro introduces an efficient multi-scale Vision Transformer (ViT) architecture to capture both global context and fine-grained details, enabling the production of sharp and high-resolution depth maps.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"First, we design an efficient multi-scale ViT-based architecture for capturing the global image context while also adhering to fine structures at high resolution.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand the basics of Vision Transformers (ViTs).\n",
    "- Learn how Depth Pro modifies ViTs for multi-scale processing.\n",
    "- Explore the encoder architecture in Depth Pro's implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## Vision Transformers (ViTs)\n",
    "\n",
    "ViTs apply the Transformer architecture, originally designed for natural language processing, to image data.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **Patch Embedding**: The image is divided into patches (e.g., 16x16 pixels), which are flattened and linearly projected to form patch embeddings.\n",
    "- **Position Embedding**: Positional information is added to the patch embeddings.\n",
    "- **Transformer Encoder**: The embeddings are processed through transformer layers to capture relationships between patches.\n",
    "\n",
    "### Challenges with ViTs:\n",
    "\n",
    "- **Computational Complexity**: The self-attention mechanism scales quadratically with the number of patches.\n",
    "- **High-Resolution Images**: Directly applying ViTs to high-resolution images is computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## Depth Pro's Multi-Scale Approach\n",
    "\n",
    "Depth Pro addresses the computational challenges by applying the ViT encoder to image patches at multiple scales and fusing the results.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"We propose a network architecture that applies a plain ViT backbone at multiple scales and fuses predictions into a single high-resolution output.\"\n",
    "\n",
    "### Steps in Depth Pro's Encoder:\n",
    "\n",
    "1. **Create Image Pyramid**: Generate downsampled versions of the input image.\n",
    "2. **Extract Patches at Each Scale**: Split each image in the pyramid into patches.\n",
    "3. **Apply Shared ViT Encoder**: Process patches from all scales using the same ViT encoder.\n",
    "4. **Merge Encodings**: Combine the outputs to form multi-resolution feature maps.\n",
    "5. **Fuse Features**: Integrate features from different scales to capture both global context and fine details.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Walkthrough\n",
    "\n",
    "We'll examine parts of the encoder code to understand how it's implemented.\n",
    "\n",
    "### Import Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e01a4",
   "metadata": {},
   "source": [
    "### Encoder Class Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deeca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthProEncoder(nn.Module):\n",
    "    \"\"\"DepthPro Encoder for multi-scale processing.\"\"\"\n",
    "    def __init__(self, dims_encoder, patch_encoder, image_encoder, hook_block_ids, decoder_features):\n",
    "        super().__init__()\n",
    "        self.dims_encoder = list(dims_encoder)\n",
    "        self.patch_encoder = patch_encoder  # Shared ViT encoder\n",
    "        self.image_encoder = image_encoder  # Global context encoder\n",
    "        self.hook_block_ids = list(hook_block_ids)\n",
    "        # Initialization of upsampling layers and hooks..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfee65",
   "metadata": {},
   "source": [
    "### Creating the Image Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f65cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pyramid(self, x):\n",
    "    x0 = x\n",
    "    x1 = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
    "    x2 = F.interpolate(x, scale_factor=0.25, mode='bilinear', align_corners=False)\n",
    "    return x0, x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301938c",
   "metadata": {},
   "source": [
    "- **x0**: Original image (high resolution).\n",
    "- **x1**: Downsampled by a factor of 0.5.\n",
    "- **x2**: Downsampled by a factor of 0.25.\n",
    "\n",
    "### Splitting Images into Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c00d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(self, x, overlap_ratio=0.25):\n",
    "    patch_size = 384  # For a standard ViT\n",
    "    stride = int(patch_size * (1 - overlap_ratio))\n",
    "    # Compute number of patches and extract them...\n",
    "    # Returns concatenated patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2b845",
   "metadata": {},
   "source": [
    "- **Overlap**: Ensures smooth transitions between patches.\n",
    "- **Patches**: Extracted at different scales to capture varying levels of detail.\n",
    "\n",
    "### Processing Patches with ViT Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75227371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "    x0, x1, x2 = self._create_pyramid(x)\n",
    "    x0_patches = self.split(x0, overlap_ratio=0.25)\n",
    "    x1_patches = self.split(x1, overlap_ratio=0.5)\n",
    "    x2_patches = x2.unsqueeze(0)  # Single patch covering the whole image\n",
    "\n",
    "    # Concatenate patches from all scales\n",
    "    x_patches = torch.cat([x0_patches, x1_patches, x2_patches], dim=0)\n",
    "\n",
    "    # Apply the shared ViT encoder\n",
    "    x_encodings = self.patch_encoder(x_patches)\n",
    "\n",
    "    # Merge encodings into feature maps\n",
    "    # Fusion steps...\n",
    "    return multi_scale_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a8348",
   "metadata": {},
   "source": [
    "### Fusion of Features\n",
    "\n",
    "- The outputs from different scales are combined.\n",
    "- Feature maps are upsampled to a common resolution.\n",
    "- Fused features capture both global context and fine details.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing the Architecture\n",
    "\n",
    "Here's a simplified representation:\n",
    "\n",
    "![Depth Pro Encoder Architecture](https://i.imgur.com/your_image_link.png)\n",
    "\n",
    "*Note: Replace with an actual diagram if available.*\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Example\n",
    "\n",
    "Since we don't have the actual ViT implementation here, we'll create mock encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65924cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockViTEncoder(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return torch.rand(batch_size, self.output_dim)\n",
    "\n",
    "# Instantiate mock encoders\n",
    "patch_encoder = MockViTEncoder(output_dim=768)\n",
    "image_encoder = MockViTEncoder(output_dim=768)\n",
    "\n",
    "# Create the DepthPro encoder\n",
    "encoder = DepthProEncoder(\n",
    "    dims_encoder=[64, 128, 256, 512],\n",
    "    patch_encoder=patch_encoder,\n",
    "    image_encoder=image_encoder,\n",
    "    hook_block_ids=[],\n",
    "    decoder_features=256,\n",
    ")\n",
    "\n",
    "# Create a dummy input image\n",
    "input_image = torch.randn(1, 3, 1536, 1536)\n",
    "\n",
    "# Forward pass\n",
    "features = encoder(input_image)\n",
    "\n",
    "# Print the output feature shapes\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"Feature {i} shape: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5e6f3",
   "metadata": {},
   "source": [
    "*Note: In the actual implementation, the ViT encoders process the patches and images to generate meaningful features.*\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, we've explored how Depth Pro's multi-scale ViT architecture processes images at different resolutions to capture detailed features efficiently. In the next lesson, we'll delve into the training protocol and the loss functions used to train Depth Pro.\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 3: Training Protocol and Loss Functions\n",
    "\n",
    "*Understanding how Depth Pro is trained to produce sharp, high-quality depth maps using a combination of real and synthetic datasets.*\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Depth Pro employs a carefully designed training curriculum that leverages both real-world and synthetic datasets. The goal is to achieve high generalization while also ensuring depth maps have sharp boundaries and fine details.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"We devise a set of loss functions and a training curriculum that promote sharp depth estimates while training on real-world datasets that provide coarse and inaccurate supervision around boundaries, along with synthetic datasets that offer accurate pixelwise ground truth but limited realism.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn about the two-stage training curriculum.\n",
    "- Understand the loss functions used in training.\n",
    "- Implement the loss functions in code.\n",
    "\n",
    "---\n",
    "\n",
    "## Two-Stage Training Curriculum\n",
    "\n",
    "### Stage 1: Generalization\n",
    "\n",
    "- **Goal**: Learn robust features that generalize across domains.\n",
    "- **Datasets**: Mix of real-world and synthetic datasets.\n",
    "- **Loss Functions**:\n",
    "  - Mean Absolute Error (MAE) on metric datasets.\n",
    "  - Scale-and-shift-invariant MAE on non-metric datasets.\n",
    "\n",
    "### Stage 2: Boundary Sharpness\n",
    "\n",
    "- **Goal**: Enhance sharpness and fine details in depth maps.\n",
    "- **Datasets**: Only synthetic datasets with accurate pixel-wise ground truth.\n",
    "- **Loss Functions**:\n",
    "  - MAE, plus additional gradient-based losses.\n",
    "\n",
    "---\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "### Canonical Inverse Depth\n",
    "\n",
    "- **Definition**: Normalized depth representation prioritizing closer areas.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MAE}}(C, \\hat{C}) = \\frac{1}{N} \\sum_{i=1}^N |C_i - \\hat{C}_i|\n",
    "\\]\n",
    "\n",
    "- \\( C \\): Predicted canonical inverse depth.\n",
    "- \\( \\hat{C} \\): Ground truth canonical inverse depth.\n",
    "- \\( N \\): Number of valid pixels.\n",
    "\n",
    "### Multi-Scale Gradient Losses\n",
    "\n",
    "#### Mean Absolute Gradient Error (MAGE)\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MAGE}} = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\left| \\nabla_S C_i^j - \\nabla_S \\hat{C}_i^j \\right|\n",
    "\\]\n",
    "\n",
    "#### Mean Absolute Laplace Error (MALE)\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MALE}} = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\left| \\nabla_L C_i^j - \\nabla_L \\hat{C}_i^j \\right|\n",
    "\\]\n",
    "\n",
    "#### Mean Squared Gradient Error (MSGE)\n",
    "\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{MSGE}} = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{N_j} \\sum_{i=1}^{N_j} \\left( \\nabla_S C_i^j - \\nabla_S \\hat{C}_i^j \\right)^2\n",
    "\\]\n",
    "\n",
    "- \\( M \\): Number of scales.\n",
    "- \\( \\nabla_S \\): Scharr operator.\n",
    "- \\( \\nabla_L \\): Laplace operator.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Loss Functions in Code\n",
    "\n",
    "### Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae_loss(pred, gt, mask=None):\n",
    "    if mask is not None:\n",
    "        valid = mask > 0\n",
    "        loss = torch.abs(pred[valid] - gt[valid]).mean()\n",
    "    else:\n",
    "        loss = torch.abs(pred - gt).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0fc0a",
   "metadata": {},
   "source": [
    "### Gradient Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718acdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(img):\n",
    "    # Scharr operator kernels\n",
    "    scharr_x = torch.Tensor([[3, 0, -3], [10, 0, -10], [3, 0, -3]]).to(img.device) / 16\n",
    "    scharr_y = torch.Tensor([[3, 10, 3], [0, 0, 0], [-3, -10, -3]]).to(img.device) / 16\n",
    "    scharr_x = scharr_x.view(1, 1, 3, 3)\n",
    "    scharr_y = scharr_y.view(1, 1, 3, 3)\n",
    "\n",
    "    grad_x = F.conv2d(img, scharr_x, padding=1)\n",
    "    grad_y = F.conv2d(img, scharr_y, padding=1)\n",
    "    return grad_x, grad_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6efbb3",
   "metadata": {},
   "source": [
    "### Mean Absolute Gradient Error (MAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f36449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mage_loss(pred, gt):\n",
    "    pred_grad_x, pred_grad_y = compute_gradient(pred)\n",
    "    gt_grad_x, gt_grad_y = compute_gradient(gt)\n",
    "    loss = (torch.abs(pred_grad_x - gt_grad_x) + torch.abs(pred_grad_y - gt_grad_y)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a94f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff833423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we have a model, optimizer, and dataloaders\n",
    "for epoch in range(num_epochs):\n",
    "    for data in dataloader:\n",
    "        images, gt_depth = data  # Load your data batch\n",
    "        optimizer.zero_grad()\n",
    "        pred_depth = model(images)\n",
    "        loss_mae = compute_mae_loss(pred_depth, gt_depth)\n",
    "        loss_mage = compute_mage_loss(pred_depth, gt_depth)\n",
    "        loss = loss_mae + loss_mage  # Combine losses as needed\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34391eb7",
   "metadata": {},
   "source": [
    "*Note: This is a simplified example. In practice, you'll need to handle data loading, masking invalid pixels, multi-scale losses, etc.*\n",
    "\n",
    "---\n",
    "\n",
    "## Discussion\n",
    "\n",
    "By combining the MAE loss with gradient-based losses, Depth Pro encourages the model to produce depth maps that are not only accurate in terms of depth values but also have sharp transitions and details.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, we've learned about the training strategy and loss functions that enable Depth Pro to produce high-quality, sharp depth maps. In the next lesson, we'll explore the evaluation metrics used to assess boundary accuracy and how they're implemented.\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 4: Boundary Accuracy Metrics and Evaluation\n",
    "\n",
    "*Understanding the importance of sharp boundaries in depth estimation and how Depth Pro evaluates and improves boundary accuracy.*\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Sharp boundaries in depth maps are crucial for applications like novel view synthesis, where blurry or misaligned edges can lead to visual artifacts. Depth Pro introduces dedicated evaluation metrics for boundary accuracy.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"We derive a new set of metrics that enable leveraging highly accurate matting datasets for quantifying the accuracy of boundary tracing in evaluating monocular depth maps.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn why boundary accuracy matters in depth estimation.\n",
    "- Understand the boundary evaluation metrics introduced in the paper.\n",
    "- Implement the metrics in code.\n",
    "\n",
    "---\n",
    "\n",
    "## Importance of Sharp Boundaries\n",
    "\n",
    "- **Visual Quality**: Blurry edges in depth maps can result in \"flying pixels\" or artifacts in rendered images.\n",
    "- **Applications**: Tasks like image segmentation, matting, and augmented reality require precise depth discontinuities.\n",
    "\n",
    "---\n",
    "\n",
    "## Boundary Evaluation Metrics\n",
    "\n",
    "### Pairwise Depth Ratio\n",
    "\n",
    "To determine if there's an occluding contour between neighboring pixels \\( i \\) and \\( j \\), we define:\n",
    "\n",
    "\\[\n",
    "c_d(i, j) = \\left[ \\frac{d(j)}{d(i)} > 1 + \\frac{t}{100} \\right]\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( d(i) \\): Depth at pixel \\( i \\).\n",
    "- \\( t \\): Threshold percentage.\n",
    "\n",
    "### Precision and Recall\n",
    "\n",
    "- **Precision (\\( P \\))**: The proportion of predicted boundaries that are correct.\n",
    "- **Recall (\\( R \\))**: The proportion of true boundaries that are detected.\n",
    "\n",
    "\\[\n",
    "P(t) = \\frac{\\sum_{i,j} c_d(i,j) \\wedge c_{\\hat{d}}(i,j)}{\\sum_{i,j} c_d(i,j)} \\\\\n",
    "R(t) = \\frac{\\sum_{i,j} c_d(i,j) \\wedge c_{\\hat{d}}(i,j)}{\\sum_{i,j} c_{\\hat{d}}(i,j)}\n",
    "\\]\n",
    "\n",
    "### Boundary F1 Score\n",
    "\n",
    "The harmonic mean of precision and recall:\n",
    "\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{P \\times R}{P + R}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing Boundary Metrics in Code\n",
    "\n",
    "### Defining Occluding Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contours(depth_map, threshold):\n",
    "    # Compute depth ratios between neighboring pixels\n",
    "    depth_ratio_right = depth_map[:, :, 1:] / depth_map[:, :, :-1]\n",
    "    depth_ratio_down = depth_map[:, 1:, :] / depth_map[:, :-1, :]\n",
    "\n",
    "    # Determine occluding contours based on the threshold\n",
    "    contour_right = depth_ratio_right > (1 + threshold / 100)\n",
    "    contour_down = depth_ratio_down > (1 + threshold / 100)\n",
    "    return contour_right, contour_down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c443ad0",
   "metadata": {},
   "source": [
    "### Precision and Recall Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa281d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_precision_recall(pred_contour, gt_contour):\n",
    "    true_positive = (pred_contour & gt_contour).sum()\n",
    "    pred_positive = pred_contour.sum()\n",
    "    gt_positive = gt_contour.sum()\n",
    "\n",
    "    precision = true_positive / (pred_positive + 1e-6)\n",
    "    recall = true_positive / (gt_positive + 1e-6)\n",
    "    return precision.item(), recall.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baef7e9",
   "metadata": {},
   "source": [
    "### Boundary F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_boundary_f1(pred_depth, gt_depth, thresholds):\n",
    "    f1_scores = []\n",
    "    for t in thresholds:\n",
    "        pred_contours = compute_contours(pred_depth, t)\n",
    "        gt_contours = compute_contours(gt_depth, t)\n",
    "        precision, recall = compute_precision_recall(pred_contours, gt_contours)\n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    return sum(f1_scores) / len(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9efa7b",
   "metadata": {},
   "source": [
    "*Note: Ensure that depth maps are properly normalized and handle cases where division by zero might occur.*\n",
    "\n",
    "---\n",
    "\n",
    "## Example Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e56331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume pred_depth and gt_depth are numpy arrays of the same shape\n",
    "thresholds = np.linspace(5, 25, 5)  # Thresholds from 5% to 25%\n",
    "boundary_f1 = compute_boundary_f1(pred_depth, gt_depth, thresholds)\n",
    "print(f'Boundary F1 Score: {boundary_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02437f4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Discussion\n",
    "\n",
    "By evaluating the model based on boundary accuracy, Depth Pro ensures that it's not just the overall depth values that are accurate, but also the transitions and edges, which are critical for high-quality rendering.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lesson, we've delved into how Depth Pro measures and improves boundary accuracy in depth estimation. In the final lesson, we'll explore how Depth Pro estimates focal length and how to perform inference using the trained model.\n",
    "\n",
    "---\n",
    "\n",
    "# Lesson 5: Focal Length Estimation and Inference with Depth Pro\n",
    "\n",
    "*Understanding how Depth Pro estimates focal length from a single image and how to use the model for inference.*\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Depth Pro can estimate the focal length (field of view) from a single image, enabling metric depth estimation without requiring camera intrinsics.\n",
    "\n",
    "From the paper:\n",
    "\n",
    "> \"We contribute zero-shot focal length estimation from a single image that dramatically outperforms the prior state of the art.\"\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn how Depth Pro estimates focal length.\n",
    "- Run inference using Depth Pro on sample images.\n",
    "- Understand how to obtain metric depth maps.\n",
    "\n",
    "---\n",
    "\n",
    "## Focal Length Estimation\n",
    "\n",
    "### Why Estimate Focal Length?\n",
    "\n",
    "- **Metric Depth**: To convert relative depth estimates to metric measurements, focal length is needed.\n",
    "- **Camera Metadata**: EXIF data may be missing or inaccurate, especially for images \"in the wild.\"\n",
    "\n",
    "### Approach\n",
    "\n",
    "- **Separate Focal Length Estimation Head**: A small network predicts the horizontal field of view (FoV) based on features from the depth estimation network.\n",
    "- **Training**: The focal length head is trained separately using images with known focal lengths.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "### Focal Length Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795183aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOVNetwork(nn.Module):\n",
    "    \"\"\"Field of View estimation network.\"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(num_features, num_features // 2, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_features // 2, num_features // 4, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(num_features // 4, 1, 3, 2, 1),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346f1ae",
   "metadata": {},
   "source": [
    "### Integrating with Depth Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6309caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthPro(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fov_network = FOVNetwork(num_features=encoder.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        depth = self.decoder(features)\n",
    "        fov = self.fov_network(features)\n",
    "        return depth, fov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc7cbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inference with Depth Pro\n",
    "\n",
    "### Loading a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b75db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume model weights are saved in 'depth_pro.pth'\n",
    "model = DepthPro(encoder, decoder)\n",
    "model.load_state_dict(torch.load('depth_pro.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864ecb0a",
   "metadata": {},
   "source": [
    "### Running Inference on an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1536, 1536)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "])\n",
    "img = Image.open('sample_image.jpg').convert('RGB')\n",
    "input_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    depth, fov = model(input_tensor)\n",
    "\n",
    "# Convert depth to numpy array\n",
    "depth_map = depth.squeeze().cpu().numpy()\n",
    "fov_value = fov.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04203ce",
   "metadata": {},
   "source": [
    "### Converting Depth to Metric Units\n",
    "\n",
    "- **Canonical Inverse Depth**: The model outputs canonical inverse depth.\n",
    "- **Conversion to Metric Depth**:\n",
    "\n",
    "\\[\n",
    "D_{\\text{metric}} = \\frac{f_{\\text{pixels}}}{w} \\times C\n",
    "\\]\n",
    "\n",
    "- \\( f_{\\text{pixels}} \\): Focal length in pixels.\n",
    "- \\( w \\): Image width in pixels.\n",
    "- \\( C \\): Canonical inverse depth.\n",
    "\n",
    "- **Calculating Focal Length in Pixels**:\n",
    "\n",
    "\\[\n",
    "f_{\\text{pixels}} = \\frac{0.5 \\times w}{\\tan(0.5 \\times \\text{FoV (in radians)})}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display the depth map\n",
    "plt.imshow(depth_map, cmap='plasma')\n",
    "plt.colorbar(label='Depth (arbitrary units)')\n",
    "plt.title('Predicted Depth Map')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199fd1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Discussion\n",
    "\n",
    "Depth Pro's ability to estimate focal length allows it to produce metric depth maps without relying on camera metadata. This is particularly useful for images where such data is unavailable or unreliable.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this final lesson, we've learned how Depth Pro estimates focal length and performs inference to generate metric depth maps from single images. With this knowledge, you're now equipped to understand and work with Depth Pro's implementation and contribute to advancements in monocular depth estimation.\n",
    "\n",
    "---\n",
    "\n",
    "# Final Remarks\n",
    "\n",
    "Throughout these lessons, we've explored Depth Pro's approach to zero-shot metric monocular depth estimation. By understanding the architecture, training protocol, evaluation metrics, and inference process, you can now delve deeper into the codebase and potentially contribute to this exciting area of research.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
