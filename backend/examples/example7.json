{
  "lessons": [
    {
      "title": "Lesson 1: Introduction to Vision Transformers in Depth Estimation",
      "description": "Learn how Vision Transformers (ViT) are utilized as backbone models in DepthPro for depth estimation.",
      "content": [
        {
          "code": "def make_vit_b16_backbone(\n    model,\n    encoder_feature_dims,\n    encoder_feature_layer_ids,\n    vit_features,\n    start_index=1,\n    use_grad_checkpointing=False,\n) -> nn.Module:\n    \"\"\"Make a ViT-B16 backbone for the DPT model.\"\"\"\n    if use_grad_checkpointing:\n        model.set_grad_checkpointing()\n\n    vit_model = nn.Module()\n    vit_model.hooks = encoder_feature_layer_ids\n    vit_model.model = model\n    vit_model.features = encoder_feature_dims\n    vit_model.vit_features = vit_features\n    vit_model.model.start_index = start_index\n    vit_model.model.patch_size = vit_model.model.patch_embed.patch_size\n    vit_model.model.is_vit = True\n    vit_model.model.forward = vit_model.model.forward_features\n\n    return vit_model",
          "notes": "## Understanding the ViT Backbone\n\nIn DepthPro, Vision Transformers (ViTs) are employed as the backbone for feature extraction, leveraging their ability to capture global context. The `make_vit_b16_backbone` function adapts a pre-trained ViT model to be used within the DepthPro architecture.\n\n### Key Components:\n- **Model adaptation**: The function wraps a pre-trained ViT model, setting up necessary attributes and methods for it to interface correctly with the DepthPro encoder.\n- **Feature hooks**: The `encoder_feature_layer_ids` specify which layers of the ViT model will be used to extract features at different resolutions.\n\n### Relevant Sections from the Paper:\n\nAs mentioned in **Section 3.1** of the paper:\n\n> *\"The key idea of our architecture is to apply plain ViT encoders on patches extracted at multiple scales and fuse the patch predictions into a single high-resolution dense prediction in an end-to-end trainable model.\"*\n\nThis approach allows DepthPro to benefit from pre-trained ViT models while efficiently handling high-resolution images for depth estimation.\n\n### Additional Reading:\n- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al.\n- [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al.\n\n### Formula:\n\nThe ViT splits an image into patches and processes them similarly to tokens in NLP transformers. If the image has dimensions $H \\times W$, and each patch has dimensions $P \\times P$, then the number of patches $N$ is:\n\n$$\nN = \\frac{H \\times W}{P^2}\n$$\n\nThese patches are linearly embedded and combined with positional embeddings before being fed into the transformer encoder."
        }
      ]
    },
    {
      "title": "Lesson 2: Building the DepthPro Encoder",
      "description": "Understand how the DepthProEncoder processes multi-scale inputs and produces multi-resolution encodings.",
      "content": [
        {
          "code": "class DepthProEncoder(nn.Module):\n    \"\"\"DepthPro Encoder combining patch and image encoders.\"\"\"\n\n    def __init__(\n        self,\n        dims_encoder: Iterable[int],\n        patch_encoder: nn.Module,\n        image_encoder: nn.Module,\n        hook_block_ids: Iterable[int],\n        decoder_features: int,\n    ):\n        \"\"\"Initialize DepthProEncoder.\"\"\"\n        super().__init__()\n        self.dims_encoder = list(dims_encoder)\n        self.patch_encoder = patch_encoder\n        self.image_encoder = image_encoder\n        self.hook_block_ids = list(hook_block_ids)\n\n        # Additional initialization code...\n\n    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n        \"\"\"Encode input at multiple resolutions.\"\"\"\n        # Implementation of the forward pass...\n\n        return [\n            x_latent0_features,\n            x_latent1_features,\n            x0_features,\n            x1_features,\n            x_global_features,\n        ]",
          "notes": "## DepthPro Encoder\n\nThe `DepthProEncoder` is responsible for creating multi-resolution encodings from the input image by processing it at multiple scales.\n\n### Key Components:\n- **Multi-scale Processing**: The encoder downsamples the input image to create an image pyramid and processes patches at different scales.\n- **Patch and Image Encoders**: Combines outputs from a patch-based encoder (processing image patches) and an image encoder (processing the whole image at a lower resolution).\n- **Feature Fusion**: The outputs are merged to produce multi-resolution features for the decoder.\n\n### Relevant Sections from the Paper:\n\nFrom **Section 3.1**:\n\n> *\"The whole network operates at a fixed resolution of 1536 × 1536...At each scale, the patches are fed into the patch encoder, which produces a feature tensor at resolution 24 × 24 per input patch...We merge the feature patches into maps, which are fed into the decoder module.\"*\n\n### Image Pyramid Creation\n\nAn image pyramid is created by downsampling the input image:\n\n$$\nI_0 = I \\\\\nI_1 = \\text{Downsample}(I, \\text{scale}=0.5) \\\\\nI_2 = \\text{Downsample}(I, \\text{scale}=0.25)\n$$\n\n### Additional Reading:\n- [Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144) by Lin et al.\n- [Understanding Multi-Scale Feature Hierarchies](https://distill.pub/2019/computing-receptive-fields/)\n\n### Code Walkthrough:\n\n- The `__init__` method initializes the encoder with the given encoders and configurations.\n- The `forward` method processes the input image through the pyramid, splits it into patches, encodes them, and merges the features."
        }
      ]
    },
    {
      "title": "Lesson 3: Designing the DepthPro Decoder",
      "description": "Explore how the decoder reconstructs depth maps from multi-resolution features using feature fusion and upsampling.",
      "content": [
        {
          "code": "class MultiresConvDecoder(nn.Module):\n    \"\"\"Decoder for multi-resolution encodings.\"\"\"\n\n    def __init__(\n        self,\n        dims_encoder: Iterable[int],\n        dim_decoder: int,\n    ):\n        \"\"\"Initialize multiresolution convolutional decoder.\"\"\"\n        super().__init__()\n        self.dims_encoder = list(dims_encoder)\n        self.dim_decoder = dim_decoder\n\n        # Additional initialization code...\n\n    def forward(self, encodings: torch.Tensor) -> torch.Tensor:\n        \"\"\"Decode the multi-resolution encodings.\"\"\"\n        # Implementation of the forward pass...\n\n        return features, lowres_features",
          "notes": "## Multiresolution Convolutional Decoder\n\nThe `MultiresConvDecoder` takes the multi-resolution encodings from the encoder and reconstructs the depth map through a series of convolutional and upsampling layers.\n\n### Key Components:\n- **Feature Fusion Blocks**: Uses `FeatureFusionBlock2d` to combine features from different scales.\n- **Upsampling**: Gradually increases the spatial resolution of the feature maps to reconstruct the high-resolution depth map.\n- **Residual Connections**: Implements `ResidualBlock` to facilitate better gradient flow and learning.\n\n### Relevant Sections from the Paper:\n\nFrom **Section 3.1**:\n\n> *\"Features are merged into a single high-resolution output through a decoder module, which resembles the DPT decoder.\"*\n\nThe decoder is designed to effectively combine multi-scale features and reconstruct detailed depth maps.\n\n### Residual Blocks\n\nAs introduced by He et al., residual blocks help in training deeper networks by allowing gradients to flow directly through skip connections.\n\n### Formula:\n\nResidual connection:\n\n$$\n\\mathbf{y} = \\mathcal{F}(\\mathbf{x}, \\mathcal{W}) + \\mathbf{x}\n$$\n\nwhere:\n\n- $\\mathbf{x}$ is the input to the block.\n- $\\mathcal{F}(\\mathbf{x}, \\mathcal{W})$ is the residual function (e.g., convolution, batch norm, and ReLU).\n\n### Additional Reading:\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by He et al.\n- [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ranftl et al.\n\n### Code Walkthrough:\n\n- The `__init__` method sets up convolutional layers and fusion blocks for combining features.\n- The `forward` method processes the encoder outputs, fusing and upsampling them to produce the final feature map."
        }
      ]
    },
    {
      "title": "Lesson 4: Implementing Field of View Estimation",
      "description": "Learn how DepthPro estimates the field of view (focal length) from an input image to produce metric depth maps.",
      "content": [
        {
          "code": "class FOVNetwork(nn.Module):\n    \"\"\"Field of View estimation network.\"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        fov_encoder: Optional[nn.Module] = None,\n    ):\n        \"\"\"Initialize the Field of View estimation block.\"\"\"\n        super().__init__()\n\n        # Implementation details...\n\n    def forward(self, x: torch.Tensor, lowres_feature: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward the fov network.\"\"\"\n        # Implementation of the forward pass...\n\n        return self.head(x)",
          "notes": "## Field of View Estimation in DepthPro\n\nDepthPro estimates the field of view (FoV) or focal length directly from the input image, enabling metric depth estimation without requiring camera intrinsics.\n\n### Key Components:\n- **FOV Network**: A neural network that predicts the FoV using features from the encoder.\n- **Integration with DepthPro**: The FoV estimation is integrated into the depth estimation pipeline, allowing for depth scaling.\n\n### Relevant Sections from the Paper:\n\nFrom **Section 3.3**:\n\n> *\"To handle images that may have inaccurate or missing EXIF metadata, we supplement our network with a focal length estimation head.\"*\n\nThe network predicts the horizontal angular field-of-view ($\\theta$), which is then used to compute the focal length $f_{px}$ in pixels:\n\n$$\nf_{px} = \\frac{0.5 \\times w}{\\tan(0.5 \\times \\theta)}\n$$\n\nwhere $w$ is the image width.\n\n### Importance of FoV Estimation\n\nAccurate FoV estimation allows DepthPro to produce metric depth maps with absolute scale, even when camera intrinsics are unavailable.\n\n### Additional Reading:\n- [Towards Zero-Shot Scale-Aware Monocular Depth Estimation](https://arxiv.org/abs/2304.08484) by Guizilini et al.\n- [Learning the Camera Intrinsics of Streaming Videos](https://arxiv.org/abs/2104.14567) by Kocabas et al.\n\n### Code Walkthrough:\n\n- The `FOVNetwork` uses convolutional layers to regress the FoV from image features.\n- The `forward` method processes the input features and returns the estimated FoV."
        }
      ]
    },
    {
      "title": "Lesson 5: End-to-End Depth Estimation with DepthPro",
      "description": "Bring together all components to understand how DepthPro performs end-to-end depth estimation from an input image.",
      "content": [
        {
          "code": "class DepthPro(nn.Module):\n    \"\"\"DepthPro network.\"\"\"\n\n    def __init__(\n        self,\n        encoder: DepthProEncoder,\n        decoder: MultiresConvDecoder,\n        last_dims: tuple[int, int],\n        use_fov_head: bool = True,\n        fov_encoder: Optional[nn.Module] = None,\n    ):\n        \"\"\"Initialize DepthPro.\"\"\"\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        \n        # Additional initialization code...\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"Decode by projection and fusion of multi-resolution encodings.\"\"\"\n        # Implementation of the forward pass...\n\n        return canonical_inverse_depth, fov_deg\n\n    @torch.no_grad()\n    def infer(\n        self,\n        x: torch.Tensor,\n        f_px: Optional[Union[float, torch.Tensor]] = None,\n        interpolation_mode=\"bilinear\",\n    ) -> Mapping[str, torch.Tensor]:\n        \"\"\"Infer depth and fov for a given image.\"\"\"\n        # Implementation of the inference method...\n\n        return {\n            \"depth\": depth.squeeze(),\n            \"focallength_px\": f_px,\n        }",
          "notes": "## DepthPro: End-to-End Depth Estimation\n\nThe `DepthPro` class combines the encoder, decoder, and FoV estimation to perform depth estimation from an input image.\n\n### Key Components:\n- **Encoder and Decoder**: Work together to extract features and reconstruct the depth map.\n- **FoV Estimation**: If enabled, estimates the field of view for metric depth scaling.\n- **Inference Method**: The `infer` method handles preprocessing and postprocessing to produce the final depth map.\n\n### Relevant Sections from the Paper:\n\nFrom **Section 3.2**:\n\n> *\"The network operates at a fixed resolution of 1536 × 1536... The outputs are merged to produce multi-resolution features for the decoder.\"*\n\nAnd from **Section 3.3**:\n\n> *\"We supplement our network with a focal length estimation head... predicted canonical inverse depth is then scaled by the horizontal field of view.\"*\n\n### Depth Scaling Equation\n\nThe predicted depth is scaled using the estimated focal length $f_{px}$ and the image width $w$:\n\n$$\nD_m = \\frac{f_{px}}{w \\times C}\n$$\n\nwhere $C$ is the predicted canonical inverse depth.\n\n### Additional Reading:\n- [Monocular Depth Estimation: An Overview](https://arxiv.org/abs/2006.05914)\n\n### Code Walkthrough:\n\n- The `DepthPro` class initializes with the encoder, decoder, and optionally the FoV network.\n- The `forward` method computes the canonical inverse depth and optionally the FoV in degrees.\n- The `infer` method handles the scaling of the canonical inverse depth to produce metric depth, considering the estimated or provided focal length.\n- The method also resizes the depth map to the original image size if necessary."
        },
        {
          "code": "# Sample usage of DepthPro for inference\n\n# Assuming `model` is an instance of DepthPro and `transform` is the input preprocessing function\n\nfrom PIL import Image\nimport torch\n\n# Load and preprocess the image\nimage = Image.open('path_to_image.jpg')\nx = transform(image)  # Apply the same transforms used during training\n\n# Perform inference\nmodel.eval()\nwith torch.no_grad():\n    output = model.infer(x)\n\n# Retrieve the depth map\ndepth_map = output['depth']\n\n# Display or save the depth map as needed",
          "notes": "## Running Inference with DepthPro\n\nThis sample code demonstrates how to use the `DepthPro` model to perform depth estimation on an input image.\n\n### Steps:\n1. **Load the Image**: Use PIL or another library to load the input image.\n2. **Preprocess**: Apply the same transforms used during training to ensure consistency.\n3. **Inference**: Set the model to evaluation mode and perform inference within `torch.no_grad()` to disable gradient calculations.\n4. **Retrieve Output**: The output dictionary contains the depth map and estimated focal length (if applicable).\n\n### Note on Preprocessing\n\nThe preprocessing transforms should match those used during training, which may include resizing, normalization, and conversion to tensor.\n\n### Additional Notes\n\n- Ensure that the input image size and aspect ratio are compatible with the model's expected input.\n- The depth map produced is a single-channel tensor representing depth values in meters.\n\n### Displaying the Depth Map\n\nTo visualize the depth map, you can convert it to a NumPy array and use matplotlib:\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.imshow(depth_map.cpu().numpy(), cmap='plasma')\nplt.colorbar(label='Depth in meters')\nplt.show()\n```\n\n### Relevant Sections from the Paper:\n\nFrom **Section 4** (Experiments):\n\n> *\"We release code and weights at https://github.com/apple/ml-depth-pro\"*\n\nThis indicates that the model can be used as provided for inference tasks.\n\n### Additional Reading:\n- [PyTorch Documentation on Inference and Model Evaluation](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
        }
      ]
    }
  ]
}
