{
  "lessons": [
    {
      "title": "Lesson 1: Introduction to DepthPro Architecture",
      "description": "In this lesson, we will explore the main `DepthPro` class and understand how it integrates the encoder and decoder to perform monocular depth estimation.",
      "content": [
        {
          "code": "class DepthPro(nn.Module):\n    \"\"\"DepthPro network.\"\"\"\n\n    def __init__(\n        self,\n        encoder: DepthProEncoder,\n        decoder: MultiresConvDecoder,\n        last_dims: tuple[int, int],\n        use_fov_head: bool = True,\n        fov_encoder: Optional[nn.Module] = None,\n    ):\n        \"\"\"Initialize DepthPro.\n\n        Args:\n        ----\n            encoder: The DepthProEncoder backbone.\n            decoder: The MultiresConvDecoder decoder.\n            last_dims: The dimension for the last convolution layers.\n            use_fov_head: Whether to use the field-of-view head.\n            fov_encoder: A separate encoder for the field of view.\n\n        \"\"\"\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n    \n        dim_decoder = decoder.dim_decoder\n        self.head = nn.Sequential(\n            nn.Conv2d(\n                dim_decoder, dim_decoder // 2, kernel_size=3, stride=1, padding=1\n            ),\n            nn.ConvTranspose2d(\n                in_channels=dim_decoder // 2,\n                out_channels=dim_decoder // 2,\n                kernel_size=2,\n                stride=2,\n                padding=0,\n                bias=True,\n            ),\n            nn.Conv2d(\n                dim_decoder // 2,\n                last_dims[0],\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            ),\n            nn.ReLU(True),\n            nn.Conv2d(last_dims[0], last_dims[1], kernel_size=1, stride=1, padding=0),\n            nn.ReLU(),\n        )\n\n        # Set the final convolution layer's bias to be 0.\n        self.head[4].bias.data.fill_(0)\n\n        # Set the FOV estimation head.\n        if use_fov_head:\n            self.fov = FOVNetwork(num_features=dim_decoder, fov_encoder=fov_encoder)",
          "note": "The `DepthPro` class is the main module that combines the encoder and decoder to produce depth estimates from input images. It initializes the encoder and decoder, sets up the final convolution layers (`self.head`), and optionally includes a field-of-view (FOV) estimation network.\n\nThis class is crucial as it defines the overall architecture of the DepthPro model, orchestrating how features are extracted and processed.\n\nTo learn more about PyTorch module structures and how models are built, you can read the [PyTorch documentation on `nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)."
        },
        {
          "code": "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"Decode by projection and fusion of multi-resolution encodings.\"\"\"\n    _, _, H, W = x.shape\n    assert H == self.img_size and W == self.img_size\n\n    encodings = self.encoder(x)\n    features, features_0 = self.decoder(encodings)\n    canonical_inverse_depth = self.head(features)\n\n    fov_deg = None\n    if hasattr(self, \"fov\"):\n        fov_deg = self.fov.forward(x, features_0.detach())\n\n    return canonical_inverse_depth, fov_deg",
          "note": "The `forward` method defines how the input image `x` is processed through the DepthPro model. It passes the image through the encoder to get multi-resolution encodings, decodes these features using the decoder, and then applies the final convolution layers to get the depth prediction.\n\nOptionally, it also estimates the field of view if the FOV head is included.\n\nUnderstanding the `forward` method is key to seeing how data flows through the network.\n\nFor more on defining forward methods in PyTorch, see [Defining `forward` functions in PyTorch](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html)."
        }
      ]
    },
    {
      "title": "Lesson 2: Understanding the DepthPro Encoder",
      "description": "In this lesson, we will dive into the `DepthProEncoder`, focusing on how it processes multi-scale images and overlapping patches to create multi-resolution encodings.",
      "content": [
        {
          "code": "class DepthProEncoder(nn.Module):\n    \"\"\"DepthPro Encoder.\n\n    An encoder aimed at creating multi-resolution encodings from Vision Transformers.\n    \"\"\"\n\n    def __init__(\n        self,\n        dims_encoder: Iterable[int],\n        patch_encoder: nn.Module,\n        image_encoder: nn.Module,\n        hook_block_ids: Iterable[int],\n        decoder_features: int,\n    ):\n        \"\"\"Initialize DepthProEncoder.\n\n        Args:\n        ----\n            dims_encoder: Dimensions of the encoder at different layers.\n            patch_encoder: Backbone used for patches.\n            image_encoder: Backbone used for global image encoder.\n            hook_block_ids: Hooks to obtain intermediate features.\n            decoder_features: Number of feature outputs in the decoder.\n\n        \"\"\"\n        super().__init__()\n\n        self.dims_encoder = list(dims_encoder)\n        self.patch_encoder = patch_encoder\n        self.image_encoder = image_encoder\n        self.hook_block_ids = list(hook_block_ids)\n        # Further initialization...",
          "note": "The `DepthProEncoder` class is responsible for generating multi-resolution encodings by processing the input image at multiple scales. It uses a patch encoder to process patches of the image and an image encoder for capturing global context.\n\nBy understanding this class, we can see how the model captures both local and global features, which is essential for accurate depth estimation.\n\nTo learn more about encoder architectures and multi-scale processing, consider reading about [Convolutional Neural Network (CNN) architectures](https://cs231n.github.io/convolutional-networks/)."
        },
        {
          "code": "def _create_pyramid(\n        self, x: torch.Tensor\n    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Create a 3-level image pyramid.\"\"\"\n        x0 = x\n        x1 = F.interpolate(\n            x, size=None, scale_factor=0.5, mode=\"bilinear\", align_corners=False\n        )\n        x2 = F.interpolate(\n            x, size=None, scale_factor=0.25, mode=\"bilinear\", align_corners=False\n        )\n        return x0, x1, x2",
          "note": "The `_create_pyramid` method generates a three-level image pyramid by downsampling the input image to different scales. This allows the encoder to process the image at multiple resolutions, capturing features at different levels of detail.\n\nUnderstanding image pyramids is important for grasping how multi-scale features are extracted.\n\nFor more on image pyramids, you can read about [Image Pyramids in Computer Vision](https://en.wikipedia.org/wiki/Pyramid_(image_processing))."
        },
        {
          "code": "def split(self, x: torch.Tensor, overlap_ratio: float = 0.25) -> torch.Tensor:\n        \"\"\"Split the input into small patches with a sliding window.\"\"\"\n        patch_size = 384\n        patch_stride = int(patch_size * (1 - overlap_ratio))\n\n        image_size = x.shape[-1]\n        steps = int(math.ceil((image_size - patch_size) / patch_stride)) + 1\n\n        x_patch_list = []\n        for j in range(steps):\n            j0 = j * patch_stride\n            j1 = j0 + patch_size\n\n            for i in range(steps):\n                i0 = i * patch_stride\n                i1 = i0 + patch_size\n                x_patch_list.append(x[..., j0:j1, i0:i1])\n\n        return torch.cat(x_patch_list, dim=0)",
          "note": "The `split` method divides the input image into overlapping patches using a sliding window approach. The `overlap_ratio` parameter controls how much the patches overlap. This is useful for capturing local features while maintaining continuity across patches.\n\nSliding window methods are common in computer vision for tasks like object detection.\n\nTo learn more about sliding window techniques, see [Sliding Window Object Detection](https://medium.com/@nikasa1889/sliding-window-object-detection-with-python-and-opencv-python-be6a4792f3c3)."
        }
      ]
    },
    {
      "title": "Lesson 3: Decoding Multi-Resolution Features",
      "description": "In this lesson, we will explore the `MultiresConvDecoder` and understand how it decodes and fuses multi-resolution features to produce the final depth map.",
      "content": [
        {
          "code": "class MultiresConvDecoder(nn.Module):\n    \"\"\"Decoder for multi-resolution encodings.\"\"\"\n\n    def __init__(\n        self,\n        dims_encoder: Iterable[int],\n        dim_decoder: int,\n    ):\n        \"\"\"Initialize multiresolution convolutional decoder.\n\n        Args:\n        ----\n            dims_encoder: Expected dims at each level from the encoder.\n            dim_decoder: Dim of decoder features.\n\n        \"\"\"\n        super().__init__()\n        self.dims_encoder = list(dims_encoder)\n        self.dim_decoder = dim_decoder\n        self.dim_out = dim_decoder\n\n        # Initialization of convolutions and fusions...\n        # Further code...",
          "note": "The `MultiresConvDecoder` class takes the multi-resolution features from the encoder and decodes them to produce the depth map. It projects features from different resolutions to a common decoder dimension and fuses them using convolutional layers.\n\nThis decoder plays a crucial role in combining features from multiple scales into a comprehensive representation.\n\nFor more on decoder architectures in neural networks, you can read about [U-Net Architecture](https://arxiv.org/abs/1505.04597), which is popular in segmentation tasks."
        },
        {
          "code": "def forward(self, encodings: torch.Tensor) -> torch.Tensor:\n        \"\"\"Decode the multi-resolution encodings.\"\"\"\n        num_levels = len(encodings)\n\n        # Project features to the decoder dimension and fuse them\n        features = self.convs[-1](encodings[-1])\n        features = self.fusions[-1](features)\n        for i in range(num_levels - 2, -1, -1):\n            features_i = self.convs[i](encodings[i])\n            features = self.fusions[i](features, features_i)\n        return features, lowres_features",
          "note": "In the `forward` method, the decoder processes the list of encoder outputs from the lowest to the highest resolution. It projects each encoding to the decoder dimension using convolutions, then fuses them using `FeatureFusionBlock2d` modules. This sequential fusion helps in reconstructing detailed spatial information.\n\nUnderstanding this fusion process is important for grasping how multi-scale features contribute to the final prediction.\n\nFor more on feature fusion techniques, consider reading [Feature Pyramid Networks (FPN) for Object Detection](https://arxiv.org/abs/1612.03144)."
        },
        {
          "code": "class FeatureFusionBlock2d(nn.Module):\n    \"\"\"Feature fusion for DPT.\"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        deconv: bool = False,\n        batch_norm: bool = False,\n    ):\n        \"\"\"Initialize feature fusion block.\n\n        Args:\n        ----\n            num_features: Input and output dimensions.\n            deconv: Whether to use deconvolution before the final output conv.\n            batch_norm: Whether to use batch normalization in resnet blocks.\n\n        \"\"\"\n        super().__init__()\n\n        # Initialization of residual blocks and convolution layers...\n        # Further code...",
          "note": "The `FeatureFusionBlock2d` class is a key component in the decoder for merging features from different resolutions. It uses residual blocks and optional deconvolution (upsampling) to adjust feature maps before fusing them.\n\nThis module helps in preserving spatial details while combining features.\n\nTo learn more about residual connections and their benefits, you can read the paper on [Deep Residual Learning](https://arxiv.org/abs/1512.03385)."
        }
      ]
    },
    {
      "title": "Lesson 4: Estimating Field of View with FOVNetwork",
      "description": "In this lesson, we will learn how DepthPro estimates the field of view using the `FOVNetwork` and why this is important for depth estimation.",
      "content": [
        {
          "code": "class FOVNetwork(nn.Module):\n    \"\"\"Field of View estimation network.\"\"\"\n\n    def __init__(\n        self,\n        num_features: int,\n        fov_encoder: Optional[nn.Module] = None,\n    ):\n        \"\"\"Initialize the Field of View estimation block.\n\n        Args:\n        ----\n            num_features: Number of features used.\n            fov_encoder: Optional encoder to bring additional network capacity.\n\n        \"\"\"\n        super().__init__()\n\n        # Define FOV estimation head...\n        # Further code...",
          "note": "The `FOVNetwork` class estimates the field of view (FOV) from the input features. Knowing the FOV is crucial for producing metric depth maps with absolute scale, especially when camera intrinsics are not available.\n\nBy estimating the FOV directly from the image, the model can adjust depth predictions to be more accurate in real-world units.\n\nFor more on the importance of FOV in depth estimation, consider reading about [Camera Models and Projection](https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_Models_Projection.pdf)."
        },
        {
          "code": "def forward(self, x: torch.Tensor, lowres_feature: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward the FOV network.\n\n        Args:\n        ----\n            x (torch.Tensor): Input image.\n            lowres_feature (torch.Tensor): Low resolution feature.\n\n        Returns:\n        -------\n            The field of view tensor.\n\n        \"\"\"\n        if hasattr(self, \"encoder\"):\n            x = F.interpolate(\n                x,\n                size=None,\n                scale_factor=0.25,\n                mode=\"bilinear\",\n                align_corners=False,\n            )\n            x = self.encoder(x)[:, 1:].permute(0, 2, 1)\n            lowres_feature = self.downsample(lowres_feature)\n            x = x.reshape_as(lowres_feature) + lowres_feature\n        else:\n            x = lowres_feature\n        return self.head(x)",
          "note": "The `forward` method processes the input image and low-resolution features to estimate the FOV. If an additional encoder (`fov_encoder`) is provided, it extracts features from the downsampled image and combines them with the existing features.\n\nThis method shows how the model can predict camera parameters directly from the input data.\n\nFor further reading on estimating camera intrinsics from images, see [Camera Calibration](https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html)."
        }
      ]
    },
    {
      "title": "Lesson 5: Integrating Vision Transformers into DepthPro",
      "description": "In this final lesson, we will understand how Vision Transformers (ViTs) are integrated into the DepthPro architecture, focusing on loading and adapting pre-trained ViT models.",
      "content": [
        {
          "code": "def create_vit(\n        preset: ViTPreset,\n        use_pretrained: bool = False,\n        checkpoint_uri: str | None = None,\n        use_grad_checkpointing: bool = False,\n    ) -> nn.Module:\n        \"\"\"Create and load a VIT backbone module.\n\n        Args:\n        ----\n            preset: The VIT preset to load the pre-defined config.\n            use_pretrained: Load pretrained weights if True.\n            checkpoint_uri: Checkpoint to load the weights from.\n            use_grad_checkpointing: Use gradient checkpointing.\n\n        Returns:\n        -------\n            A Torch ViT backbone module.\n\n        \"\"\"\n        config = VIT_CONFIG_DICT[preset]\n\n        img_size = (config.img_size, config.img_size)\n        patch_size = (config.patch_size, config.patch_size)\n\n        model = timm.create_model(\n            config.timm_preset, pretrained=use_pretrained, dynamic_img_size=True\n        )\n        # Adjust model as needed...\n        # Further code...",
          "note": "The `create_vit` function loads a Vision Transformer model using a preset configuration. It can load pre-trained weights and adjust the model settings, such as image size and patch size, to fit the DepthPro architecture.\n\nIntegrating ViTs allows the model to leverage powerful transformer-based feature extraction.\n\nTo learn more about Vision Transformers, you can read the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)."
        },
        {
          "code": "def resize_vit(model: nn.Module, img_size) -> nn.Module:\n    \"\"\"Resample the ViT module to the given size.\"\"\"\n    patch_size = model.patch_embed.patch_size\n    model.patch_embed.img_size = img_size\n    grid_size = tuple([s // p for s, p in zip(img_size, patch_size)])\n    model.patch_embed.grid_size = grid_size\n\n    pos_embed = resample_abs_pos_embed(\n        model.pos_embed,\n        grid_size,\n        num_prefix_tokens=(\n            0 if getattr(model, \"no_embed_class\", False) else model.num_prefix_tokens\n        ),\n    )\n    model.pos_embed = torch.nn.Parameter(pos_embed)\n\n    return model",
          "note": "The `resize_vit` function adjusts the Vision Transformer to work with images of a different size by resizing its positional embeddings. This is necessary because ViTs use fixed-size position embeddings, and when the input image size changes, the embeddings need to be adapted.\n\nUnderstanding how to adjust positional embeddings is important for customizing ViTs for various tasks.\n\nFor more on positional embeddings in transformers, see [Sinusoidal Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
        },
        {
          "code": "# In vit.py\nclass ViTConfig:\n    \"\"\"Configuration for ViT.\"\"\"\n\n    in_chans: int\n    embed_dim: int\n    # Further configuration fields...\n\nVIT_CONFIG_DICT: Dict[ViTPreset, ViTConfig] = {\n    \"dinov2l16_384\": ViTConfig(\n        in_chans=3,\n        embed_dim=1024,\n        # Further settings...\n    ),\n}\n",
          "note": "The `ViTConfig` class and `VIT_CONFIG_DICT` dictionary define configurations for different ViT presets. These configurations specify parameters like input channels, embedding dimensions, and image sizes.\n\nBy organizing configurations, the model can easily switch between different ViT variants.\n\nFor more on model configuration management, you can read about [Using Config Files in PyTorch](https://pytorch.org/tutorials/intermediate/model_serialization_torchscript.html)."
        }
      ]
    }
  ]
}